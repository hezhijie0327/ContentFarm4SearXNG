#!/usr/bin/env python3
"""
SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
ä»å¤šä¸ªäº‘ç«¯é…ç½®æ–‡ä»¶ä¸­è·å–åŸŸååˆ—è¡¨ï¼Œç”Ÿæˆ SearXNG hostnames è§„åˆ™
ä¿®å¤äº†è·¯å¾„è§„åˆ™è¯¯åˆ¤å’ŒåŸŸåæå–çš„é—®é¢˜

pip install requests pyyaml argparse
"""

import requests
import yaml
import json
import re
from urllib.parse import urlparse
from typing import Dict, List, Set, Union, Tuple
import argparse
import sys
import time
import os
from collections import OrderedDict, defaultdict

class SearXNGHostnamesGenerator:
    def __init__(self, config_file: str = None, force_single_regex: bool = False):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            force_single_regex: å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™è¡¨è¾¾å¼
        """
        self.config = self.load_config(config_file)
        self.force_single_regex = force_single_regex
        self.domains = set()
        self.stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0
        }
        # æ–°å¢ï¼šè®°å½•æ¯ä¸ªç±»åˆ«çš„åŸŸåæ•°é‡
        self.category_domain_counts = {
            'remove': 0,
            'low_priority': 0,
            'high_priority': 0,
            'replace': 0
        }

    def load_config(self, config_file: str) -> Dict:
        """
        åŠ è½½é…ç½®æ–‡ä»¶

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„

        Returns:
            é…ç½®å­—å…¸
        """
        default_config = {
            # æ•°æ®æºé…ç½®
            "sources": [
                {
                    "name": "Chinese Internet is Dead",
                    "url": "https://raw.githubusercontent.com/obgnail/chinese-internet-is-dead/master/blocklist.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Content Farm Terminator - Content Farm Filters",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/content-farms.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Content Farm Terminator - Nearly Content Farm Filters",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/nearly-content-farms.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Content Farm Terminator - Bad Cloners",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/bad-cloners.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
            ],
            # åŸŸåæ›¿æ¢è§„åˆ™
            "replace_rules": {
                #'(.*\.)?youtube\.com$': 'yt.example.com',
                #'(.*\.)?youtu\.be$': 'yt.example.com',
                #'(.*\.)?reddit\.com$': 'teddit.example.com',
                #'(.*\.)?redd\.it$': 'teddit.example.com',
                #'(www\.)?twitter\.com$': 'nitter.example.com'
            },
            # å›ºå®šçš„ç§»é™¤è§„åˆ™
            "fixed_remove": [
                #'(.*\.)?facebook.com$'
            ],
            # å›ºå®šçš„ä½ä¼˜å…ˆçº§è§„åˆ™
            "fixed_low_priority": [
                #'(.*\.)?google(\..*)?$'
            ],
            # å›ºå®šçš„é«˜ä¼˜å…ˆçº§è§„åˆ™
            "fixed_high_priority": [
                #'(.*\.)?wikipedia.org$'
            ],
            # è§£æé…ç½®
            "parsing": {
                "ignore_specific_paths": True,  # å¿½ç•¥æŒ‡å‘ç‰¹å®šè·¯å¾„çš„è§„åˆ™
                "ignore_ip": True,     # å¿½ç•¥IPåœ°å€
                "ignore_localhost": True,  # å¿½ç•¥æœ¬åœ°ä¸»æœº
                "strict_domain_level_check": True  # ä¸¥æ ¼æ£€æŸ¥åŸŸåçº§åˆ«è§„åˆ™
            },
            # æ€§èƒ½ä¼˜åŒ–é…ç½®
            "optimization": {
                "merge_domains": True,          # å¯ç”¨åŸŸååˆå¹¶ä¼˜åŒ–
                "max_domains_per_rule": 256,     # æ¯ä¸ªåˆå¹¶è§„åˆ™çš„æœ€å¤§åŸŸåæ•°
                "group_by_tld": True,           # æŒ‰é¡¶çº§åŸŸååˆ†ç»„
                "use_trie_optimization": True,  # ä½¿ç”¨å­—å…¸æ ‘ä¼˜åŒ–
                "max_rule_length": 65536,       # å•ä¸ªè§„åˆ™çš„æœ€å¤§é•¿åº¦é™åˆ¶
                "optimize_tld_grouping": True,   # ä¼˜åŒ–TLDåˆ†ç»„ï¼Œé¿å…é‡å¤
                "enable_prefix_optimization": True,  # å¯ç”¨å‰ç¼€ä¼˜åŒ–
                "enable_suffix_optimization": True,  # å¯ç”¨åç¼€ä¼˜åŒ–
                "min_common_prefix_length": 3,      # æœ€å°å…¬å…±å‰ç¼€é•¿åº¦
                "min_common_suffix_length": 3,      # æœ€å°å…¬å…±åç¼€é•¿åº¦
                "force_single_regex": False,         # å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™è¡¨è¾¾å¼
                "sort_before_merge": True,          # åˆå¹¶å‰æ’åºåŸŸå
                "enable_advanced_tld_merge": True   # å¯ç”¨é«˜çº§TLDåˆå¹¶
            },
            # è¯·æ±‚é…ç½®
            "request_config": {
                "timeout": 30,
                "retry_count": 3,
                "retry_delay": 1
            },
            # è¾“å‡ºé…ç½®
            "output": {
                "mode": "separate_files",  # separate_files æˆ– single_file
                "directory": "./hostnames_rules/",
                "files": {
                    "replace": "rewrite-hosts.yml",
                    "remove": "remove-hosts.yml",
                    "low_priority": "low-priority-hosts.yml",
                    "high_priority": "high-priority-hosts.yml",
                    "main_config": "hostnames-config.yml"
                }
            }
        }

        if config_file:
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    # æ·±åº¦åˆå¹¶é…ç½®
                    self._deep_merge(default_config, user_config)
            except FileNotFoundError:
                print(f"é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            except Exception as e:
                print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                return default_config

        return default_config

    def _deep_merge(self, base_dict: Dict, update_dict: Dict) -> None:
        """
        æ·±åº¦åˆå¹¶å­—å…¸
        """
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_merge(base_dict[key], value)
            else:
                base_dict[key] = value

    def is_domain_level_rule(self, url_string: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™ï¼ˆè€Œéç‰¹å®šè·¯å¾„è§„åˆ™ï¼‰
        ä¿®å¤ç‰ˆæœ¬ï¼šæ›´ä¸¥æ ¼åœ°æ£€æŸ¥è·¯å¾„è§„åˆ™

        Args:
            url_string: URLå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦æ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™
        """
        url_string = url_string.strip()
        
        # ä¸¥æ ¼æ£€æŸ¥æ¨¡å¼
        if self.config["parsing"].get("strict_domain_level_check", True):
            # è¿™äº›æ¨¡å¼è¢«è®¤ä¸ºæ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™ï¼š
            domain_level_patterns = [
                # uBlock åŸŸåçº§åˆ«æ¨¡å¼ - ç²¾ç¡®æ¨¡å¼
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/?$',                    # *://*.example.com æˆ– *://*.example.com/
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/\*?$',                 # *://*.example.com/* 
                r'^\*://[a-zA-Z0-9.-]+/?$',                         # *://example.com æˆ– *://example.com/
                r'^\*://[a-zA-Z0-9.-]+/\*?$',                       # *://example.com/*
                r'^\|\|[a-zA-Z0-9.-]+\^?$',                         # ||example.com^
                r'^[a-zA-Z0-9.-]+/?$',                              # example.com æˆ– example.com/
                r'^[a-zA-Z0-9.-]+/\*?$',                            # example.com/* æˆ– example.com/
            ]
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…åŸŸåçº§åˆ«æ¨¡å¼
            for pattern in domain_level_patterns:
                if re.match(pattern, url_string):
                    # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœåŒ…å«å…·ä½“è·¯å¾„ï¼ˆé™¤äº†/å’Œ/*ï¼‰ï¼Œåˆ™ä¸æ˜¯åŸŸåçº§åˆ«
                    if self._has_specific_path(url_string):
                        return False
                    return True
            
            return False
        else:
            # å…¼å®¹æ¨¡å¼ï¼ˆåŸæ¥çš„é€»è¾‘ï¼‰
            domain_level_patterns = [
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/?$',
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/\*$',
                r'^\|\|[a-zA-Z0-9.-]+\^?$',
                r'^[a-zA-Z0-9.-]+/?$',
                r'^[a-zA-Z0-9.-]+/\*$',
            ]

            for pattern in domain_level_patterns:
                if re.match(pattern, url_string):
                    return True

            return False

    def _has_specific_path(self, url_string: str) -> bool:
        """
        æ£€æŸ¥URLæ˜¯å¦åŒ…å«å…·ä½“çš„è·¯å¾„ï¼ˆéåŸŸåçº§åˆ«ï¼‰
        
        Args:
            url_string: URLå­—ç¬¦ä¸²
            
        Returns:
            æ˜¯å¦åŒ…å«å…·ä½“è·¯å¾„
        """
        # ç§»é™¤åè®®éƒ¨åˆ†
        if url_string.startswith('*://'):
            url_part = url_string[4:]
        elif url_string.startswith('||'):
            url_part = url_string[2:].rstrip('^')
        else:
            url_part = url_string
            
        # æ£€æŸ¥æ˜¯å¦æœ‰è·¯å¾„éƒ¨åˆ†
        if '/' in url_part:
            domain_and_path = url_part.split('/', 1)
            if len(domain_and_path) > 1:
                path_part = domain_and_path[1]
                # å¦‚æœè·¯å¾„ä¸æ˜¯ç©ºã€å•ä¸ª*æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™è®¤ä¸ºæ˜¯å…·ä½“è·¯å¾„
                if path_part and path_part not in ['', '*']:
                    return True
                    
        return False

    def extract_domain_from_rule(self, rule: str) -> str:
        """
        ä»è§„åˆ™ä¸­æå–åŸŸå
        ä¿®å¤ç‰ˆæœ¬ï¼šæ›´å¥½åœ°å¤„ç†è·¯å¾„è§„åˆ™

        Args:
            rule: è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            åŸŸåæˆ– None
        """
        rule = rule.strip()

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“è·¯å¾„
        if self._has_specific_path(rule):
            # å¯¹äºåŒ…å«å…·ä½“è·¯å¾„çš„è§„åˆ™ï¼Œéœ€è¦æ›´è°¨æ…åœ°æå–åŸŸå
            return self._extract_domain_from_path_rule(rule)

        # uBlock è¯­æ³•æ¨¡å¼ - ä»…ç”¨äºåŸŸåçº§åˆ«è§„åˆ™
        patterns = [
            # *://*.domain.com/* æˆ– *://*.domain.com (é€šé…ç¬¦å­åŸŸå)
            r'^\*://\*\.([a-zA-Z0-9.-]+)(?:/\*?)?$',
            # *://domain.com/* æˆ– *://domain.com (æ— é€šé…ç¬¦)
            r'^\*://([a-zA-Z0-9.-]+)(?:/\*?)?$',
            # ||domain.com^
            r'^\|\|([a-zA-Z0-9.-]+)\^?$',
            # æ™®é€šåŸŸåæ ¼å¼
            r'^([a-zA-Z0-9.-]+)(?:/\*?)?$',
        ]

        for pattern in patterns:
            match = re.match(pattern, rule)
            if match:
                return match.group(1)

        # é€šç”¨åŸŸåæå–ï¼ˆæœ€åçš„åå¤‡æ–¹æ¡ˆï¼‰
        domain_match = re.search(r'([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', rule)
        if domain_match:
            candidate = domain_match.group(1)
            # éªŒè¯è¿™ä¸ªåŸŸåæ˜¯å¦åˆç†
            if self.is_valid_domain(candidate):
                return candidate

        return None

    def _extract_domain_from_path_rule(self, rule: str) -> str:
        """
        ä»åŒ…å«è·¯å¾„çš„è§„åˆ™ä¸­æå–åŸŸå
        è¿™ä¸ªå‡½æ•°æ›´åŠ è°¨æ…ï¼Œé¿å…è¿‡åº¦æ³›åŒ–
        
        Args:
            rule: åŒ…å«è·¯å¾„çš„è§„åˆ™å­—ç¬¦ä¸²
            
        Returns:
            åŸŸåæˆ– None
        """
        rule = rule.strip()
        
        # å¯¹äºåŒ…å«å…·ä½“è·¯å¾„çš„è§„åˆ™ï¼Œæˆ‘ä»¬é€šå¸¸ä¸æå–åŸŸå
        # é™¤éç”¨æˆ·æ˜ç¡®é…ç½®å…è®¸
        if self.config["parsing"]["ignore_specific_paths"]:
            return None
            
        # å¦‚æœç”¨æˆ·å…è®¸å¤„ç†è·¯å¾„è§„åˆ™ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼
        path_patterns = [
            # *://*.subdomain.domain.com/path/* -> æå– subdomain.domain.com
            r'^\*://\*\.([a-zA-Z0-9.-]+)/[^/]+',
            # *://subdomain.domain.com/path/* -> æå– subdomain.domain.com  
            r'^\*://([a-zA-Z0-9.-]+)/[^/]+',
        ]
        
        for pattern in path_patterns:
            match = re.match(pattern, rule)
            if match:
                domain = match.group(1)
                # åªæœ‰å½“è¿™æ˜¯ä¸€ä¸ªå­åŸŸåæ—¶æ‰è¿”å›ï¼Œé¿å…æå–ä¸»åŸŸå
                if '.' in domain and len(domain.split('.')) >= 2:
                    return domain
                    
        return None

    def parse_ublock_rule(self, rule: str) -> Tuple[str, str]:
        """
        è§£æ uBlock Origin è¯­æ³•è§„åˆ™ï¼Œæå–åŸŸå

        Args:
            rule: uBlock è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            (åŸŸåæˆ– None, å¿½ç•¥åŸå› )
        """
        rule = rule.strip()
        if not rule or rule.startswith('!') or rule.startswith('#'):
            return None, "æ³¨é‡Šæˆ–ç©ºè¡Œ"

        # å¤„ç†è¡Œæœ«æ³¨é‡Š - ç§»é™¤ # åé¢çš„æ‰€æœ‰å†…å®¹
        if '#' in rule:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª # çš„ä½ç½®ï¼Œç§»é™¤å®ƒåŠåé¢çš„å†…å®¹
            comment_pos = rule.find('#')
            rule = rule[:comment_pos].strip()

            # å¦‚æœç§»é™¤æ³¨é‡Šåè§„åˆ™ä¸ºç©ºï¼Œåˆ™å¿½ç•¥
            if not rule:
                return None, "ä»…åŒ…å«æ³¨é‡Š"

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™
        if not self.is_domain_level_rule(rule):
            if self.config["parsing"]["ignore_specific_paths"]:
                return None, "æŒ‡å‘ç‰¹å®šè·¯å¾„"

        # æå–åŸŸå
        domain = self.extract_domain_from_rule(rule)

        if domain:
            cleaned_domain = self.clean_domain(domain)
            return cleaned_domain, None if cleaned_domain else "æ— æ•ˆåŸŸå"

        return None, "æ— æ³•è§£æè§„åˆ™æ ¼å¼"

    def fetch_domain_list(self, url: str, format_type: str = "domain") -> Tuple[Set[str], Dict]:
        """
        ä»URLè·å–åŸŸååˆ—è¡¨

        Args:
            url: åŸŸååˆ—è¡¨URL
            format_type: æ ¼å¼ç±»å‹ï¼Œ"domain" æˆ– "ublock"

        Returns:
            (åŸŸåé›†åˆ, ç»Ÿè®¡ä¿¡æ¯)
        """
        domains = set()
        stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0
        }

        retry_count = self.config["request_config"]["retry_count"]
        timeout = self.config["request_config"]["timeout"]
        retry_delay = self.config["request_config"]["retry_delay"]

        for attempt in range(retry_count):
            try:
                print(f"æ­£åœ¨è·å– {url} (å°è¯• {attempt + 1}/{retry_count})")

                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }

                response = requests.get(url, timeout=timeout, headers=headers)
                response.raise_for_status()

                # è®°å½•ä¸€äº›è¢«å¿½ç•¥çš„è§„åˆ™ç”¨äºè°ƒè¯•
                ignored_samples = []
                accepted_samples = []
                comment_samples = []
                path_samples = []  # æ–°å¢ï¼šè·¯å¾„è§„åˆ™æ ·æœ¬

                # è§£æåŸŸå
                for line_num, line in enumerate(response.text.strip().split('\n'), 1):
                    line = line.strip()
                    if not line:
                        continue

                    stats['total_rules'] += 1

                    try:
                        if format_type == "ublock":
                            # ä½¿ç”¨ uBlock è¯­æ³•è§£æ
                            domain, ignore_reason = self.parse_ublock_rule(line)
                        else:
                            # æ™®é€šåŸŸåæ ¼å¼ - ä¹Ÿéœ€è¦å¤„ç†è¡Œæœ«æ³¨é‡Š
                            cleaned_line = line
                            if '#' in line:
                                cleaned_line = line[:line.find('#')].strip()
                                if not cleaned_line:
                                    domain, ignore_reason = None, "ä»…åŒ…å«æ³¨é‡Š"
                                else:
                                    if self.config["parsing"]["ignore_specific_paths"] and not self.is_domain_level_rule(cleaned_line):
                                        domain, ignore_reason = None, "æŒ‡å‘ç‰¹å®šè·¯å¾„"
                                    else:
                                        domain = self.clean_domain(self.extract_domain_from_rule(cleaned_line))
                                        ignore_reason = None if domain else "æ— æ•ˆåŸŸå"
                            else:
                                if self.config["parsing"]["ignore_specific_paths"] and not self.is_domain_level_rule(cleaned_line):
                                    domain, ignore_reason = None, "æŒ‡å‘ç‰¹å®šè·¯å¾„"
                                else:
                                    domain = self.clean_domain(self.extract_domain_from_rule(cleaned_line))
                                    ignore_reason = None if domain else "æ— æ•ˆåŸŸå"

                        if domain:
                            if domain in domains:
                                stats['duplicate_domains'] += 1
                            else:
                                domains.add(domain)
                                stats['parsed_domains'] += 1
                                # è®°å½•ä¸€äº›è¢«æ¥å—çš„è§„åˆ™æ ·æœ¬
                                if len(accepted_samples) < 3:
                                    accepted_samples.append(f"{line} -> {domain}")
                        else:
                            # ç»Ÿè®¡å¿½ç•¥åŸå› 
                            if ignore_reason == "æŒ‡å‘ç‰¹å®šè·¯å¾„":
                                stats['ignored_with_path'] += 1
                                # è®°å½•ä¸€äº›è¢«å¿½ç•¥çš„è·¯å¾„è§„åˆ™æ ·æœ¬
                                if len(path_samples) < 3:
                                    path_samples.append(line)
                            elif ignore_reason in ["æ³¨é‡Šæˆ–ç©ºè¡Œ", "ä»…åŒ…å«æ³¨é‡Š"]:
                                stats['ignored_comments'] += 1
                                if len(comment_samples) < 3:
                                    comment_samples.append(line)
                            elif ignore_reason == "æ— æ•ˆåŸŸå":
                                stats['invalid_domains'] += 1
                                if len(ignored_samples) < 3:
                                    ignored_samples.append(line)

                    except Exception as e:
                        print(f"è§£æç¬¬ {line_num} è¡Œæ—¶å‡ºé”™: {line[:50]}... - {e}")
                        stats['invalid_domains'] += 1
                        continue

                print(f"æˆåŠŸè·å– {len(domains)} ä¸ªåŸŸå")
                print(f"  - æ€»è§„åˆ™: {stats['total_rules']}")
                print(f"  - æˆåŠŸè§£æ: {stats['parsed_domains']}")
                print(f"  - å¿½ç•¥(ç‰¹å®šè·¯å¾„): {stats['ignored_with_path']}")
                print(f"  - å¿½ç•¥(æ³¨é‡Š): {stats['ignored_comments']}")
                print(f"  - å¿½ç•¥(æ— æ•ˆåŸŸå): {stats['invalid_domains']}")
                print(f"  - é‡å¤åŸŸå: {stats['duplicate_domains']}")

                # æ˜¾ç¤ºæ ·æœ¬
                if accepted_samples:
                    print(f"  - æ¥å—çš„è§„åˆ™æ ·æœ¬:")
                    for sample in accepted_samples:
                        print(f"    âœ“ {sample}")

                if path_samples:
                    print(f"  - å¿½ç•¥çš„è·¯å¾„è§„åˆ™æ ·æœ¬:")
                    for sample in path_samples:
                        print(f"    ğŸ›¤ï¸  {sample}")

                if comment_samples:
                    print(f"  - å¿½ç•¥çš„æ³¨é‡Šè§„åˆ™æ ·æœ¬:")
                    for sample in comment_samples:
                        print(f"    # {sample}")

                if ignored_samples:
                    print(f"  - å…¶ä»–å¿½ç•¥çš„è§„åˆ™æ ·æœ¬:")
                    for sample in ignored_samples:
                        print(f"    âœ— {sample}")

                return domains, stats

            except requests.RequestException as e:
                print(f"è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}")
                if attempt < retry_count - 1:
                    time.sleep(retry_delay)
                else:
                    print(f"æ”¾å¼ƒè·å– {url}")

        return domains, stats

    def clean_domain(self, domain: str) -> str:
        """
        æ¸…ç†åŸŸåå­—ç¬¦ä¸²

        Args:
            domain: åŸå§‹åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ¸…ç†åçš„åŸŸå
        """
        if not domain:
            return None

        # ç§»é™¤åè®®
        if domain.startswith(('http://', 'https://')):
            domain = urlparse(domain).netloc

        # ç§»é™¤ç«¯å£
        if ':' in domain:
            domain = domain.split(':')[0]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è·¯å¾„ï¼ˆè¿™é‡Œä¸åº”è¯¥æœ‰ï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
        if '/' in domain:
            domain = domain.split('/')[0]

        # ç§»é™¤ www. å‰ç¼€
        if domain.startswith('www.'):
            domain = domain[4:]

        # ç§»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        domain = re.sub(r'[^\w.-]', '', domain)

        # æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€
        if self.config["parsing"]["ignore_ip"] and self.is_ip_address(domain):
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯localhost
        if self.config["parsing"]["ignore_localhost"] and domain in ['localhost', '127.0.0.1', '0.0.0.0']:
            return None

        # éªŒè¯åŸŸåæ ¼å¼
        if self.is_valid_domain(domain):
            return domain.lower()

        return None

    def is_ip_address(self, domain: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€

        Args:
            domain: åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦æ˜¯IPåœ°å€
        """
        ip_pattern = re.compile(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')
        return bool(ip_pattern.match(domain))

    def is_valid_domain(self, domain: str) -> bool:
        """
        éªŒè¯åŸŸåæ ¼å¼

        Args:
            domain: åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦ä¸ºæœ‰æ•ˆåŸŸå
        """
        if not domain or len(domain) > 255:
            return False

        # åŸºæœ¬çš„åŸŸåæ ¼å¼éªŒè¯
        domain_pattern = re.compile(
            r'^(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)*[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$'
        )

        return bool(domain_pattern.match(domain))

    def domain_to_regex(self, domain: str) -> str:
        """
        å°†åŸŸåè½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼

        Args:
            domain: åŸŸå

        Returns:
            æ­£åˆ™è¡¨è¾¾å¼å­—ç¬¦ä¸²
        """
        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
        escaped_domain = re.escape(domain)
        # æ·»åŠ å­åŸŸååŒ¹é…
        return f'(.*\.)?{escaped_domain}$'

    def smart_sort_domains(self, domains: Set[str]) -> List[str]:
        """
        æ™ºèƒ½æ’åºåŸŸåï¼Œä¾¿äºåç»­åˆå¹¶
        å…ˆæŒ‰TLDæ’åºï¼Œå†æŒ‰åŸŸåä¸»ä½“æ’åº

        Args:
            domains: åŸŸåé›†åˆ

        Returns:
            æ’åºåçš„åŸŸååˆ—è¡¨
        """
        def domain_sort_key(domain: str) -> Tuple[str, str]:
            """
            ç”ŸæˆåŸŸåæ’åºé”®ï¼š(TLD, åå‘åŸŸåä¸»ä½“)
            è¿™æ ·å¯ä»¥å°†åŒTLDçš„åŸŸåèšé›†åœ¨ä¸€èµ·ï¼Œä¾¿äºåˆå¹¶
            """
            parts = domain.split('.')
            if len(parts) >= 2:
                # TLD ä½œä¸ºä¸»è¦æ’åºé”®
                tld = parts[-1]
                # åŸŸåä¸»ä½“ä½œä¸ºæ¬¡è¦æ’åºé”®ï¼Œåå‘æ’åºä¾¿äºæ‰¾åˆ°å…¬å…±åç¼€
                base = '.'.join(parts[:-1])
                return (tld, base)
            else:
                return (domain, '')

        if self.config["optimization"].get("sort_before_merge", True):
            sorted_domains = sorted(list(domains), key=domain_sort_key)
            print(f"  ğŸ”„ åŸŸåå·²æŒ‰TLDæ™ºèƒ½æ’åºï¼Œä¾¿äºåˆå¹¶ä¼˜åŒ–")
            return sorted_domains
        else:
            return list(domains)

    def group_domains_by_tld(self, domains: Union[Set[str], List[str]]) -> Dict[str, List[str]]:
        """
        æŒ‰é¡¶çº§åŸŸååˆ†ç»„åŸŸåï¼Œå¹¶ä¿æŒæ’åº

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            æŒ‰TLDåˆ†ç»„çš„åŸŸåå­—å…¸ï¼Œå€¼ä¸ºæ’åºåçš„åˆ—è¡¨
        """
        tld_groups = defaultdict(list)

        # å¦‚æœè¾“å…¥æ˜¯é›†åˆï¼Œå…ˆè½¬æ¢ä¸ºæ™ºèƒ½æ’åºçš„åˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        for domain in domains:
            parts = domain.split('.')
            if len(parts) >= 2:
                # è·å–é¡¶çº§åŸŸåï¼ˆå¦‚ .com, .orgï¼‰
                tld = parts[-1]
                tld_groups[tld].append(domain)
            else:
                # å¤„ç†æ— æ•ˆåŸŸå
                tld_groups['other'].append(domain)

        return dict(tld_groups)

    def get_domain_base_and_tld(self, domain: str) -> Tuple[str, str]:
        """
        æå–åŸŸåçš„ä¸»ä½“éƒ¨åˆ†å’ŒTLD

        Args:
            domain: å®Œæ•´åŸŸå

        Returns:
            (åŸŸåä¸»ä½“, TLD)
        """
        parts = domain.split('.')
        if len(parts) >= 2:
            tld = parts[-1]
            base = '.'.join(parts[:-1])
            return base, tld
        return domain, ''

    def find_common_prefix(self, strings: List[str]) -> str:
        """
        æ‰¾åˆ°å­—ç¬¦ä¸²åˆ—è¡¨çš„å…¬å…±å‰ç¼€

        Args:
            strings: å­—ç¬¦ä¸²åˆ—è¡¨

        Returns:
            å…¬å…±å‰ç¼€
        """
        if not strings:
            return ""

        strings = [s for s in strings if s]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not strings:
            return ""

        min_len = min(len(s) for s in strings)
        prefix = ""

        for i in range(min_len):
            char = strings[0][i]
            if all(s[i] == char for s in strings):
                prefix += char
            else:
                break

        return prefix

    def find_common_suffix(self, strings: List[str]) -> str:
        """
        æ‰¾åˆ°å­—ç¬¦ä¸²åˆ—è¡¨çš„å…¬å…±åç¼€

        Args:
            strings: å­—ç¬¦ä¸²åˆ—è¡¨

        Returns:
            å…¬å…±åç¼€
        """
        if not strings:
            return ""

        strings = [s for s in strings if s]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not strings:
            return ""

        # åè½¬å­—ç¬¦ä¸²ï¼Œæ‰¾å‰ç¼€ï¼Œå†åè½¬å›æ¥
        reversed_strings = [s[::-1] for s in strings]
        reversed_suffix = self.find_common_prefix(reversed_strings)
        return reversed_suffix[::-1]

    def create_advanced_tld_regex(self, tld_domains: List[str], tld: str) -> str:
        """
        ä¸ºåŒä¸€TLDçš„åŸŸååˆ›å»ºé«˜çº§ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼

        Args:
            tld_domains: åŒä¸€TLDçš„åŸŸååˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
            tld: é¡¶çº§åŸŸå

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        if len(tld_domains) == 1:
            return re.escape(tld_domains[0])

        # æå–åŸŸåä¸»ä½“éƒ¨åˆ†
        domain_bases = []
        for domain in tld_domains:
            base, domain_tld = self.get_domain_base_and_tld(domain)
            if domain_tld == tld:
                domain_bases.append(base)
            else:
                # TLDä¸åŒ¹é…çš„æƒ…å†µï¼Œä½¿ç”¨å®Œæ•´åŸŸå
                domain_bases.append(domain)

        if not domain_bases:
            return '|'.join(re.escape(d) for d in tld_domains)

        # å°è¯•æ‰¾åˆ°å…¬å…±æ¨¡å¼
        optimized_pattern = self.optimize_domain_bases(domain_bases)

        # å¦‚æœæ‰€æœ‰åŸŸååŸºç¡€éƒ¨åˆ†éƒ½ä¸åŒ…å«ç‚¹ï¼Œè¯´æ˜éƒ½æ˜¯äºŒçº§åŸŸåï¼Œå¯ä»¥è¿›è¡ŒTLDä¼˜åŒ–
        if all('.' not in base for base in domain_bases):
            return f"({optimized_pattern})\\.{re.escape(tld)}"
        else:
            # æ··åˆæƒ…å†µï¼Œæœ‰äº›æ˜¯å¤šçº§åŸŸåï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´åŸŸå
            return optimized_pattern

    def optimize_domain_bases(self, domain_bases: List[str]) -> str:
        """
        ä¼˜åŒ–åŸŸååŸºç¡€éƒ¨åˆ†åˆ—è¡¨

        Args:
            domain_bases: åŸŸååŸºç¡€éƒ¨åˆ†åˆ—è¡¨

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        """
        if len(domain_bases) <= 1:
            return '|'.join(re.escape(base) for base in domain_bases)

        optimization_config = self.config["optimization"]

        # å°è¯•å‰ç¼€ä¼˜åŒ–
        if optimization_config.get("enable_prefix_optimization", True):
            common_prefix = self.find_common_prefix(domain_bases)
            min_prefix_len = optimization_config.get("min_common_prefix_length", 3)

            if len(common_prefix) >= min_prefix_len:
                # ç§»é™¤å…¬å…±å‰ç¼€
                suffixes = [base[len(common_prefix):] for base in domain_bases]
                suffixes = [s for s in suffixes if s]  # è¿‡æ»¤ç©ºåç¼€
                if suffixes and len(set(suffixes)) > 1:  # ç¡®ä¿æœ‰ä¸åŒçš„åç¼€
                    suffix_pattern = self.optimize_domain_bases(suffixes)
                    return f"{re.escape(common_prefix)}({suffix_pattern})"

        # å°è¯•åç¼€ä¼˜åŒ–
        if optimization_config.get("enable_suffix_optimization", True):
            common_suffix = self.find_common_suffix(domain_bases)
            min_suffix_len = optimization_config.get("min_common_suffix_length", 3)

            if len(common_suffix) >= min_suffix_len:
                # ç§»é™¤å…¬å…±åç¼€
                prefixes = [base[:-len(common_suffix)] for base in domain_bases]
                prefixes = [p for p in prefixes if p]  # è¿‡æ»¤ç©ºå‰ç¼€
                if prefixes and len(set(prefixes)) > 1:  # ç¡®ä¿æœ‰ä¸åŒçš„å‰ç¼€
                    prefix_pattern = self.optimize_domain_bases(prefixes)
                    return f"({prefix_pattern}){re.escape(common_suffix)}"

        # æ²¡æœ‰æ‰¾åˆ°ä¼˜åŒ–æ¨¡å¼ï¼Œç›´æ¥è¿æ¥
        return '|'.join(re.escape(base) for base in domain_bases)

    def create_single_regex_rule(self, domains: Union[Set[str], List[str]]) -> str:
        """
        åˆ›å»ºåŒ…å«æ‰€æœ‰åŸŸåçš„å•è¡Œæ­£åˆ™è¡¨è¾¾å¼ï¼ˆé«˜çº§TLDä¼˜åŒ–ï¼‰

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            å•è¡Œæ­£åˆ™è¡¨è¾¾å¼
        """
        if not domains:
            return ""

        # è½¬æ¢ä¸ºæ’åºåˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"

        print(f"ğŸš€ æ­£åœ¨ç”Ÿæˆé«˜çº§TLDä¼˜åŒ–å•è¡Œæ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ…å« {len(domains)} ä¸ªåŸŸå")

        # å¯ç”¨é«˜çº§TLDåˆå¹¶
        if self.config["optimization"].get("enable_advanced_tld_merge", True):
            # æŒ‰TLDåˆ†ç»„
            tld_groups = self.group_domains_by_tld(domains)
            tld_patterns = []

            print(f"  ğŸ“Š TLDåˆ†å¸ƒæƒ…å†µ:")
            for tld, tld_domains in sorted(tld_groups.items(), key=lambda x: len(x[1]), reverse=True):
                print(f"    .{tld}: {len(tld_domains)} ä¸ªåŸŸå")

            for tld, tld_domains in tld_groups.items():
                if len(tld_domains) == 1:
                    # å•ä¸ªåŸŸåç›´æ¥å¤„ç†
                    domain = tld_domains[0]
                    tld_patterns.append(re.escape(domain))
                else:
                    # å¤šä¸ªåŸŸåè¿›è¡Œé«˜çº§ä¼˜åŒ–
                    optimized_pattern = self.create_advanced_tld_regex(tld_domains, tld)
                    tld_patterns.append(optimized_pattern)
                    print(f"  âœ… TLD .{tld}: {len(tld_domains)} ä¸ªåŸŸåå·²ä¼˜åŒ–åˆå¹¶")

            # åˆå¹¶æ‰€æœ‰TLDç»„çš„æ¨¡å¼
            if len(tld_patterns) == 1:
                combined_pattern = tld_patterns[0]
            else:
                combined_pattern = f"({'|'.join(tld_patterns)})"

            single_regex = f"(.*\\.)?{combined_pattern}$"
        else:
            # ç®€å•åˆå¹¶æ¨¡å¼
            escaped_domains = [re.escape(d) for d in domains]
            combined_pattern = '|'.join(escaped_domains)
            single_regex = f"(.*\\.)?({combined_pattern})$"

        # æ˜¾ç¤ºè§„åˆ™é•¿åº¦ä¿¡æ¯
        rule_length = len(single_regex)
        print(f"  ğŸ“ ç”Ÿæˆçš„å•è¡Œè§„åˆ™é•¿åº¦: {rule_length:,} å­—ç¬¦")

        if rule_length > 100000:
            print("  âš ï¸  è§„åˆ™æé•¿ï¼Œå¯èƒ½ä¸¥é‡å½±å“æ€§èƒ½ï¼Œå»ºè®®åˆ†å‰²")
        elif rule_length > 50000:
            print("  âš ï¸  è§„åˆ™å¾ˆé•¿ï¼Œå¯èƒ½å½±å“åŒ¹é…æ€§èƒ½")
        elif rule_length > 10000:
            print("  âš ï¸  è§„åˆ™è¾ƒé•¿ï¼Œè¯·æ³¨æ„æ€§èƒ½")
        else:
            print("  âœ… è§„åˆ™é•¿åº¦é€‚ä¸­")

        return single_regex

    def create_multiple_optimized_rules(self, domains: Union[Set[str], List[str]]) -> List[str]:
        """
        åˆ›å»ºå¤šä¸ªä¼˜åŒ–çš„è§„åˆ™ï¼ˆéå•è¡Œæ¨¡å¼ï¼‰

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            ä¼˜åŒ–åçš„è§„åˆ™åˆ—è¡¨
        """
        if not domains:
            return []

        # è½¬æ¢ä¸ºæ’åºåˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        optimization_config = self.config["optimization"]
        max_domains_per_rule = optimization_config.get("max_domains_per_rule", 30)
        max_rule_length = optimization_config.get("max_rule_length", 4000)

        # æŒ‰TLDåˆ†ç»„å¤„ç†
        if optimization_config.get("group_by_tld", True):
            tld_groups = self.group_domains_by_tld(domains)
            rules = []

            for tld, tld_domains in tld_groups.items():
                if len(tld_domains) <= 1:
                    # å•ä¸ªåŸŸåç›´æ¥è½¬æ¢
                    rules.extend([self.domain_to_regex(domain) for domain in tld_domains])
                else:
                    # å¤šä¸ªåŸŸååˆ†æ‰¹å¤„ç†
                    tld_rules = self._create_batched_rules(tld_domains, tld, max_domains_per_rule, max_rule_length)
                    rules.extend(tld_rules)
                    print(f"  ğŸ“¦ TLD .{tld}: {len(tld_domains)} ä¸ªåŸŸå -> {len(tld_rules)} ä¸ªè§„åˆ™")

            return rules
        else:
            # ä¸åˆ†ç»„ï¼Œç›´æ¥åˆ†æ‰¹å¤„ç†
            return self._create_batched_rules(domains, None, max_domains_per_rule, max_rule_length)

    def _create_batched_rules(self, domains: List[str], tld: str = None, max_domains_per_rule: int = 30, max_rule_length: int = 4000) -> List[str]:
        """
        ä¸ºåŸŸååˆ—è¡¨åˆ›å»ºåˆ†æ‰¹çš„è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨
            tld: é¡¶çº§åŸŸåï¼ˆå¯é€‰ï¼Œç”¨äºä¼˜åŒ–ï¼‰
            max_domains_per_rule: æ¯ä¸ªè§„åˆ™çš„æœ€å¤§åŸŸåæ•°
            max_rule_length: æœ€å¤§è§„åˆ™é•¿åº¦

        Returns:
            åˆ†æ‰¹åçš„è§„åˆ™åˆ—è¡¨
        """
        rules = []
        current_batch = []

        for domain in domains:
            test_batch = current_batch + [domain]

            # åˆ›å»ºæµ‹è¯•è§„åˆ™
            if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                test_rule = self._create_tld_optimized_rule(test_batch, tld)
            else:
                test_rule = self._create_simple_rule(test_batch)

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if (len(test_batch) > max_domains_per_rule or
                len(test_rule) > max_rule_length):

                # ä¿å­˜å½“å‰æ‰¹æ¬¡
                if current_batch:
                    if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                        rules.append(self._create_tld_optimized_rule(current_batch, tld))
                    else:
                        rules.append(self._create_simple_rule(current_batch))

                # å¼€å§‹æ–°æ‰¹æ¬¡
                current_batch = [domain]
            else:
                current_batch.append(domain)

        # å¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                rules.append(self._create_tld_optimized_rule(current_batch, tld))
            else:
                rules.append(self._create_simple_rule(current_batch))

        return rules

    def _create_tld_optimized_rule(self, domains: List[str], tld: str) -> str:
        """
        ä¸ºåŒTLDåŸŸååˆ›å»ºä¼˜åŒ–è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨
            tld: é¡¶çº§åŸŸå

        Returns:
            TLDä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™
        """
        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"

        optimized_pattern = self.create_advanced_tld_regex(domains, tld)
        return f"(.*\\.)?{optimized_pattern}$"

    def _create_simple_rule(self, domains: List[str]) -> str:
        """
        ä¸ºåŸŸååˆ—è¡¨åˆ›å»ºç®€å•è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨

        Returns:
            ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™
        """
        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"
        else:
            pattern = self.optimize_domain_bases(domains)
            return f"(.*\\.)?({pattern})$"

    def merge_domains_to_regex(self, domains: Set[str]) -> List[str]:
        """
        å°†å¤šä¸ªåŸŸååˆå¹¶ä¸ºä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨

        Args:
            domains: åŸŸåé›†åˆ

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨
        """
        if not domains:
            return []

        # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™
        if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
            print(f"ğŸš€ å¯ç”¨å¼ºåˆ¶å•è¡Œæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼")
            single_rule = self.create_single_regex_rule(domains)
            return [single_rule] if single_rule else []

        optimization_config = self.config["optimization"]

        # å¦‚æœæœªå¯ç”¨åˆå¹¶ä¼˜åŒ–ï¼Œè¿”å›å•ç‹¬çš„è§„åˆ™
        if not optimization_config.get("merge_domains", True):
            return [self.domain_to_regex(domain) for domain in domains]

        # ä½¿ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼
        print(f"ğŸ”§ å¯ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼ï¼Œå¤„ç† {len(domains)} ä¸ªåŸŸå")
        rules = self.create_multiple_optimized_rules(domains)

        print(f"  âœ… ä¼˜åŒ–å®Œæˆ: {len(domains)} ä¸ªåŸŸå -> {len(rules)} ä¸ªè§„åˆ™")
        return rules

    def collect_domains(self) -> Dict[str, Set[str]]:
        """
        ä»æ‰€æœ‰é…ç½®çš„æºæ”¶é›†åŸŸå

        Returns:
            æŒ‰åŠ¨ä½œåˆ†ç±»çš„åŸŸåé›†åˆ
        """
        categorized_domains = {
            'remove': set(),
            'low_priority': set(),
            'high_priority': set()
        }

        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0
        }

        for source in self.config["sources"]:
            if not source.get("enabled", True):
                continue

            print(f"\nå¤„ç†æ•°æ®æº: {source['name']}")
            format_type = source.get("format", "domain")
            print(f"æ ¼å¼ç±»å‹: {format_type}")

            domains, source_stats = self.fetch_domain_list(source["url"], format_type)

            # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
            for key in self.stats:
                self.stats[key] += source_stats[key]

            action = source.get("action", "remove")
            if action in categorized_domains:
                categorized_domains[action].update(domains)
                print(f"å·²æ·»åŠ  {len(domains)} ä¸ªåŸŸååˆ° {action} ç±»åˆ«")
            else:
                print(f"è­¦å‘Š: æœªçŸ¥çš„åŠ¨ä½œç±»å‹ '{action}'")

        return categorized_domains

    def sort_rules(self, rules: Union[Dict, List]) -> Union[Dict, List]:
        """
        å¯¹è§„åˆ™è¿›è¡Œæ’åº

        Args:
            rules: è§„åˆ™æ•°æ®

        Returns:
            æ’åºåçš„è§„åˆ™
        """
        if isinstance(rules, dict):
            # å¯¹å­—å…¸æŒ‰é”®æ’åº
            return OrderedDict(sorted(rules.items()))
        elif isinstance(rules, list):
            # å¯¹åˆ—è¡¨æŒ‰å€¼æ’åº
            return sorted(rules)
        else:
            return rules

    def generate_rules(self) -> Dict[str, any]:
        """
        ç”Ÿæˆå„ç±»è§„åˆ™

        Returns:
            è§„åˆ™å­—å…¸
        """
        print("\nå¼€å§‹ç”Ÿæˆ SearXNG hostnames è§„åˆ™...")

        # æ˜¾ç¤ºä¼˜åŒ–æ¨¡å¼ä¿¡æ¯
        if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
            print("ğŸš€ å·²å¯ç”¨é«˜çº§TLDä¼˜åŒ–å•è¡Œæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼")
            print("   æ¯ä¸ªç±»åˆ«å°†ç”Ÿæˆå•ä¸ªåŒ…å«æ‰€æœ‰åŸŸåçš„é«˜çº§ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼")
        else:
            print("ğŸ”§ å·²å¯ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼")
            print("   å°†ç”Ÿæˆå¤šä¸ªæ€§èƒ½ä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™")

        # æ˜¾ç¤ºè§£æé…ç½®
        parsing_config = self.config["parsing"]
        print(f"ğŸ“ è§£æé…ç½®:")
        print(f"   - å¿½ç•¥ç‰¹å®šè·¯å¾„è§„åˆ™: {parsing_config.get('ignore_specific_paths', True)}")
        print(f"   - ä¸¥æ ¼åŸŸåçº§åˆ«æ£€æŸ¥: {parsing_config.get('strict_domain_level_check', True)}")

        # æ”¶é›†åŸŸå
        categorized_domains = self.collect_domains()

        rules = {}

        # æ›¿æ¢è§„åˆ™ (å­—å…¸æ ¼å¼)
        if self.config["replace_rules"]:
            rules["replace"] = self.config["replace_rules"]
            # è®°å½•æ›¿æ¢è§„åˆ™çš„åŸŸåæ•°é‡ï¼ˆæ›¿æ¢è§„åˆ™æ˜¯é”®å€¼å¯¹ï¼Œé”®å°±æ˜¯åŸŸåè§„åˆ™ï¼‰
            self.category_domain_counts["replace"] = len(self.config["replace_rules"])

        # ç§»é™¤è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆç§»é™¤è§„åˆ™...")
        remove_rules = []
        remove_rules.extend(self.config["fixed_remove"])

        # è®°å½•å›ºå®šç§»é™¤è§„åˆ™æ•°é‡
        fixed_remove_count = len(self.config["fixed_remove"])

        if categorized_domains["remove"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['remove'])} ä¸ªç§»é™¤åŸŸå...")
            self.category_domain_counts["remove"] = len(categorized_domains["remove"]) + fixed_remove_count
            merged_remove_rules = self.merge_domains_to_regex(categorized_domains["remove"])
            remove_rules.extend(merged_remove_rules)
        else:
            self.category_domain_counts["remove"] = fixed_remove_count

        if remove_rules:
            rules["remove"] = remove_rules

        # ä½ä¼˜å…ˆçº§è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆä½ä¼˜å…ˆçº§è§„åˆ™...")
        low_priority_rules = []
        low_priority_rules.extend(self.config["fixed_low_priority"])

        # è®°å½•å›ºå®šä½ä¼˜å…ˆçº§è§„åˆ™æ•°é‡
        fixed_low_priority_count = len(self.config["fixed_low_priority"])

        if categorized_domains["low_priority"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['low_priority'])} ä¸ªä½ä¼˜å…ˆçº§åŸŸå...")
            self.category_domain_counts["low_priority"] = len(categorized_domains["low_priority"]) + fixed_low_priority_count
            merged_low_priority_rules = self.merge_domains_to_regex(categorized_domains["low_priority"])
            low_priority_rules.extend(merged_low_priority_rules)
        else:
            self.category_domain_counts["low_priority"] = fixed_low_priority_count

        if low_priority_rules:
            rules["low_priority"] = low_priority_rules

        # é«˜ä¼˜å…ˆçº§è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆé«˜ä¼˜å…ˆçº§è§„åˆ™...")
        high_priority_rules = []
        high_priority_rules.extend(self.config["fixed_high_priority"])

        # è®°å½•å›ºå®šé«˜ä¼˜å…ˆçº§è§„åˆ™æ•°é‡
        fixed_high_priority_count = len(self.config["fixed_high_priority"])

        if categorized_domains["high_priority"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['high_priority'])} ä¸ªé«˜ä¼˜å…ˆçº§åŸŸå...")
            self.category_domain_counts["high_priority"] = len(categorized_domains["high_priority"]) + fixed_high_priority_count
            merged_high_priority_rules = self.merge_domains_to_regex(categorized_domains["high_priority"])
            high_priority_rules.extend(merged_high_priority_rules)
        else:
            self.category_domain_counts["high_priority"] = fixed_high_priority_count

        if high_priority_rules:
            rules["high_priority"] = high_priority_rules

        # å¯¹æ‰€æœ‰è§„åˆ™è¿›è¡Œæ’åºå’Œå»é‡
        for rule_type in rules:
            if rule_type == "replace":
                # æ›¿æ¢è§„åˆ™æŒ‰é”®æ’åº
                rules[rule_type] = self.sort_rules(rules[rule_type])
            else:
                # åˆ—è¡¨è§„åˆ™å»é‡å¹¶æ’åº
                rules[rule_type] = self.sort_rules(list(set(rules[rule_type])))

        return rules

    def save_separate_files(self, rules: Dict[str, any]) -> None:
        """
        ä¿å­˜ä¸ºåˆ†ç¦»çš„æ–‡ä»¶

        Args:
            rules: è§„åˆ™å­—å…¸
        """
        output_dir = self.config["output"]["directory"]
        files_config = self.config["output"]["files"]

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # ç”Ÿæˆä¸»é…ç½®æ–‡ä»¶ (ç”¨äºå¼•ç”¨å¤–éƒ¨æ–‡ä»¶)
        main_config = {"hostnames": {}}

        # ä¿å­˜å„ç±»è§„åˆ™åˆ°å•ç‹¬æ–‡ä»¶
        for rule_type, rule_data in rules.items():
            if rule_type in files_config and rule_data:
                filename = files_config[rule_type]
                filepath = os.path.join(output_dir, filename)

                try:
                    with open(filepath, 'w', encoding='utf-8') as f:
                        # æ·»åŠ æ–‡ä»¶å¤´æ³¨é‡Š
                        rule_count = len(rule_data) if isinstance(rule_data, (list, dict)) else 0
                        domain_count = self.category_domain_counts.get(rule_type, 0)

                        f.write(f"# SearXNG {rule_type} rules\n")
                        f.write(f"# Generated by SearXNG Hostnames Generator (Enhanced - Fixed Path Rules)\n")
                        f.write(f"# Total rules: {rule_count}\n")
                        f.write(f"# Total domains: {domain_count}\n")

                        if domain_count > 0 and rule_count > 0:
                            compression_ratio = (rule_count / domain_count) * 100
                            f.write(f"# Compression ratio: {compression_ratio:.1f}% ({domain_count} domains -> {rule_count} rules)\n")

                        # æ ¹æ®æ˜¯å¦å¯ç”¨å•è¡Œæ­£åˆ™æ˜¾ç¤ºä¸åŒæ³¨é‡Š
                        if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
                            f.write(f"# Advanced TLD-optimized single-line regex mode enabled\n")
                        else:
                            f.write(f"# Multi-rule performance optimized with advanced pattern matching\n")

                        f.write(f"# Note: Rules targeting specific paths are ignored to prevent over-blocking\n")
                        f.write(f"# Fixed: Improved path rule detection to avoid domain extraction issues\n")
                        f.write(f"# Smart domain sorting and TLD grouping applied for optimal performance\n\n")

                        # ç›´æ¥å†™å…¥è§„åˆ™å†…å®¹ï¼Œä¸åŒ…å«é¡¶çº§é”®
                        yaml.dump(rule_data, f, default_flow_style=False, allow_unicode=True, indent=2)

                    print(f"å·²ä¿å­˜ {rule_type} è§„åˆ™åˆ°: {filepath}")

                    # åœ¨ä¸»é…ç½®ä¸­å¼•ç”¨å¤–éƒ¨æ–‡ä»¶
                    main_config["hostnames"][rule_type] = filename

                except Exception as e:
                    print(f"ä¿å­˜ {rule_type} è§„åˆ™å¤±è´¥: {e}")

        # ä¿å­˜ä¸»é…ç½®æ–‡ä»¶
        if main_config["hostnames"]:
            main_config_path = os.path.join(output_dir, files_config["main_config"])
            try:
                with open(main_config_path, 'w', encoding='utf-8') as f:
                    f.write("# SearXNG hostnames configuration\n")
                    f.write("# This file references external rule files\n")
                    f.write("# Generated by SearXNG Hostnames Generator (Enhanced - Fixed Path Rules)\n")

                    if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
                        f.write("# Advanced TLD-optimized single-line regex mode enabled\n")
                    else:
                        f.write("# Multi-rule performance optimized with advanced pattern matching\n")

                    f.write("# Fixed: Improved path rule detection to avoid CSDN-like issues\n")
                    f.write("# Smart domain sorting and TLD grouping applied\n\n")
                    yaml.dump(main_config, f, default_flow_style=False, allow_unicode=True, indent=2)
                print(f"å·²ä¿å­˜ä¸»é…ç½®åˆ°: {main_config_path}")
            except Exception as e:
                print(f"ä¿å­˜ä¸»é…ç½®å¤±è´¥: {e}")

    def save_single_file(self, rules: Dict[str, any]) -> None:
        """
        ä¿å­˜ä¸ºå•ä¸ªæ–‡ä»¶

        Args:
            rules: è§„åˆ™å­—å…¸
        """
        output_dir = self.config["output"]["directory"]
        os.makedirs(output_dir, exist_ok=True)

        # æ„å»ºå®Œæ•´çš„ hostnames é…ç½®
        hostnames_config = {"hostnames": rules}

        filepath = os.path.join(output_dir, "hostnames.yml")

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("# SearXNG hostnames configuration\n")
                f.write("# Generated by SearXNG Hostnames Generator (Enhanced - Fixed Path Rules)\n")

                # æ·»åŠ æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
                total_rules = sum(len(rule_data) if isinstance(rule_data, (list, dict)) else 0 for rule_data in rules.values())
                total_domains = sum(self.category_domain_counts.values())
                f.write(f"# Total rules: {total_rules}\n")
                f.write(f"# Total domains: {total_domains}\n")

                if total_domains > 0 and total_rules > 0:
                    compression_ratio = (total_rules / total_domains) * 100
                    f.write(f"# Overall compression ratio: {compression_ratio:.1f}%\n")

                if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
                    f.write("# Advanced TLD-optimized single-line regex mode enabled\n")
                else:
                    f.write("# Multi-rule performance optimized with advanced pattern matching\n")

                f.write("# Note: Rules targeting specific paths are ignored to prevent over-blocking\n")
                f.write("# Fixed: Improved path rule detection to avoid CSDN-like domain extraction issues\n")
                f.write("# Smart domain sorting and TLD grouping applied for optimal performance\n\n")
                yaml.dump(hostnames_config, f, default_flow_style=False, allow_unicode=True, indent=2)

            print(f"å·²ä¿å­˜å®Œæ•´é…ç½®åˆ°: {filepath}")

        except Exception as e:
            print(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")

    def run(self) -> None:
        """
        è¿è¡Œç”Ÿæˆå™¨
        """
        print("SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨å¯åŠ¨ (é«˜çº§ä¼˜åŒ–ç‰ˆ - ä¿®å¤è·¯å¾„è§„åˆ™)")
        print("=" * 70)

        try:
            # ç”Ÿæˆè§„åˆ™
            rules = self.generate_rules()

            # æ ¹æ®é…ç½®ä¿å­˜æ–‡ä»¶
            if self.config["output"]["mode"] == "separate_files":
                self.save_separate_files(rules)
            else:
                self.save_single_file(rules)

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.print_statistics(rules)

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
        except Exception as e:
            print(f"\nç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

    def print_statistics(self, rules: Dict[str, any]) -> None:
        """
        è¾“å‡ºç»Ÿè®¡ä¿¡æ¯

        Args:
            rules: ç”Ÿæˆçš„è§„åˆ™
        """
        print("\n" + "=" * 60)
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")

        total_rules = 0
        total_domains = 0

        for rule_type, rule_data in rules.items():
            if isinstance(rule_data, dict):
                rule_count = len(rule_data)
                domain_count = self.category_domain_counts.get(rule_type, 0)
                print(f"  {rule_type} è§„åˆ™: {rule_count} æ¡ (åŒ…å« {domain_count} ä¸ªåŸŸå)")
                total_rules += rule_count
                total_domains += domain_count
            elif isinstance(rule_data, list):
                rule_count = len(rule_data)
                domain_count = self.category_domain_counts.get(rule_type, 0)
                print(f"  {rule_type} è§„åˆ™: {rule_count} æ¡ (åŒ…å« {domain_count} ä¸ªåŸŸå)")
                total_rules += rule_count
                total_domains += domain_count

                # å¦‚æœæ˜¯å•è¡Œæ­£åˆ™æ¨¡å¼ï¼Œæ˜¾ç¤ºè§„åˆ™é•¿åº¦ä¿¡æ¯
                if (self.force_single_regex or self.config["optimization"].get("force_single_regex", False)) and rule_data:
                    for i, rule in enumerate(rule_data, 1):
                        rule_length = len(rule)
                        if rule_length > 1000:
                            print(f"    è§„åˆ™ {i} é•¿åº¦: {rule_length:,} å­—ç¬¦")

        print(f"\nğŸ“ˆ æ€»è®¡: {total_rules} æ¡è§„åˆ™ (åŒ…å« {total_domains} ä¸ªåŸŸå)")

        print(f"\nğŸ” è§£æç»Ÿè®¡:")
        print(f"  - æ€»è¾“å…¥è§„åˆ™: {self.stats['total_rules']:,}")
        print(f"  - æˆåŠŸè§£æåŸŸå: {self.stats['parsed_domains']:,}")
        print(f"  - å¿½ç•¥(ç‰¹å®šè·¯å¾„): {self.stats['ignored_with_path']:,}")
        print(f"  - å¿½ç•¥(æ³¨é‡Š): {self.stats['ignored_comments']:,}")
        print(f"  - å¿½ç•¥(æ— æ•ˆåŸŸå): {self.stats['invalid_domains']:,}")
        print(f"  - é‡å¤åŸŸå: {self.stats['duplicate_domains']:,}")

        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.config['output']['directory']}")

        print(f"\nğŸ“¡ æ•°æ®æº:")
        for source in self.config["sources"]:
            if source.get("enabled", True):
                print(f"  âœ… {source['name']} ({source.get('format', 'domain')})")
            else:
                print(f"  âŒ {source['name']} (å·²ç¦ç”¨)")

        print(f"\nâš™ï¸  é…ç½®:")
        print(f"  - å¿½ç•¥ç‰¹å®šè·¯å¾„è§„åˆ™: {self.config['parsing']['ignore_specific_paths']}")
        print(f"  - ä¸¥æ ¼åŸŸåçº§åˆ«æ£€æŸ¥: {self.config['parsing'].get('strict_domain_level_check', True)}")
        print(f"  - å¿½ç•¥IPåœ°å€: {self.config['parsing']['ignore_ip']}")
        print(f"  - å¿½ç•¥localhost: {self.config['parsing']['ignore_localhost']}")

        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        opt_config = self.config["optimization"]
        print(f"\nğŸš€ æ€§èƒ½ä¼˜åŒ–:")
        print(f"  - å¯ç”¨åŸŸååˆå¹¶: {opt_config.get('merge_domains', True)}")
        print(f"  - æ™ºèƒ½åŸŸåæ’åº: {opt_config.get('sort_before_merge', True)}")
        print(f"  - é«˜çº§TLDä¼˜åŒ–: {opt_config.get('enable_advanced_tld_merge', True)}")
        print(f"  - å¼ºåˆ¶å•è¡Œæ­£åˆ™: {self.force_single_regex or opt_config.get('force_single_regex', False)}")

        if not (self.force_single_regex or opt_config.get('force_single_regex', False)):
            print(f"  - æ¯è§„åˆ™æœ€å¤§åŸŸåæ•°: {opt_config.get('max_domains_per_rule', 30)}")
            print(f"  - æŒ‰TLDåˆ†ç»„: {opt_config.get('group_by_tld', True)}")
            print(f"  - æœ€å¤§è§„åˆ™é•¿åº¦: {opt_config.get('max_rule_length', 4000):,}")

        print(f"  - å‰ç¼€ä¼˜åŒ–: {opt_config.get('enable_prefix_optimization', True)}")
        print(f"  - åç¼€ä¼˜åŒ–: {opt_config.get('enable_suffix_optimization', True)}")

        print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        if self.config["output"]["mode"] == "separate_files":
            print("åœ¨ SearXNG settings.yml ä¸­æ·»åŠ :")
            print("hostnames:")
            for rule_type, filename in self.config["output"]["files"].items():
                if rule_type != "main_config" and rule_type in rules:
                    print(f"  {rule_type}: '{filename}'")
        else:
            print("å°†ç”Ÿæˆçš„ hostnames.yml å†…å®¹å¤åˆ¶åˆ° SearXNG settings.yml ä¸­")

        print(f"\nâœ¨ ä¼˜åŒ–æ•ˆæœ:")
        if total_domains > 0 and total_rules > 0:
            compression_ratio = (total_rules / total_domains) * 100
            print(f"  - å‹ç¼©æ¯”ç‡: {compression_ratio:.1f}% ({total_domains:,} ä¸ªåŸŸå -> {total_rules} æ¡è§„åˆ™)")
            if compression_ratio < 10:
                print("  - ğŸ‰ å‹ç¼©æ•ˆæœæä½³ï¼å¤§é‡åŸŸåè¢«åˆå¹¶ä¼˜åŒ–")
            elif compression_ratio < 50:
                print("  - ğŸ‘ å‹ç¼©æ•ˆæœè‰¯å¥½")
            else:
                print("  - ğŸ“ è§„åˆ™è¾ƒå¤šï¼Œå¯è€ƒè™‘å¯ç”¨å•è¡Œæ­£åˆ™æ¨¡å¼")

        # æ˜¾ç¤ºå„ç±»åˆ«çš„å‹ç¼©æƒ…å†µ
        print(f"\nğŸ“ˆ å„ç±»åˆ«å‹ç¼©è¯¦æƒ…:")
        for rule_type, rule_data in rules.items():
            if isinstance(rule_data, (list, dict)):
                rule_count = len(rule_data)
                domain_count = self.category_domain_counts.get(rule_type, 0)
                if domain_count > 0 and rule_count > 0:
                    category_ratio = (rule_count / domain_count) * 100
                    print(f"  - {rule_type}: {category_ratio:.1f}% ({domain_count} ä¸ªåŸŸå -> {rule_count} æ¡è§„åˆ™)")

        print(f"\nğŸ”§ ä¿®å¤è¯´æ˜:")
        print(f"  - ä¿®å¤äº†è·¯å¾„è§„åˆ™è¯¯åˆ¤é—®é¢˜ï¼Œå¦‚ '*://*.csdn.net/tags/*' ç°åœ¨ä¼šè¢«æ­£ç¡®å¿½ç•¥")
        print(f"  - æ”¹è¿›äº†åŸŸåæå–é€»è¾‘ï¼Œé¿å…ä»è·¯å¾„è§„åˆ™ä¸­é”™è¯¯æå–ä¸»åŸŸå")
        print(f"  - å¯ç”¨äº†ä¸¥æ ¼çš„åŸŸåçº§åˆ«æ£€æŸ¥ï¼Œæé«˜è§„åˆ™ç”Ÿæˆçš„å‡†ç¡®æ€§")


def create_sample_config():
    """
    åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶
    """
    sample_config = {
        "sources": [
            {
                "name": "Google Chinese Results Blocklist",
                "url": "https://raw.githubusercontent.com/cobaltdisco/Google-Chinese-Results-Blocklist/refs/heads/master/GHHbD_perma_ban_list.txt",
                "action": "remove",
                "format": "domain",
                "enabled": True
            },
            {
                "name": "Chinese Internet is Dead",
                "url": "https://raw.githubusercontent.com/obgnail/chinese-internet-is-dead/master/blocklist.txt",
                "action": "remove",
                "format": "ublock",
                "enabled": True
            },
            {
                "name": "Luxirty Search Block List",
                "url": "https://raw.githubusercontent.com/KoriIku/luxirty-search/refs/heads/main/docs/block_list.txt",
                "action": "remove",
                "format": "domain",
                "enabled": True
            },
            {
                "name": "Custom Blocklist",
                "url": "https://example.com/blocklist.txt",
                "action": "remove",
                "format": "domain",
                "enabled": False
            }
        ],
        "replace_rules": {
            '(.*\.)?youtube\.com$': 'yt.example.com',
            '(.*\.)?youtu\.be$': 'yt.example.com',
            '(.*\.)?reddit\.com$': 'teddit.example.com',
            '(.*\.)?redd\.it$': 'teddit.example.com',
            '(www\.)?twitter\.com$': 'nitter.example.com'
        },
        "fixed_remove": [
            '(.*\.)?facebook.com$'
        ],
        "fixed_low_priority": [
            '(.*\.)?google(\..*)?$'
        ],
        "fixed_high_priority": [
            '(.*\.)?wikipedia.org$'
        ],
        "parsing": {
            "ignore_specific_paths": True,
            "ignore_ip": True,
            "ignore_localhost": True,
            "strict_domain_level_check": True  # æ–°å¢ï¼šä¸¥æ ¼æ£€æŸ¥åŸŸåçº§åˆ«è§„åˆ™
        },
        "optimization": {
            "merge_domains": True,
            "max_domains_per_rule": 256,
            "group_by_tld": True,
            "use_trie_optimization": True,
            "max_rule_length": 65536,
            "optimize_tld_grouping": True,
            "enable_prefix_optimization": True,
            "enable_suffix_optimization": True,
            "min_common_prefix_length": 3,
            "min_common_suffix_length": 3,
            "force_single_regex": False,
            "sort_before_merge": True,
            "enable_advanced_tld_merge": True
        },
        "request_config": {
            "timeout": 30,
            "retry_count": 3,
            "retry_delay": 1
        },
        "output": {
            "mode": "separate_files",
            "directory": "./hostnames_rules/",
            "files": {
                "replace": "rewrite-hosts.yml",
                "remove": "remove-hosts.yml",
                "low_priority": "low-priority-hosts.yml",
                "high_priority": "high-priority-hosts.yml",
                "main_config": "hostnames-config.yml"
            }
        }
    }

    with open("config.yaml", "w", encoding="utf-8") as f:
        yaml.dump(sample_config, f, default_flow_style=False, allow_unicode=True, indent=2)

    print("ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²åˆ›å»º: config.yaml")


def main():
    parser = argparse.ArgumentParser(description="SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨ (é«˜çº§ä¼˜åŒ–ç‰ˆ - ä¿®å¤è·¯å¾„è§„åˆ™)")
    parser.add_argument("-c", "--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--create-config", action="store_true", help="åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶")
    parser.add_argument("--single-regex", action="store_true", help="å¼ºåˆ¶ç”Ÿæˆé«˜çº§TLDä¼˜åŒ–çš„å•è¡Œæ­£åˆ™è¡¨è¾¾å¼ï¼ˆå°†æ‰€æœ‰åŸŸååˆå¹¶ä¸ºä¸€ä¸ªè§„åˆ™ï¼‰")

    args = parser.parse_args()

    if args.create_config:
        create_sample_config()
        return

    generator = SearXNGHostnamesGenerator(args.config, force_single_regex=args.single_regex)
    generator.run()


if __name__ == "__main__":
    main()
