#!/usr/bin/env python3
"""
SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨ - ä¿®å¤TLDä¼˜åŒ–é—®é¢˜ + ç²¾ç¡®ç™½åå•åŠŸèƒ½
ä¿®å¤äº†åŸŸåTLDä¼˜åŒ–ä¸­ä¸¢å¤±é¡¶çº§åŸŸåçš„é—®é¢˜ï¼Œå¹¶æ·»åŠ äº†ç²¾ç¡®ç™½åå•è¿‡æ»¤åŠŸèƒ½

pip install requests pyyaml argparse
"""

import requests
import yaml
import json
import re
from urllib.parse import urlparse
from typing import Dict, List, Set, Union, Tuple
import argparse
import sys
import time
import os
from collections import OrderedDict, defaultdict

class SearXNGHostnamesGenerator:
    def __init__(self, config_file: str = None, force_single_regex: bool = False):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            force_single_regex: å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™è¡¨è¾¾å¼
        """
        self.config = self.load_config(config_file)
        self.force_single_regex = force_single_regex
        self.domains = set()
        self.whitelist_patterns = []  # ç¼–è¯‘åçš„ç™½åå•æ­£åˆ™æ¨¡å¼
        self.whitelist_domains = set()  # ç²¾ç¡®åŒ¹é…çš„ç™½åå•åŸŸå
        self.stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0,
            'whitelist_filtered': 0  # æ–°å¢ï¼šè¢«ç™½åå•è¿‡æ»¤çš„åŸŸåæ•°é‡
        }
        # æ–°å¢ï¼šè®°å½•æ¯ä¸ªç±»åˆ«çš„åŸŸåæ•°é‡
        self.category_domain_counts = {
            'remove': 0,
            'low_priority': 0,
            'high_priority': 0,
            'replace': 0
        }

        # åŠ è½½ç™½åå•
        self.load_whitelist()

    def load_config(self, config_file: str) -> Dict:
        """
        åŠ è½½é…ç½®æ–‡ä»¶

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„

        Returns:
            é…ç½®å­—å…¸
        """
        default_config = {
            # æ•°æ®æºé…ç½®
            "sources": [
                {
                    "name": "Chinese Internet is Dead",
                    "url": "https://raw.githubusercontent.com/obgnail/chinese-internet-is-dead/master/blocklist.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Content Farm Terminator - Content Farm Filters",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/content-farms.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Content Farm Terminator - Nearly Content Farm Filters",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/nearly-content-farms.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Content Farm Terminator - Extra Content Farm Filters",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist/extra-content-farms.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Content Farm Terminator - Bad Cloners",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/bad-cloners.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
            ],
            # ç™½åå•é…ç½®
            "whitelist": {
                "enabled": True,
                "mode": "remove_from_all",  # remove_from_all, remove_from_sources, custom_action
                "sources": [
                    {
                        "name": "Local Whitelist",
                        "file": "./whitelist.txt",
                        "format": "domain",
                        "enabled": True
                    }
                ],
                # ç›´æ¥åœ¨é…ç½®ä¸­å®šä¹‰çš„ç™½åå•
                "domains": [
                    # ç²¾ç¡®åŸŸååŒ¹é… - åªåŒ¹é…å®Œå…¨ç›¸åŒçš„åŸŸå
                    #"baidu.com",        # åªåŒ¹é… baidu.comï¼Œä¸åŒ¹é… test.baidu.com
                    #"google.com",       # åªåŒ¹é… google.comï¼Œä¸åŒ¹é… sub.google.com
                    #"bing.com"          # åªåŒ¹é… bing.comï¼Œä¸åŒ¹é… cn.bing.com
                ],
                "patterns": [
                    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åŒ¹é…
                    #r".*\.gov\..*",      # æ”¿åºœç½‘ç«™
                    #r".*\.edu\..*",      # æ•™è‚²ç½‘ç«™
                    #r".*wikipedia\..*"   # ç»´åŸºç™¾ç§‘
                ],
                "wildcard_domains": [
                    # é€šé…ç¬¦åŸŸååŒ¹é…ï¼ˆä¼šè½¬æ¢ä¸ºæ­£åˆ™ï¼‰- ç”¨äºåŒ¹é…å­åŸŸå
                    #"*.github.com",      # åŒ¹é…æ‰€æœ‰GitHubå­åŸŸå
                    #"*.stackoverflow.com", # åŒ¹é…æ‰€æœ‰Stack Overflowå­åŸŸå
                    #"*.baidu.com"        # å¦‚æœè¦åŒ¹é…æ‰€æœ‰ç™¾åº¦å­åŸŸåï¼Œä½¿ç”¨è¿™ç§æ–¹å¼
                ],
                # æŒ‰æºåç§°é…ç½®ç‰¹å®šç™½åå•
                "source_specific": {
                    # åªå¯¹ç‰¹å®šæ•°æ®æºåº”ç”¨çš„ç™½åå•
                    #"Chinese Internet is Dead": {
                    #    "domains": ["example.com"],     # ç²¾ç¡®åŒ¹é…
                    #    "patterns": [r".*\.example\..*"], # æ­£åˆ™åŒ¹é…
                    #    "wildcard_domains": ["*.example.com"] # é€šé…ç¬¦åŒ¹é…
                    #}
                }
            },
            # åŸŸåæ›¿æ¢è§„åˆ™
            "replace_rules": {
                #'(.*\.)?youtube\.com$': 'yt.example.com',
                #'(.*\.)?youtu\.be$': 'yt.example.com',
                #'(.*\.)?reddit\.com$': 'teddit.example.com',
                #'(.*\.)?redd\.it$': 'teddit.example.com',
                #'(www\.)?twitter\.com$': 'nitter.example.com'
            },
            # å›ºå®šçš„ç§»é™¤è§„åˆ™
            "fixed_remove": [
                #'(.*\.)?facebook.com$'
            ],
            # å›ºå®šçš„ä½ä¼˜å…ˆçº§è§„åˆ™
            "fixed_low_priority": [
                #'(.*\.)?google(\..*)?$'
            ],
            # å›ºå®šçš„é«˜ä¼˜å…ˆçº§è§„åˆ™
            "fixed_high_priority": [
                #'(.*\.)?wikipedia.org$'
            ],
            # è§£æé…ç½®
            "parsing": {
                "ignore_specific_paths": True,  # å¿½ç•¥æŒ‡å‘ç‰¹å®šè·¯å¾„çš„è§„åˆ™
                "ignore_ip": True,     # å¿½ç•¥IPåœ°å€
                "ignore_localhost": True,  # å¿½ç•¥æœ¬åœ°ä¸»æœº
                "strict_domain_level_check": True  # ä¸¥æ ¼æ£€æŸ¥åŸŸåçº§åˆ«è§„åˆ™
            },
            # æ€§èƒ½ä¼˜åŒ–é…ç½®
            "optimization": {
                "merge_domains": True,          # å¯ç”¨åŸŸååˆå¹¶ä¼˜åŒ–
                "max_domains_per_rule": 256,     # æ¯ä¸ªåˆå¹¶è§„åˆ™çš„æœ€å¤§åŸŸåæ•°
                "group_by_tld": True,           # æŒ‰é¡¶çº§åŸŸååˆ†ç»„
                "use_trie_optimization": True,  # ä½¿ç”¨å­—å…¸æ ‘ä¼˜åŒ–
                "max_rule_length": 65536,       # å•ä¸ªè§„åˆ™çš„æœ€å¤§é•¿åº¦é™åˆ¶
                "optimize_tld_grouping": True,   # ä¼˜åŒ–TLDåˆ†ç»„ï¼Œé¿å…é‡å¤
                "enable_prefix_optimization": True,  # å¯ç”¨å‰ç¼€ä¼˜åŒ–
                "enable_suffix_optimization": True,  # å¯ç”¨åç¼€ä¼˜åŒ–
                "min_common_prefix_length": 3,      # æœ€å°å…¬å…±å‰ç¼€é•¿åº¦
                "min_common_suffix_length": 3,      # æœ€å°å…¬å…±åç¼€é•¿åº¦
                "force_single_regex": False,         # å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™è¡¨è¾¾å¼
                "sort_before_merge": True,          # åˆå¹¶å‰æ’åºåŸŸå
                "enable_advanced_tld_merge": True   # å¯ç”¨é«˜çº§TLDåˆå¹¶
            },
            # è¯·æ±‚é…ç½®
            "request_config": {
                "timeout": 30,
                "retry_count": 3,
                "retry_delay": 1
            },
            # è¾“å‡ºé…ç½®
            "output": {
                "mode": "separate_files",  # separate_files æˆ– single_file
                "directory": "./rules/",
                "files": {
                    "replace": "rewrite-hosts.yml",
                    "remove": "remove-hosts.yml",
                    "low_priority": "low-priority-hosts.yml",
                    "high_priority": "high-priority-hosts.yml",
                    "main_config": "hostnames-config.yml"
                }
            }
        }

        if config_file:
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    # æ·±åº¦åˆå¹¶é…ç½®
                    self._deep_merge(default_config, user_config)
            except FileNotFoundError:
                print(f"é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            except Exception as e:
                print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                return default_config

        return default_config

    def _deep_merge(self, base_dict: Dict, update_dict: Dict) -> None:
        """
        æ·±åº¦åˆå¹¶å­—å…¸
        """
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_merge(base_dict[key], value)
            else:
                base_dict[key] = value

    def load_whitelist(self) -> None:
        """
        åŠ è½½ç™½åå•é…ç½®
        """
        if not self.config.get("whitelist", {}).get("enabled", False):
            print("ğŸš« ç™½åå•åŠŸèƒ½å·²ç¦ç”¨")
            return

        print("ğŸ“ æ­£åœ¨åŠ è½½ç™½åå•é…ç½®...")
        whitelist_config = self.config["whitelist"]

        # åŠ è½½ç›´æ¥é…ç½®çš„åŸŸåï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
        domains = whitelist_config.get("domains", [])
        self.whitelist_domains.update(d.lower() for d in domains if d)
        if domains:
            print(f"  âœ… åŠ è½½äº† {len(domains)} ä¸ªç²¾ç¡®åŒ¹é…åŸŸå")
            for domain in domains[:5]:  # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬
                print(f"    - {domain} (ç²¾ç¡®åŒ¹é…)")
            if len(domains) > 5:
                print(f"    ... è¿˜æœ‰ {len(domains)-5} ä¸ªåŸŸå")

        # åŠ è½½æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        patterns = whitelist_config.get("patterns", [])
        for pattern in patterns:
            try:
                compiled_pattern = re.compile(pattern, re.IGNORECASE)
                self.whitelist_patterns.append(compiled_pattern)
            except re.error as e:
                print(f"  âŒ æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ '{pattern}': {e}")
        if patterns:
            print(f"  âœ… åŠ è½½äº† {len(patterns)} ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼")
            for pattern in patterns[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬
                print(f"    - {pattern}")

        # åŠ è½½é€šé…ç¬¦åŸŸåï¼ˆè½¬æ¢ä¸ºæ­£åˆ™ï¼‰
        wildcard_domains = whitelist_config.get("wildcard_domains", [])
        converted_wildcards = 0
        for wildcard in wildcard_domains:
            try:
                # è½¬æ¢é€šé…ç¬¦ä¸ºæ­£åˆ™è¡¨è¾¾å¼
                regex_pattern = self._wildcard_to_regex(wildcard)
                compiled_pattern = re.compile(regex_pattern, re.IGNORECASE)
                self.whitelist_patterns.append(compiled_pattern)
                converted_wildcards += 1
            except re.error as e:
                print(f"  âŒ æ— æ•ˆçš„é€šé…ç¬¦åŸŸå '{wildcard}': {e}")
        if wildcard_domains:
            print(f"  âœ… åŠ è½½äº† {len(wildcard_domains)} ä¸ªé€šé…ç¬¦åŸŸåï¼ŒæˆåŠŸè½¬æ¢ {converted_wildcards} ä¸ª")
            for wildcard in wildcard_domains[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬
                print(f"    - {wildcard} -> åŒ¹é…å­åŸŸå")

        # ä»è¿œç¨‹æºå’Œæœ¬åœ°æ–‡ä»¶åŠ è½½ç™½åå•
        sources = whitelist_config.get("sources", [])
        for source in sources:
            if not source.get("enabled", True):
                continue

            try:
                if "url" in source:
                    # ä»URLåŠ è½½
                    print(f"  ğŸŒ æ­£åœ¨ä»URLåŠ è½½ç™½åå•: {source['name']}")
                    domains_from_url = self._load_whitelist_from_url(source)
                    self._add_domains_to_whitelist(domains_from_url, source.get("format", "domain"))
                elif "file" in source:
                    # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
                    print(f"  ğŸ“ æ­£åœ¨ä»æ–‡ä»¶åŠ è½½ç™½åå•: {source['name']}")
                    domains_from_file = self._load_whitelist_from_file(source)
                    self._add_domains_to_whitelist(domains_from_file, source.get("format", "domain"))
            except Exception as e:
                print(f"  âŒ åŠ è½½ç™½åå•æº '{source['name']}' å¤±è´¥: {e}")

        total_domains = len(self.whitelist_domains)
        total_patterns = len(self.whitelist_patterns)
        print(f"ğŸ“‹ ç™½åå•åŠ è½½å®Œæˆ: {total_domains} ä¸ªç²¾ç¡®åŸŸå, {total_patterns} ä¸ªæ¨¡å¼ï¼ˆå«é€šé…ç¬¦ï¼‰")
        print(f"âš ï¸  æ³¨æ„: ç²¾ç¡®åŒ¹é…åªåŒ¹é…å®Œå…¨ç›¸åŒçš„åŸŸåï¼Œå­åŸŸåéœ€è¦ä½¿ç”¨é€šé…ç¬¦æˆ–æ­£åˆ™è¡¨è¾¾å¼")

    def _wildcard_to_regex(self, wildcard: str) -> str:
        """
        å°†é€šé…ç¬¦åŸŸåè½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼

        Args:
            wildcard: é€šé…ç¬¦åŸŸå (å¦‚ *.example.com)

        Returns:
            æ­£åˆ™è¡¨è¾¾å¼å­—ç¬¦ä¸²
        """
        # è½¬ä¹‰é™¤äº†*ä¹‹å¤–çš„ç‰¹æ®Šå­—ç¬¦
        escaped = re.escape(wildcard)
        # å°†è½¬ä¹‰åçš„*æ›¿æ¢ä¸º.*
        regex = escaped.replace(r'\*', '.*')
        # æ·»åŠ å¼€å§‹å’Œç»“æŸé”šç‚¹
        return f"^{regex}$"

    def _load_whitelist_from_url(self, source: dict) -> List[str]:
        """
        ä»URLåŠ è½½ç™½åå•

        Args:
            source: ç™½åå•æºé…ç½®

        Returns:
            åŸŸååˆ—è¡¨
        """
        url = source["url"]
        timeout = self.config["request_config"]["timeout"]
        retry_count = self.config["request_config"]["retry_count"]
        retry_delay = self.config["request_config"]["retry_delay"]

        for attempt in range(retry_count):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, timeout=timeout, headers=headers)
                response.raise_for_status()

                return self._parse_whitelist_content(response.text, source.get("format", "domain"))

            except requests.RequestException as e:
                print(f"    âŒ è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}")
                if attempt < retry_count - 1:
                    time.sleep(retry_delay)

        return []

    def _load_whitelist_from_file(self, source: dict) -> List[str]:
        """
        ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ç™½åå•

        Args:
            source: ç™½åå•æºé…ç½®

        Returns:
            åŸŸååˆ—è¡¨
        """
        file_path = source["file"]
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return self._parse_whitelist_content(content, source.get("format", "domain"))
        except FileNotFoundError:
            print(f"    âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        except Exception as e:
            print(f"    âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

        return []

    def _parse_whitelist_content(self, content: str, format_type: str) -> List[str]:
        """
        è§£æç™½åå•å†…å®¹

        Args:
            content: æ–‡ä»¶å†…å®¹
            format_type: æ ¼å¼ç±»å‹ (domain, ublock, regex)

        Returns:
            åŸŸååˆ—è¡¨
        """
        domains = []

        for line in content.strip().split('\n'):
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            if format_type == "domain":
                # çº¯åŸŸåæ ¼å¼
                cleaned_domain = self.clean_domain(line)
                if cleaned_domain:
                    domains.append(cleaned_domain)
            elif format_type == "ublock":
                # uBlockæ ¼å¼
                domain, _ = self.parse_ublock_rule(line)
                if domain:
                    domains.append(domain)
            elif format_type == "regex":
                # æ­£åˆ™è¡¨è¾¾å¼æ ¼å¼ï¼Œç›´æ¥ä½œä¸ºæ¨¡å¼å¤„ç†
                try:
                    compiled_pattern = re.compile(line, re.IGNORECASE)
                    self.whitelist_patterns.append(compiled_pattern)
                except re.error as e:
                    print(f"    âŒ æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼: {line} - {e}")

        return domains

    def _add_domains_to_whitelist(self, domains: List[str], format_type: str) -> None:
        """
        å°†åŸŸåæ·»åŠ åˆ°ç™½åå•

        Args:
            domains: åŸŸååˆ—è¡¨
            format_type: æ ¼å¼ç±»å‹
        """
        if format_type == "regex":
            # æ­£åˆ™è¡¨è¾¾å¼å·²ç»åœ¨è§£ææ—¶å¤„ç†äº†
            return

        added_count = 0
        for domain in domains:
            if domain and domain not in self.whitelist_domains:
                self.whitelist_domains.add(domain.lower())
                added_count += 1

        if added_count > 0:
            print(f"    âœ… æ·»åŠ äº† {added_count} ä¸ªåŸŸååˆ°ç™½åå•")

    def is_whitelisted(self, domain: str, source_name: str = None) -> Tuple[bool, str]:
        """
        æ£€æŸ¥åŸŸåæ˜¯å¦åœ¨ç™½åå•ä¸­ï¼ˆç²¾ç¡®åŒ¹é…æ¨¡å¼ï¼‰

        Args:
            domain: è¦æ£€æŸ¥çš„åŸŸå
            source_name: æ•°æ®æºåç§°ï¼ˆç”¨äºæºç‰¹å®šç™½åå•ï¼‰

        Returns:
            (æ˜¯å¦åœ¨ç™½åå•ä¸­, åŒ¹é…åŸå› )
        """
        if not self.config.get("whitelist", {}).get("enabled", False):
            return False, ""

        domain_lower = domain.lower()

        # 1. æ£€æŸ¥ç²¾ç¡®åŸŸååŒ¹é…ï¼ˆåªåŒ¹é…å®Œå…¨ç›¸åŒçš„åŸŸåï¼‰
        if domain_lower in self.whitelist_domains:
            return True, f"ç²¾ç¡®åŒ¹é…: {domain}"

        # 2. æ£€æŸ¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åŒ¹é…ï¼ˆåŒ…æ‹¬é€šé…ç¬¦è½¬æ¢çš„æ­£åˆ™ï¼‰
        for pattern in self.whitelist_patterns:
            if pattern.match(domain_lower):
                return True, f"æ¨¡å¼åŒ¹é…: {pattern.pattern}"

        # 3. æ£€æŸ¥æºç‰¹å®šç™½åå•
        if source_name:
            source_whitelist = self.config["whitelist"].get("source_specific", {}).get(source_name, {})

            # æ£€æŸ¥æºç‰¹å®šç²¾ç¡®åŸŸåï¼ˆåªç²¾ç¡®åŒ¹é…ï¼‰
            source_domains = source_whitelist.get("domains", [])
            for wd in source_domains:
                if domain_lower == wd.lower():
                    return True, f"æºç‰¹å®šç²¾ç¡®åŒ¹é…: {wd}"

            # æ£€æŸ¥æºç‰¹å®šé€šé…ç¬¦åŸŸå
            source_wildcards = source_whitelist.get("wildcard_domains", [])
            for wildcard in source_wildcards:
                try:
                    regex_pattern = self._wildcard_to_regex(wildcard)
                    pattern = re.compile(regex_pattern, re.IGNORECASE)
                    if pattern.match(domain_lower):
                        return True, f"æºç‰¹å®šé€šé…ç¬¦åŒ¹é…: {wildcard}"
                except re.error:
                    continue

            # æ£€æŸ¥æºç‰¹å®šæ­£åˆ™æ¨¡å¼
            source_patterns = source_whitelist.get("patterns", [])
            for pattern_str in source_patterns:
                try:
                    pattern = re.compile(pattern_str, re.IGNORECASE)
                    if pattern.match(domain_lower):
                        return True, f"æºç‰¹å®šæ¨¡å¼åŒ¹é…: {pattern_str}"
                except re.error:
                    continue

        return False, ""

    def is_domain_level_rule(self, url_string: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™ï¼ˆè€Œéç‰¹å®šè·¯å¾„è§„åˆ™ï¼‰

        Args:
            url_string: URLå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦æ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™
        """
        url_string = url_string.strip()

        # ä¸¥æ ¼æ£€æŸ¥æ¨¡å¼
        if self.config["parsing"].get("strict_domain_level_check", True):
            # è¿™äº›æ¨¡å¼è¢«è®¤ä¸ºæ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™ï¼š
            domain_level_patterns = [
                # uBlock åŸŸåçº§åˆ«æ¨¡å¼ - ç²¾ç¡®æ¨¡å¼
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/?$',                    # *://*.example.com æˆ– *://*.example.com/
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/\*?$',                 # *://*.example.com/*
                r'^\*://[a-zA-Z0-9.-]+/?$',                         # *://example.com æˆ– *://example.com/
                r'^\*://[a-zA-Z0-9.-]+/\*?$',                       # *://example.com/*
                r'^\|\|[a-zA-Z0-9.-]+\^?$',                         # ||example.com^
                r'^[a-zA-Z0-9.-]+/?$',                              # example.com æˆ– example.com/
                r'^[a-zA-Z0-9.-]+/\*?$',                            # example.com/* æˆ– example.com/
            ]

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…åŸŸåçº§åˆ«æ¨¡å¼
            for pattern in domain_level_patterns:
                if re.match(pattern, url_string):
                    # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœåŒ…å«å…·ä½“è·¯å¾„ï¼ˆé™¤äº†/å’Œ/*ï¼‰ï¼Œåˆ™ä¸æ˜¯åŸŸåçº§åˆ«
                    if self._has_specific_path(url_string):
                        return False
                    return True

            return False
        else:
            # å…¼å®¹æ¨¡å¼ï¼ˆåŸæ¥çš„é€»è¾‘ï¼‰
            domain_level_patterns = [
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/?$',
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/\*$',
                r'^\|\|[a-zA-Z0-9.-]+\^?$',
                r'^[a-zA-Z0-9.-]+/?$',
                r'^[a-zA-Z0-9.-]+/\*$',
            ]

            for pattern in domain_level_patterns:
                if re.match(pattern, url_string):
                    return True

            return False

    def _has_specific_path(self, url_string: str) -> bool:
        """
        æ£€æŸ¥URLæ˜¯å¦åŒ…å«å…·ä½“çš„è·¯å¾„ï¼ˆéåŸŸåçº§åˆ«ï¼‰

        Args:
            url_string: URLå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦åŒ…å«å…·ä½“è·¯å¾„
        """
        # ç§»é™¤åè®®éƒ¨åˆ†
        if url_string.startswith('*://'):
            url_part = url_string[4:]
        elif url_string.startswith('||'):
            url_part = url_string[2:].rstrip('^')
        else:
            url_part = url_string

        # æ£€æŸ¥æ˜¯å¦æœ‰è·¯å¾„éƒ¨åˆ†
        if '/' in url_part:
            domain_and_path = url_part.split('/', 1)
            if len(domain_and_path) > 1:
                path_part = domain_and_path[1]
                # å¦‚æœè·¯å¾„ä¸æ˜¯ç©ºã€å•ä¸ª*æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™è®¤ä¸ºæ˜¯å…·ä½“è·¯å¾„
                if path_part and path_part not in ['', '*']:
                    return True

        return False

    def extract_domain_from_rule(self, rule: str) -> str:
        """
        ä»è§„åˆ™ä¸­æå–åŸŸå

        Args:
            rule: è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            åŸŸåæˆ– None
        """
        rule = rule.strip()

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“è·¯å¾„
        if self._has_specific_path(rule):
            # å¯¹äºåŒ…å«å…·ä½“è·¯å¾„çš„è§„åˆ™ï¼Œéœ€è¦æ›´è°¨æ…åœ°æå–åŸŸå
            return self._extract_domain_from_path_rule(rule)

        # uBlock è¯­æ³•æ¨¡å¼ - ä»…ç”¨äºåŸŸåçº§åˆ«è§„åˆ™
        patterns = [
            # *://*.domain.com/* æˆ– *://*.domain.com (é€šé…ç¬¦å­åŸŸå)
            r'^\*://\*\.([a-zA-Z0-9.-]+)(?:/\*?)?$',
            # *://domain.com/* æˆ– *://domain.com (æ— é€šé…ç¬¦)
            r'^\*://([a-zA-Z0-9.-]+)(?:/\*?)?$',
            # ||domain.com^
            r'^\|\|([a-zA-Z0-9.-]+)\^?$',
            # æ™®é€šåŸŸåæ ¼å¼
            r'^([a-zA-Z0-9.-]+)(?:/\*?)?$',
        ]

        for pattern in patterns:
            match = re.match(pattern, rule)
            if match:
                return match.group(1)

        # é€šç”¨åŸŸåæå–ï¼ˆæœ€åçš„åå¤‡æ–¹æ¡ˆï¼‰
        domain_match = re.search(r'([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', rule)
        if domain_match:
            candidate = domain_match.group(1)
            # éªŒè¯è¿™ä¸ªåŸŸåæ˜¯å¦åˆç†
            if self.is_valid_domain(candidate):
                return candidate

        return None

    def _extract_domain_from_path_rule(self, rule: str) -> str:
        """
        ä»åŒ…å«è·¯å¾„çš„è§„åˆ™ä¸­æå–åŸŸå

        Args:
            rule: åŒ…å«è·¯å¾„çš„è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            åŸŸåæˆ– None
        """
        rule = rule.strip()

        # å¯¹äºåŒ…å«å…·ä½“è·¯å¾„çš„è§„åˆ™ï¼Œæˆ‘ä»¬é€šå¸¸ä¸æå–åŸŸå
        # é™¤éç”¨æˆ·æ˜ç¡®é…ç½®å…è®¸
        if self.config["parsing"]["ignore_specific_paths"]:
            return None

        # å¦‚æœç”¨æˆ·å…è®¸å¤„ç†è·¯å¾„è§„åˆ™ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼
        path_patterns = [
            # *://*.subdomain.domain.com/path/* -> æå– subdomain.domain.com
            r'^\*://\*\.([a-zA-Z0-9.-]+)/[^/]+',
            # *://subdomain.domain.com/path/* -> æå– subdomain.domain.com
            r'^\*://([a-zA-Z0-9.-]+)/[^/]+',
        ]

        for pattern in path_patterns:
            match = re.match(pattern, rule)
            if match:
                domain = match.group(1)
                # åªæœ‰å½“è¿™æ˜¯ä¸€ä¸ªå­åŸŸåæ—¶æ‰è¿”å›ï¼Œé¿å…æå–ä¸»åŸŸå
                if '.' in domain and len(domain.split('.')) >= 2:
                    return domain

        return None

    def parse_ublock_rule(self, rule: str) -> Tuple[str, str]:
        """
        è§£æ uBlock Origin è¯­æ³•è§„åˆ™ï¼Œæå–åŸŸå

        Args:
            rule: uBlock è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            (åŸŸåæˆ– None, å¿½ç•¥åŸå› )
        """
        rule = rule.strip()
        if not rule or rule.startswith('!') or rule.startswith('#'):
            return None, "æ³¨é‡Šæˆ–ç©ºè¡Œ"

        # å¤„ç†è¡Œæœ«æ³¨é‡Š - ç§»é™¤ # åé¢çš„æ‰€æœ‰å†…å®¹
        if '#' in rule:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª # çš„ä½ç½®ï¼Œç§»é™¤å®ƒåŠåé¢çš„å†…å®¹
            comment_pos = rule.find('#')
            rule = rule[:comment_pos].strip()

            # å¦‚æœç§»é™¤æ³¨é‡Šåè§„åˆ™ä¸ºç©ºï¼Œåˆ™å¿½ç•¥
            if not rule:
                return None, "ä»…åŒ…å«æ³¨é‡Š"

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™
        if not self.is_domain_level_rule(rule):
            if self.config["parsing"]["ignore_specific_paths"]:
                return None, "æŒ‡å‘ç‰¹å®šè·¯å¾„"

        # æå–åŸŸå
        domain = self.extract_domain_from_rule(rule)

        if domain:
            cleaned_domain = self.clean_domain(domain)
            return cleaned_domain, None if cleaned_domain else "æ— æ•ˆåŸŸå"

        return None, "æ— æ³•è§£æè§„åˆ™æ ¼å¼"

    def fetch_domain_list(self, url: str, format_type: str = "domain", source_name: str = None) -> Tuple[Set[str], Dict]:
        """
        ä»URLè·å–åŸŸååˆ—è¡¨

        Args:
            url: åŸŸååˆ—è¡¨URL
            format_type: æ ¼å¼ç±»å‹ï¼Œ"domain" æˆ– "ublock"
            source_name: æ•°æ®æºåç§°ï¼ˆç”¨äºç™½åå•è¿‡æ»¤ï¼‰

        Returns:
            (åŸŸåé›†åˆ, ç»Ÿè®¡ä¿¡æ¯)
        """
        domains = set()
        stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0,
            'whitelist_filtered': 0  # è¢«ç™½åå•è¿‡æ»¤çš„æ•°é‡
        }

        retry_count = self.config["request_config"]["retry_count"]
        timeout = self.config["request_config"]["timeout"]
        retry_delay = self.config["request_config"]["retry_delay"]

        for attempt in range(retry_count):
            try:
                print(f"æ­£åœ¨è·å– {url} (å°è¯• {attempt + 1}/{retry_count})")

                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }

                response = requests.get(url, timeout=timeout, headers=headers)
                response.raise_for_status()

                # è®°å½•ä¸€äº›è¢«å¿½ç•¥çš„è§„åˆ™ç”¨äºè°ƒè¯•
                ignored_samples = []
                accepted_samples = []
                comment_samples = []
                path_samples = []  # æ–°å¢ï¼šè·¯å¾„è§„åˆ™æ ·æœ¬
                whitelist_samples = []  # æ–°å¢ï¼šç™½åå•è¿‡æ»¤æ ·æœ¬

                # è§£æåŸŸå
                for line_num, line in enumerate(response.text.strip().split('\n'), 1):
                    line = line.strip()
                    if not line:
                        continue

                    stats['total_rules'] += 1

                    try:
                        if format_type == "ublock":
                            # ä½¿ç”¨ uBlock è¯­æ³•è§£æ
                            domain, ignore_reason = self.parse_ublock_rule(line)
                        else:
                            # æ™®é€šåŸŸåæ ¼å¼ - ä¹Ÿéœ€è¦å¤„ç†è¡Œæœ«æ³¨é‡Š
                            cleaned_line = line
                            if '#' in line:
                                cleaned_line = line[:line.find('#')].strip()
                                if not cleaned_line:
                                    domain, ignore_reason = None, "ä»…åŒ…å«æ³¨é‡Š"
                                else:
                                    if self.config["parsing"]["ignore_specific_paths"] and not self.is_domain_level_rule(cleaned_line):
                                        domain, ignore_reason = None, "æŒ‡å‘ç‰¹å®šè·¯å¾„"
                                    else:
                                        domain = self.clean_domain(self.extract_domain_from_rule(cleaned_line))
                                        ignore_reason = None if domain else "æ— æ•ˆåŸŸå"
                            else:
                                if self.config["parsing"]["ignore_specific_paths"] and not self.is_domain_level_rule(cleaned_line):
                                    domain, ignore_reason = None, "æŒ‡å‘ç‰¹å®šè·¯å¾„"
                                else:
                                    domain = self.clean_domain(self.extract_domain_from_rule(cleaned_line))
                                    ignore_reason = None if domain else "æ— æ•ˆåŸŸå"

                        if domain:
                            # æ£€æŸ¥ç™½åå•
                            is_whitelisted, whitelist_reason = self.is_whitelisted(domain, source_name)

                            if is_whitelisted:
                                stats['whitelist_filtered'] += 1
                                if len(whitelist_samples) < 3:
                                    whitelist_samples.append(f"{line} -> {domain} ({whitelist_reason})")
                            else:
                                if domain in domains:
                                    stats['duplicate_domains'] += 1
                                else:
                                    domains.add(domain)
                                    stats['parsed_domains'] += 1
                                    # è®°å½•ä¸€äº›è¢«æ¥å—çš„è§„åˆ™æ ·æœ¬
                                    if len(accepted_samples) < 3:
                                        accepted_samples.append(f"{line} -> {domain}")
                        else:
                            # ç»Ÿè®¡å¿½ç•¥åŸå› 
                            if ignore_reason == "æŒ‡å‘ç‰¹å®šè·¯å¾„":
                                stats['ignored_with_path'] += 1
                                # è®°å½•ä¸€äº›è¢«å¿½ç•¥çš„è·¯å¾„è§„åˆ™æ ·æœ¬
                                if len(path_samples) < 3:
                                    path_samples.append(line)
                            elif ignore_reason in ["æ³¨é‡Šæˆ–ç©ºè¡Œ", "ä»…åŒ…å«æ³¨é‡Š"]:
                                stats['ignored_comments'] += 1
                                if len(comment_samples) < 3:
                                    comment_samples.append(line)
                            elif ignore_reason == "æ— æ•ˆåŸŸå":
                                stats['invalid_domains'] += 1
                                if len(ignored_samples) < 3:
                                    ignored_samples.append(line)

                    except Exception as e:
                        print(f"è§£æç¬¬ {line_num} è¡Œæ—¶å‡ºé”™: {line[:50]}... - {e}")
                        stats['invalid_domains'] += 1
                        continue

                print(f"æˆåŠŸè·å– {len(domains)} ä¸ªåŸŸå")
                print(f"  - æ€»è§„åˆ™: {stats['total_rules']}")
                print(f"  - æˆåŠŸè§£æ: {stats['parsed_domains']}")
                print(f"  - å¿½ç•¥(ç‰¹å®šè·¯å¾„): {stats['ignored_with_path']}")
                print(f"  - å¿½ç•¥(æ³¨é‡Š): {stats['ignored_comments']}")
                print(f"  - å¿½ç•¥(æ— æ•ˆåŸŸå): {stats['invalid_domains']}")
                print(f"  - é‡å¤åŸŸå: {stats['duplicate_domains']}")
                print(f"  - ç™½åå•è¿‡æ»¤: {stats['whitelist_filtered']}")

                # æ˜¾ç¤ºæ ·æœ¬
                if accepted_samples:
                    print(f"  - æ¥å—çš„è§„åˆ™æ ·æœ¬:")
                    for sample in accepted_samples:
                        print(f"    âœ“ {sample}")

                if whitelist_samples:
                    print(f"  - ç™½åå•è¿‡æ»¤æ ·æœ¬:")
                    for sample in whitelist_samples:
                        print(f"    ğŸš« {sample}")

                if path_samples:
                    print(f"  - å¿½ç•¥çš„è·¯å¾„è§„åˆ™æ ·æœ¬:")
                    for sample in path_samples:
                        print(f"    ğŸ›¤ï¸  {sample}")

                if comment_samples:
                    print(f"  - å¿½ç•¥çš„æ³¨é‡Šè§„åˆ™æ ·æœ¬:")
                    for sample in comment_samples:
                        print(f"    # {sample}")

                if ignored_samples:
                    print(f"  - å…¶ä»–å¿½ç•¥çš„è§„åˆ™æ ·æœ¬:")
                    for sample in ignored_samples:
                        print(f"    âœ— {sample}")

                return domains, stats

            except requests.RequestException as e:
                print(f"è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}")
                if attempt < retry_count - 1:
                    time.sleep(retry_delay)
                else:
                    print(f"æ”¾å¼ƒè·å– {url}")

        return domains, stats

    def clean_domain(self, domain: str) -> str:
        """
        æ¸…ç†åŸŸåå­—ç¬¦ä¸²

        Args:
            domain: åŸå§‹åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ¸…ç†åçš„åŸŸå
        """
        if not domain:
            return None

        # ç§»é™¤åè®®
        if domain.startswith(('http://', 'https://')):
            domain = urlparse(domain).netloc

        # ç§»é™¤ç«¯å£
        if ':' in domain:
            domain = domain.split(':')[0]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è·¯å¾„ï¼ˆè¿™é‡Œä¸åº”è¯¥æœ‰ï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
        if '/' in domain:
            domain = domain.split('/')[0]

        # ç§»é™¤ www. å‰ç¼€
        if domain.startswith('www.'):
            domain = domain[4:]

        # ç§»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        domain = re.sub(r'[^\w.-]', '', domain)

        # æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€
        if self.config["parsing"]["ignore_ip"] and self.is_ip_address(domain):
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯localhost
        if self.config["parsing"]["ignore_localhost"] and domain in ['localhost', '127.0.0.1', '0.0.0.0']:
            return None

        # éªŒè¯åŸŸåæ ¼å¼
        if self.is_valid_domain(domain):
            return domain.lower()

        return None

    def is_ip_address(self, domain: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€

        Args:
            domain: åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦æ˜¯IPåœ°å€
        """
        ip_pattern = re.compile(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')
        return bool(ip_pattern.match(domain))

    def is_valid_domain(self, domain: str) -> bool:
        """
        éªŒè¯åŸŸåæ ¼å¼

        Args:
            domain: åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦ä¸ºæœ‰æ•ˆåŸŸå
        """
        if not domain or len(domain) > 255:
            return False

        # åŸºæœ¬çš„åŸŸåæ ¼å¼éªŒè¯
        domain_pattern = re.compile(
            r'^(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)*[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$'
        )

        return bool(domain_pattern.match(domain))

    def domain_to_regex(self, domain: str) -> str:
        """
        å°†åŸŸåè½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼

        Args:
            domain: åŸŸå

        Returns:
            æ­£åˆ™è¡¨è¾¾å¼å­—ç¬¦ä¸²
        """
        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
        escaped_domain = re.escape(domain)
        # æ·»åŠ å­åŸŸååŒ¹é…
        return f'(.*\.)?{escaped_domain}$'

    def smart_sort_domains(self, domains: Set[str]) -> List[str]:
        """
        æ™ºèƒ½æ’åºåŸŸåï¼Œä¾¿äºåç»­åˆå¹¶
        å…ˆæŒ‰TLDæ’åºï¼Œå†æŒ‰åŸŸåä¸»ä½“æ’åº

        Args:
            domains: åŸŸåé›†åˆ

        Returns:
            æ’åºåçš„åŸŸååˆ—è¡¨
        """
        def domain_sort_key(domain: str) -> Tuple[str, str]:
            """
            ç”ŸæˆåŸŸåæ’åºé”®ï¼š(TLD, åå‘åŸŸåä¸»ä½“)
            è¿™æ ·å¯ä»¥å°†åŒTLDçš„åŸŸåèšé›†åœ¨ä¸€èµ·ï¼Œä¾¿äºåˆå¹¶
            """
            parts = domain.split('.')
            if len(parts) >= 2:
                # TLD ä½œä¸ºä¸»è¦æ’åºé”®
                tld = parts[-1]
                # åŸŸåä¸»ä½“ä½œä¸ºæ¬¡è¦æ’åºé”®ï¼Œåå‘æ’åºä¾¿äºæ‰¾åˆ°å…¬å…±åç¼€
                base = '.'.join(parts[:-1])
                return (tld, base)
            else:
                return (domain, '')

        if self.config["optimization"].get("sort_before_merge", True):
            sorted_domains = sorted(list(domains), key=domain_sort_key)
            print(f"  ğŸ”„ åŸŸåå·²æŒ‰TLDæ™ºèƒ½æ’åºï¼Œä¾¿äºåˆå¹¶ä¼˜åŒ–")
            return sorted_domains
        else:
            return list(domains)

    def group_domains_by_tld(self, domains: Union[Set[str], List[str]]) -> Dict[str, List[str]]:
        """
        æŒ‰é¡¶çº§åŸŸååˆ†ç»„åŸŸåï¼Œå¹¶ä¿æŒæ’åº

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            æŒ‰TLDåˆ†ç»„çš„åŸŸåå­—å…¸ï¼Œå€¼ä¸ºæ’åºåçš„åˆ—è¡¨
        """
        tld_groups = defaultdict(list)

        # å¦‚æœè¾“å…¥æ˜¯é›†åˆï¼Œå…ˆè½¬æ¢ä¸ºæ™ºèƒ½æ’åºçš„åˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        for domain in domains:
            parts = domain.split('.')
            if len(parts) >= 2:
                # è·å–é¡¶çº§åŸŸåï¼ˆå¦‚ .com, .orgï¼‰
                tld = parts[-1]
                tld_groups[tld].append(domain)
            else:
                # å¤„ç†æ— æ•ˆåŸŸå
                tld_groups['other'].append(domain)

        return dict(tld_groups)

    def get_domain_base_and_tld(self, domain: str) -> Tuple[str, str]:
        """
        æå–åŸŸåçš„ä¸»ä½“éƒ¨åˆ†å’ŒTLD

        Args:
            domain: å®Œæ•´åŸŸå

        Returns:
            (åŸŸåä¸»ä½“, TLD)
        """
        parts = domain.split('.')
        if len(parts) >= 2:
            tld = parts[-1]
            base = '.'.join(parts[:-1])
            return base, tld
        return domain, ''

    def find_common_prefix(self, strings: List[str]) -> str:
        """
        æ‰¾åˆ°å­—ç¬¦ä¸²åˆ—è¡¨çš„å…¬å…±å‰ç¼€

        Args:
            strings: å­—ç¬¦ä¸²åˆ—è¡¨

        Returns:
            å…¬å…±å‰ç¼€
        """
        if not strings:
            return ""

        strings = [s for s in strings if s]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not strings:
            return ""

        min_len = min(len(s) for s in strings)
        prefix = ""

        for i in range(min_len):
            char = strings[0][i]
            if all(s[i] == char for s in strings):
                prefix += char
            else:
                break

        return prefix

    def find_common_suffix(self, strings: List[str]) -> str:
        """
        æ‰¾åˆ°å­—ç¬¦ä¸²åˆ—è¡¨çš„å…¬å…±åç¼€

        Args:
            strings: å­—ç¬¦ä¸²åˆ—è¡¨

        Returns:
            å…¬å…±åç¼€
        """
        if not strings:
            return ""

        strings = [s for s in strings if s]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not strings:
            return ""

        # åè½¬å­—ç¬¦ä¸²ï¼Œæ‰¾å‰ç¼€ï¼Œå†åè½¬å›æ¥
        reversed_strings = [s[::-1] for s in strings]
        reversed_suffix = self.find_common_prefix(reversed_strings)
        return reversed_suffix[::-1]

    def create_advanced_tld_regex(self, tld_domains: List[str], tld: str) -> str:
        """
        ä¸ºåŒä¸€TLDçš„åŸŸååˆ›å»ºé«˜çº§ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼
        ä¿®å¤ç‰ˆæœ¬ï¼šç¡®ä¿TLDä¸ä¼šä¸¢å¤±

        Args:
            tld_domains: åŒä¸€TLDçš„åŸŸååˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
            tld: é¡¶çº§åŸŸå

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        if len(tld_domains) == 1:
            return re.escape(tld_domains[0])

        # æå–åŸŸåä¸»ä½“éƒ¨åˆ†
        domain_bases = []
        for domain in tld_domains:
            base, domain_tld = self.get_domain_base_and_tld(domain)
            if domain_tld == tld:
                domain_bases.append(base)
            else:
                # TLDä¸åŒ¹é…çš„æƒ…å†µï¼Œä½¿ç”¨å®Œæ•´åŸŸå
                domain_bases.append(domain)

        if not domain_bases:
            return '|'.join(re.escape(d) for d in tld_domains)

        # å°è¯•æ‰¾åˆ°å…¬å…±æ¨¡å¼
        optimized_pattern = self.optimize_domain_bases(domain_bases)

        # æ£€æŸ¥åŸŸååŸºç¡€éƒ¨åˆ†çš„ç»“æ„
        simple_domains = [base for base in domain_bases if '.' not in base]  # äºŒçº§åŸŸå
        complex_domains = [base for base in domain_bases if '.' in base]     # å¤šçº§åŸŸå

        if len(simple_domains) == len(domain_bases):
            # æ‰€æœ‰éƒ½æ˜¯äºŒçº§åŸŸåï¼Œå¯ä»¥è¿›è¡ŒTLDä¼˜åŒ–
            return f"({optimized_pattern})\\.{re.escape(tld)}"
        elif len(complex_domains) == len(domain_bases):
            # æ‰€æœ‰éƒ½æ˜¯å¤šçº§åŸŸåï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰å…¬å…±çš„äºŒçº§+TLDåç¼€
            return self._optimize_complex_domains_with_tld(domain_bases, tld)
        else:
            # æ··åˆæƒ…å†µï¼šäºŒçº§åŸŸå+å¤šçº§åŸŸå
            return self._optimize_mixed_domains_with_tld(simple_domains, complex_domains, tld)

    def _optimize_complex_domains_with_tld(self, domain_bases: List[str], tld: str) -> str:
        """
        ä¼˜åŒ–å¤šçº§åŸŸåï¼Œç¡®ä¿ä¿ç•™TLD

        Args:
            domain_bases: åŸŸååŸºç¡€éƒ¨åˆ†åˆ—è¡¨ï¼ˆéƒ½æ˜¯å¤šçº§åŸŸåï¼‰
            tld: é¡¶çº§åŸŸå

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¬å…±çš„äºŒçº§åŸŸå+TLDæ¨¡å¼
        # ä¾‹å¦‚ï¼ša.pixnet.net, b.pixnet.net -> (a|b).pixnet.net

        # æ‰¾åˆ°æ‰€æœ‰åŸŸåçš„å…¬å…±åç¼€ï¼ˆä¸åŒ…æ‹¬ç¬¬ä¸€éƒ¨åˆ†ï¼‰
        if len(domain_bases) <= 1:
            if domain_bases:
                return f"{re.escape(domain_bases[0])}\\.{re.escape(tld)}"
            return f".*\\.{re.escape(tld)}"

        # åˆ†æç»“æ„ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰åŸŸåéƒ½æœ‰ç›¸åŒçš„åç¼€ç»“æ„
        common_suffix_parts = None
        prefixes = []

        for base in domain_bases:
            parts = base.split('.')
            if common_suffix_parts is None:
                # ç¬¬ä¸€ä¸ªåŸŸåï¼Œè®¾ç½®å…¬å…±åç¼€å€™é€‰
                if len(parts) >= 2:
                    common_suffix_parts = parts[1:]  # é™¤äº†ç¬¬ä¸€éƒ¨åˆ†çš„å…¶ä½™éƒ¨åˆ†
                    prefixes.append(parts[0])
                else:
                    # å¤„ç†å¼‚å¸¸æƒ…å†µ
                    common_suffix_parts = []
                    prefixes.append(base)
            else:
                # æ£€æŸ¥æ˜¯å¦ä¸å…¬å…±åç¼€åŒ¹é…
                if len(parts) >= len(common_suffix_parts) + 1:
                    current_suffix = parts[-(len(common_suffix_parts)):]
                    if current_suffix == common_suffix_parts:
                        prefixes.append(parts[0])
                    else:
                        # åç¼€ä¸åŒ¹é…ï¼Œæ— æ³•ä¼˜åŒ–ï¼Œç›´æ¥è¿”å›å®Œæ•´åŸŸååˆ—è¡¨
                        escaped_bases = [re.escape(base) for base in domain_bases]
                        return f"({'|'.join(escaped_bases)})\\.{re.escape(tld)}"
                else:
                    # é•¿åº¦ä¸å¤Ÿï¼Œæ— æ³•ä¼˜åŒ–
                    escaped_bases = [re.escape(base) for base in domain_bases]
                    return f"({'|'.join(escaped_bases)})\\.{re.escape(tld)}"

        # å¦‚æœæ‰¾åˆ°äº†å…¬å…±åç¼€ï¼Œè¿›è¡Œä¼˜åŒ–
        if common_suffix_parts and len(set(prefixes)) > 1:
            # ä¼˜åŒ–å‰ç¼€éƒ¨åˆ†
            optimized_prefixes = self.optimize_domain_bases(prefixes)
            escaped_suffix = '\\.'.join(re.escape(part) for part in common_suffix_parts)
            return f"({optimized_prefixes})\\.{escaped_suffix}\\.{re.escape(tld)}"
        else:
            # æ— æ³•æ‰¾åˆ°å…¬å…±æ¨¡å¼ï¼Œä½¿ç”¨åŸºç¡€ä¼˜åŒ–
            optimized_pattern = self.optimize_domain_bases(domain_bases)
            return f"({optimized_pattern})\\.{re.escape(tld)}"

    def _optimize_mixed_domains_with_tld(self, simple_domains: List[str], complex_domains: List[str], tld: str) -> str:
        """
        ä¼˜åŒ–æ··åˆåŸŸåï¼ˆäºŒçº§+å¤šçº§ï¼‰ï¼Œç¡®ä¿ä¿ç•™TLD

        Args:
            simple_domains: äºŒçº§åŸŸååˆ—è¡¨
            complex_domains: å¤šçº§åŸŸååˆ—è¡¨
            tld: é¡¶çº§åŸŸå

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        patterns = []

        # å¤„ç†äºŒçº§åŸŸå
        if simple_domains:
            if len(simple_domains) == 1:
                patterns.append(f"{re.escape(simple_domains[0])}\\.{re.escape(tld)}")
            else:
                optimized_simple = self.optimize_domain_bases(simple_domains)
                patterns.append(f"({optimized_simple})\\.{re.escape(tld)}")

        # å¤„ç†å¤šçº§åŸŸå
        if complex_domains:
            complex_pattern = self._optimize_complex_domains_with_tld(complex_domains, tld)
            patterns.append(complex_pattern)

        # åˆå¹¶æ‰€æœ‰æ¨¡å¼
        if len(patterns) == 1:
            return patterns[0]
        else:
            return f"({'|'.join(patterns)})"

    def optimize_domain_bases(self, domain_bases: List[str]) -> str:
        """
        ä¼˜åŒ–åŸŸååŸºç¡€éƒ¨åˆ†åˆ—è¡¨

        Args:
            domain_bases: åŸŸååŸºç¡€éƒ¨åˆ†åˆ—è¡¨

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        """
        if len(domain_bases) <= 1:
            return '|'.join(re.escape(base) for base in domain_bases)

        optimization_config = self.config["optimization"]

        # å°è¯•å‰ç¼€ä¼˜åŒ–
        if optimization_config.get("enable_prefix_optimization", True):
            common_prefix = self.find_common_prefix(domain_bases)
            min_prefix_len = optimization_config.get("min_common_prefix_length", 3)

            if len(common_prefix) >= min_prefix_len:
                # ç§»é™¤å…¬å…±å‰ç¼€
                suffixes = [base[len(common_prefix):] for base in domain_bases]
                suffixes = [s for s in suffixes if s]  # è¿‡æ»¤ç©ºåç¼€
                if suffixes and len(set(suffixes)) > 1:  # ç¡®ä¿æœ‰ä¸åŒçš„åç¼€
                    suffix_pattern = self.optimize_domain_bases(suffixes)
                    return f"{re.escape(common_prefix)}({suffix_pattern})"

        # å°è¯•åç¼€ä¼˜åŒ–
        if optimization_config.get("enable_suffix_optimization", True):
            common_suffix = self.find_common_suffix(domain_bases)
            min_suffix_len = optimization_config.get("min_common_suffix_length", 3)

            if len(common_suffix) >= min_suffix_len:
                # ç§»é™¤å…¬å…±åç¼€
                prefixes = [base[:-len(common_suffix)] for base in domain_bases]
                prefixes = [p for p in prefixes if p]  # è¿‡æ»¤ç©ºå‰ç¼€
                if prefixes and len(set(prefixes)) > 1:  # ç¡®ä¿æœ‰ä¸åŒçš„å‰ç¼€
                    prefix_pattern = self.optimize_domain_bases(prefixes)
                    return f"({prefix_pattern}){re.escape(common_suffix)}"

        # æ²¡æœ‰æ‰¾åˆ°ä¼˜åŒ–æ¨¡å¼ï¼Œç›´æ¥è¿æ¥
        return '|'.join(re.escape(base) for base in domain_bases)

    def create_single_regex_rule(self, domains: Union[Set[str], List[str]]) -> str:
        """
        åˆ›å»ºåŒ…å«æ‰€æœ‰åŸŸåçš„å•è¡Œæ­£åˆ™è¡¨è¾¾å¼ï¼ˆé«˜çº§TLDä¼˜åŒ–ï¼‰

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            å•è¡Œæ­£åˆ™è¡¨è¾¾å¼
        """
        if not domains:
            return ""

        # è½¬æ¢ä¸ºæ’åºåˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"

        print(f"ğŸš€ æ­£åœ¨ç”Ÿæˆé«˜çº§TLDä¼˜åŒ–å•è¡Œæ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ…å« {len(domains)} ä¸ªåŸŸå")

        # å¯ç”¨é«˜çº§TLDåˆå¹¶
        if self.config["optimization"].get("enable_advanced_tld_merge", True):
            # æŒ‰TLDåˆ†ç»„
            tld_groups = self.group_domains_by_tld(domains)
            tld_patterns = []

            print(f"  ğŸ“Š TLDåˆ†å¸ƒæƒ…å†µ:")
            for tld, tld_domains in sorted(tld_groups.items(), key=lambda x: len(x[1]), reverse=True):
                print(f"    .{tld}: {len(tld_domains)} ä¸ªåŸŸå")
                # æ˜¾ç¤ºä¸€äº›åŸŸåæ ·æœ¬
                if len(tld_domains) <= 3:
                    for domain in tld_domains:
                        print(f"      - {domain}")
                else:
                    for domain in tld_domains[:3]:
                        print(f"      - {domain}")
                    print(f"      - ... è¿˜æœ‰ {len(tld_domains)-3} ä¸ªåŸŸå")

            for tld, tld_domains in tld_groups.items():
                if len(tld_domains) == 1:
                    # å•ä¸ªåŸŸåç›´æ¥å¤„ç†
                    domain = tld_domains[0]
                    tld_patterns.append(re.escape(domain))
                else:
                    # å¤šä¸ªåŸŸåè¿›è¡Œé«˜çº§ä¼˜åŒ–
                    optimized_pattern = self.create_advanced_tld_regex(tld_domains, tld)
                    tld_patterns.append(optimized_pattern)
                    print(f"  âœ… TLD .{tld}: {len(tld_domains)} ä¸ªåŸŸåå·²ä¼˜åŒ–åˆå¹¶")

            # åˆå¹¶æ‰€æœ‰TLDç»„çš„æ¨¡å¼
            if len(tld_patterns) == 1:
                combined_pattern = tld_patterns[0]
            else:
                combined_pattern = f"({'|'.join(tld_patterns)})"

            single_regex = f"(.*\\.)?{combined_pattern}$"
        else:
            # ç®€å•åˆå¹¶æ¨¡å¼
            escaped_domains = [re.escape(d) for d in domains]
            combined_pattern = '|'.join(escaped_domains)
            single_regex = f"(.*\\.)?({combined_pattern})$"

        # æ˜¾ç¤ºè§„åˆ™é•¿åº¦ä¿¡æ¯
        rule_length = len(single_regex)
        print(f"  ğŸ“ ç”Ÿæˆçš„å•è¡Œè§„åˆ™é•¿åº¦: {rule_length:,} å­—ç¬¦")

        if rule_length > 100000:
            print("  âš ï¸  è§„åˆ™æé•¿ï¼Œå¯èƒ½ä¸¥é‡å½±å“æ€§èƒ½ï¼Œå»ºè®®åˆ†å‰²")
        elif rule_length > 50000:
            print("  âš ï¸  è§„åˆ™å¾ˆé•¿ï¼Œå¯èƒ½å½±å“åŒ¹é…æ€§èƒ½")
        elif rule_length > 10000:
            print("  âš ï¸  è§„åˆ™è¾ƒé•¿ï¼Œè¯·æ³¨æ„æ€§èƒ½")
        else:
            print("  âœ… è§„åˆ™é•¿åº¦é€‚ä¸­")

        return single_regex

    def create_multiple_optimized_rules(self, domains: Union[Set[str], List[str]]) -> List[str]:
        """
        åˆ›å»ºå¤šä¸ªä¼˜åŒ–çš„è§„åˆ™ï¼ˆéå•è¡Œæ¨¡å¼ï¼‰

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            ä¼˜åŒ–åçš„è§„åˆ™åˆ—è¡¨
        """
        if not domains:
            return []

        # è½¬æ¢ä¸ºæ’åºåˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        optimization_config = self.config["optimization"]
        max_domains_per_rule = optimization_config.get("max_domains_per_rule", 30)
        max_rule_length = optimization_config.get("max_rule_length", 4000)

        # æŒ‰TLDåˆ†ç»„å¤„ç†
        if optimization_config.get("group_by_tld", True):
            tld_groups = self.group_domains_by_tld(domains)
            rules = []

            for tld, tld_domains in tld_groups.items():
                if len(tld_domains) <= 1:
                    # å•ä¸ªåŸŸåç›´æ¥è½¬æ¢
                    rules.extend([self.domain_to_regex(domain) for domain in tld_domains])
                else:
                    # å¤šä¸ªåŸŸååˆ†æ‰¹å¤„ç†
                    tld_rules = self._create_batched_rules(tld_domains, tld, max_domains_per_rule, max_rule_length)
                    rules.extend(tld_rules)
                    print(f"  ğŸ“¦ TLD .{tld}: {len(tld_domains)} ä¸ªåŸŸå -> {len(tld_rules)} ä¸ªè§„åˆ™")

            return rules
        else:
            # ä¸åˆ†ç»„ï¼Œç›´æ¥åˆ†æ‰¹å¤„ç†
            return self._create_batched_rules(domains, None, max_domains_per_rule, max_rule_length)

    def _create_batched_rules(self, domains: List[str], tld: str = None, max_domains_per_rule: int = 30, max_rule_length: int = 4000) -> List[str]:
        """
        ä¸ºåŸŸååˆ—è¡¨åˆ›å»ºåˆ†æ‰¹çš„è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨
            tld: é¡¶çº§åŸŸåï¼ˆå¯é€‰ï¼Œç”¨äºä¼˜åŒ–ï¼‰
            max_domains_per_rule: æ¯ä¸ªè§„åˆ™çš„æœ€å¤§åŸŸåæ•°
            max_rule_length: æœ€å¤§è§„åˆ™é•¿åº¦

        Returns:
            åˆ†æ‰¹åçš„è§„åˆ™åˆ—è¡¨
        """
        rules = []
        current_batch = []

        for domain in domains:
            test_batch = current_batch + [domain]

            # åˆ›å»ºæµ‹è¯•è§„åˆ™
            if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                test_rule = self._create_tld_optimized_rule(test_batch, tld)
            else:
                test_rule = self._create_simple_rule(test_batch)

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if (len(test_batch) > max_domains_per_rule or
                len(test_rule) > max_rule_length):

                # ä¿å­˜å½“å‰æ‰¹æ¬¡
                if current_batch:
                    if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                        rules.append(self._create_tld_optimized_rule(current_batch, tld))
                    else:
                        rules.append(self._create_simple_rule(current_batch))

                # å¼€å§‹æ–°æ‰¹æ¬¡
                current_batch = [domain]
            else:
                current_batch.append(domain)

        # å¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                rules.append(self._create_tld_optimized_rule(current_batch, tld))
            else:
                rules.append(self._create_simple_rule(current_batch))

        return rules

    def _create_tld_optimized_rule(self, domains: List[str], tld: str) -> str:
        """
        ä¸ºåŒTLDåŸŸååˆ›å»ºä¼˜åŒ–è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨
            tld: é¡¶çº§åŸŸå

        Returns:
            TLDä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™
        """
        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"

        optimized_pattern = self.create_advanced_tld_regex(domains, tld)
        return f"(.*\\.)?{optimized_pattern}$"

    def _create_simple_rule(self, domains: List[str]) -> str:
        """
        ä¸ºåŸŸååˆ—è¡¨åˆ›å»ºç®€å•è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨

        Returns:
            ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™
        """
        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"
        else:
            pattern = self.optimize_domain_bases(domains)
            return f"(.*\\.)?({pattern})$"

    def merge_domains_to_regex(self, domains: Set[str]) -> List[str]:
        """
        å°†å¤šä¸ªåŸŸååˆå¹¶ä¸ºä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨

        Args:
            domains: åŸŸåé›†åˆ

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨
        """
        if not domains:
            return []

        # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™
        if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
            print(f"ğŸš€ å¯ç”¨å¼ºåˆ¶å•è¡Œæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼")
            single_rule = self.create_single_regex_rule(domains)
            return [single_rule] if single_rule else []

        optimization_config = self.config["optimization"]

        # å¦‚æœæœªå¯ç”¨åˆå¹¶ä¼˜åŒ–ï¼Œè¿”å›å•ç‹¬çš„è§„åˆ™
        if not optimization_config.get("merge_domains", True):
            return [self.domain_to_regex(domain) for domain in domains]

        # ä½¿ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼
        print(f"ğŸ”§ å¯ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼ï¼Œå¤„ç† {len(domains)} ä¸ªåŸŸå")
        rules = self.create_multiple_optimized_rules(domains)

        print(f"  âœ… ä¼˜åŒ–å®Œæˆ: {len(domains)} ä¸ªåŸŸå -> {len(rules)} ä¸ªè§„åˆ™")
        return rules

    def collect_domains(self) -> Dict[str, Set[str]]:
        """
        ä»æ‰€æœ‰é…ç½®çš„æºæ”¶é›†åŸŸå

        Returns:
            æŒ‰åŠ¨ä½œåˆ†ç±»çš„åŸŸåé›†åˆ
        """
        categorized_domains = {
            'remove': set(),
            'low_priority': set(),
            'high_priority': set()
        }

        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0,
            'whitelist_filtered': 0
        }

        for source in self.config["sources"]:
            if not source.get("enabled", True):
                continue

            print(f"\nå¤„ç†æ•°æ®æº: {source['name']}")
            format_type = source.get("format", "domain")
            print(f"æ ¼å¼ç±»å‹: {format_type}")

            domains, source_stats = self.fetch_domain_list(source["url"], format_type, source["name"])

            # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
            for key in self.stats:
                self.stats[key] += source_stats[key]

            action = source.get("action", "remove")
            if action in categorized_domains:
                categorized_domains[action].update(domains)
                print(f"å·²æ·»åŠ  {len(domains)} ä¸ªåŸŸååˆ° {action} ç±»åˆ«")
            else:
                print(f"è­¦å‘Š: æœªçŸ¥çš„åŠ¨ä½œç±»å‹ '{action}'")

        return categorized_domains

    def sort_rules(self, rules: Union[Dict, List]) -> Union[Dict, List]:
        """
        å¯¹è§„åˆ™è¿›è¡Œæ’åº

        Args:
            rules: è§„åˆ™æ•°æ®

        Returns:
            æ’åºåçš„è§„åˆ™
        """
        if isinstance(rules, dict):
            # å¯¹å­—å…¸æŒ‰é”®æ’åº
            return OrderedDict(sorted(rules.items()))
        elif isinstance(rules, list):
            # å¯¹åˆ—è¡¨æŒ‰å€¼æ’åº
            return sorted(rules)
        else:
            return rules

    def generate_rules(self) -> Dict[str, any]:
        """
        ç”Ÿæˆå„ç±»è§„åˆ™

        Returns:
            è§„åˆ™å­—å…¸
        """
        print("\nå¼€å§‹ç”Ÿæˆ SearXNG hostnames è§„åˆ™...")

        # æ˜¾ç¤ºä¼˜åŒ–æ¨¡å¼ä¿¡æ¯
        if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
            print("ğŸš€ å·²å¯ç”¨é«˜çº§TLDä¼˜åŒ–å•è¡Œæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼")
            print("   æ¯ä¸ªç±»åˆ«å°†ç”Ÿæˆå•ä¸ªåŒ…å«æ‰€æœ‰åŸŸåçš„é«˜çº§ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼")
        else:
            print("ğŸ”§ å·²å¯ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼")
            print("   å°†ç”Ÿæˆå¤šä¸ªæ€§èƒ½ä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™")

        # æ˜¾ç¤ºè§£æé…ç½®
        parsing_config = self.config["parsing"]
        print(f"ğŸ“ è§£æé…ç½®:")
        print(f"   - å¿½ç•¥ç‰¹å®šè·¯å¾„è§„åˆ™: {parsing_config.get('ignore_specific_paths', True)}")
        print(f"   - ä¸¥æ ¼åŸŸåçº§åˆ«æ£€æŸ¥: {parsing_config.get('strict_domain_level_check', True)}")

        # æ˜¾ç¤ºç™½åå•é…ç½®
        whitelist_config = self.config.get("whitelist", {})
        if whitelist_config.get("enabled", False):
            print(f"ğŸš« ç™½åå•é…ç½®:")
            print(f"   - ç™½åå•æ¨¡å¼: {whitelist_config.get('mode', 'remove_from_all')}")
            print(f"   - ç²¾ç¡®åŸŸå: {len(whitelist_config.get('domains', []))} ä¸ª")
            print(f"   - æ¨¡å¼è§„åˆ™: {len(whitelist_config.get('patterns', []))} ä¸ª")
            print(f"   - é€šé…ç¬¦åŸŸå: {len(whitelist_config.get('wildcard_domains', []))} ä¸ª")
            print(f"   - å¤–éƒ¨æº: {len(whitelist_config.get('sources', []))} ä¸ª")
            print(f"   âš ï¸  ä½¿ç”¨ç²¾ç¡®åŒ¹é…æ¨¡å¼ï¼Œå­åŸŸåéœ€è¦é€šé…ç¬¦æˆ–æ­£åˆ™è¡¨è¾¾å¼")
        else:
            print(f"ğŸš« ç™½åå•åŠŸèƒ½å·²ç¦ç”¨")

        # æ”¶é›†åŸŸå
        categorized_domains = self.collect_domains()

        rules = {}

        # æ›¿æ¢è§„åˆ™ (å­—å…¸æ ¼å¼)
        if self.config["replace_rules"]:
            rules["replace"] = self.config["replace_rules"]
            # è®°å½•æ›¿æ¢è§„åˆ™çš„åŸŸåæ•°é‡ï¼ˆæ›¿æ¢è§„åˆ™æ˜¯é”®å€¼å¯¹ï¼Œé”®å°±æ˜¯åŸŸåè§„åˆ™ï¼‰
            self.category_domain_counts["replace"] = len(self.config["replace_rules"])

        # ç§»é™¤è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆç§»é™¤è§„åˆ™...")
        remove_rules = []
        remove_rules.extend(self.config["fixed_remove"])

        # è®°å½•å›ºå®šç§»é™¤è§„åˆ™æ•°é‡
        fixed_remove_count = len(self.config["fixed_remove"])

        if categorized_domains["remove"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['remove'])} ä¸ªç§»é™¤åŸŸå...")
            self.category_domain_counts["remove"] = len(categorized_domains["remove"]) + fixed_remove_count
            merged_remove_rules = self.merge_domains_to_regex(categorized_domains["remove"])
            remove_rules.extend(merged_remove_rules)
        else:
            self.category_domain_counts["remove"] = fixed_remove_count

        if remove_rules:
            rules["remove"] = remove_rules

        # ä½ä¼˜å…ˆçº§è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆä½ä¼˜å…ˆçº§è§„åˆ™...")
        low_priority_rules = []
        low_priority_rules.extend(self.config["fixed_low_priority"])

        # è®°å½•å›ºå®šä½ä¼˜å…ˆçº§è§„åˆ™æ•°é‡
        fixed_low_priority_count = len(self.config["fixed_low_priority"])

        if categorized_domains["low_priority"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['low_priority'])} ä¸ªä½ä¼˜å…ˆçº§åŸŸå...")
            self.category_domain_counts["low_priority"] = len(categorized_domains["low_priority"]) + fixed_low_priority_count
            merged_low_priority_rules = self.merge_domains_to_regex(categorized_domains["low_priority"])
            low_priority_rules.extend(merged_low_priority_rules)
        else:
            self.category_domain_counts["low_priority"] = fixed_low_priority_count

        if low_priority_rules:
            rules["low_priority"] = low_priority_rules

        # é«˜ä¼˜å…ˆçº§è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆé«˜ä¼˜å…ˆçº§è§„åˆ™...")
        high_priority_rules = []
        high_priority_rules.extend(self.config["fixed_high_priority"])

        # è®°å½•å›ºå®šé«˜ä¼˜å…ˆçº§è§„åˆ™æ•°é‡
        fixed_high_priority_count = len(self.config["fixed_high_priority"])

        if categorized_domains["high_priority"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['high_priority'])} ä¸ªé«˜ä¼˜å…ˆçº§åŸŸå...")
            self.category_domain_counts["high_priority"] = len(categorized_domains["high_priority"]) + fixed_high_priority_count
            merged_high_priority_rules = self.merge_domains_to_regex(categorized_domains["high_priority"])
            high_priority_rules.extend(merged_high_priority_rules)
        else:
            self.category_domain_counts["high_priority"] = fixed_high_priority_count

        if high_priority_rules:
            rules["high_priority"] = high_priority_rules

        # å¯¹æ‰€æœ‰è§„åˆ™è¿›è¡Œæ’åºå’Œå»é‡
        for rule_type in rules:
            if rule_type == "replace":
                # æ›¿æ¢è§„åˆ™æŒ‰é”®æ’åº
                rules[rule_type] = self.sort_rules(rules[rule_type])
            else:
                # åˆ—è¡¨è§„åˆ™å»é‡å¹¶æ’åº
                rules[rule_type] = self.sort_rules(list(set(rules[rule_type])))

        return rules

    def save_separate_files(self, rules: Dict[str, any]) -> None:
        """
        ä¿å­˜ä¸ºåˆ†ç¦»çš„æ–‡ä»¶

        Args:
            rules: è§„åˆ™å­—å…¸
        """
        output_dir = self.config["output"]["directory"]
        files_config = self.config["output"]["files"]

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # ç”Ÿæˆä¸»é…ç½®æ–‡ä»¶ (ç”¨äºå¼•ç”¨å¤–éƒ¨æ–‡ä»¶)
        main_config = {"hostnames": {}}

        # ä¿å­˜å„ç±»è§„åˆ™åˆ°å•ç‹¬æ–‡ä»¶
        for rule_type, rule_data in rules.items():
            if rule_type in files_config and rule_data:
                filename = files_config[rule_type]
                filepath = os.path.join(output_dir, filename)

                try:
                    with open(filepath, 'w', encoding='utf-8') as f:
                        # æ·»åŠ æ–‡ä»¶å¤´æ³¨é‡Š
                        rule_count = len(rule_data) if isinstance(rule_data, (list, dict)) else 0
                        domain_count = self.category_domain_counts.get(rule_type, 0)

                        f.write(f"# SearXNG {rule_type} rules\n")
                        f.write(f"# Generated by SearXNG Hostnames Generator (Enhanced - Fixed TLD Issues + Precise Whitelist)\n")
                        f.write(f"# Total rules: {rule_count}\n")
                        f.write(f"# Total domains: {domain_count}\n")

                        if domain_count > 0 and rule_count > 0:
                            compression_ratio = (rule_count / domain_count) * 100
                            f.write(f"# Compression ratio: {compression_ratio:.1f}% ({domain_count} domains -> {rule_count} rules)\n")

                        # æ ¹æ®æ˜¯å¦å¯ç”¨å•è¡Œæ­£åˆ™æ˜¾ç¤ºä¸åŒæ³¨é‡Š
                        if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
                            f.write(f"# Advanced TLD-optimized single-line regex mode enabled\n")
                        else:
                            f.write(f"# Multi-rule performance optimized with advanced pattern matching\n")

                        f.write(f"# Note: Rules targeting specific paths are ignored to prevent over-blocking\n")
                        f.write(f"# Fixed: TLD optimization now preserves complete domain structure\n")
                        f.write(f"# Smart domain sorting and TLD grouping applied for optimal performance\n")

                        # æ·»åŠ ç™½åå•ä¿¡æ¯
                        if self.config.get("whitelist", {}).get("enabled", False):
                            f.write(f"# Precise whitelist filtering enabled - {self.stats.get('whitelist_filtered', 0)} domains filtered\n")
                            f.write(f"# Whitelist uses exact matching - use wildcards for subdomain matching\n")

                        f.write(f"\n")

                        # ç›´æ¥å†™å…¥è§„åˆ™å†…å®¹ï¼Œä¸åŒ…å«é¡¶çº§é”®
                        yaml.dump(rule_data, f, default_flow_style=False, allow_unicode=True, indent=2)

                    print(f"å·²ä¿å­˜ {rule_type} è§„åˆ™åˆ°: {filepath}")

                    # åœ¨ä¸»é…ç½®ä¸­å¼•ç”¨å¤–éƒ¨æ–‡ä»¶
                    main_config["hostnames"][rule_type] = filename

                except Exception as e:
                    print(f"ä¿å­˜ {rule_type} è§„åˆ™å¤±è´¥: {e}")

        # ä¿å­˜ä¸»é…ç½®æ–‡ä»¶
        if main_config["hostnames"]:
            main_config_path = os.path.join(output_dir, files_config["main_config"])
            try:
                with open(main_config_path, 'w', encoding='utf-8') as f:
                    f.write("# SearXNG hostnames configuration\n")
                    f.write("# This file references external rule files\n")
                    f.write("# Generated by SearXNG Hostnames Generator (Enhanced - Fixed TLD Issues + Precise Whitelist)\n")

                    if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
                        f.write("# Advanced TLD-optimized single-line regex mode enabled\n")
                    else:
                        f.write("# Multi-rule performance optimized with advanced pattern matching\n")

                    f.write("# Fixed: TLD optimization preserves complete domain structure (.pixnet.net vs .pixnet)\n")
                    f.write("# Smart domain sorting and TLD grouping applied\n")

                    # æ·»åŠ ç™½åå•ä¿¡æ¯
                    if self.config.get("whitelist", {}).get("enabled", False):
                        f.write("# Precise whitelist filtering enabled for exact control\n")

                    f.write("\n")
                    yaml.dump(main_config, f, default_flow_style=False, allow_unicode=True, indent=2)
                print(f"å·²ä¿å­˜ä¸»é…ç½®åˆ°: {main_config_path}")
            except Exception as e:
                print(f"ä¿å­˜ä¸»é…ç½®å¤±è´¥: {e}")

    def save_single_file(self, rules: Dict[str, any]) -> None:
        """
        ä¿å­˜ä¸ºå•ä¸ªæ–‡ä»¶

        Args:
            rules: è§„åˆ™å­—å…¸
        """
        output_dir = self.config["output"]["directory"]
        os.makedirs(output_dir, exist_ok=True)

        # æ„å»ºå®Œæ•´çš„ hostnames é…ç½®
        hostnames_config = {"hostnames": rules}

        filepath = os.path.join(output_dir, "hostnames.yml")

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("# SearXNG hostnames configuration\n")
                f.write("# Generated by SearXNG Hostnames Generator (Enhanced - Fixed TLD Issues + Precise Whitelist)\n")

                # æ·»åŠ æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
                total_rules = sum(len(rule_data) if isinstance(rule_data, (list, dict)) else 0 for rule_data in rules.values())
                total_domains = sum(self.category_domain_counts.values())
                f.write(f"# Total rules: {total_rules}\n")
                f.write(f"# Total domains: {total_domains}\n")

                if total_domains > 0 and total_rules > 0:
                    compression_ratio = (total_rules / total_domains) * 100
                    f.write(f"# Overall compression ratio: {compression_ratio:.1f}%\n")

                if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
                    f.write("# Advanced TLD-optimized single-line regex mode enabled\n")
                else:
                    f.write("# Multi-rule performance optimized with advanced pattern matching\n")

                f.write("# Note: Rules targeting specific paths are ignored to prevent over-blocking\n")
                f.write("# Fixed: TLD optimization now preserves complete domain structure (*.pixnet.net vs *.pixnet)\n")
                f.write("# Smart domain sorting and TLD grouping applied for optimal performance\n")

                # æ·»åŠ ç™½åå•ä¿¡æ¯
                if self.config.get("whitelist", {}).get("enabled", False):
                    f.write(f"# Precise whitelist filtering enabled - {self.stats.get('whitelist_filtered', 0)} domains filtered\n")
                    f.write(f"# Whitelist uses exact matching - use wildcards for subdomain matching\n")

                f.write("\n")
                yaml.dump(hostnames_config, f, default_flow_style=False, allow_unicode=True, indent=2)

            print(f"å·²ä¿å­˜å®Œæ•´é…ç½®åˆ°: {filepath}")

        except Exception as e:
            print(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")

    def run(self) -> None:
        """
        è¿è¡Œç”Ÿæˆå™¨
        """
        print("SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨å¯åŠ¨ (é«˜çº§ä¼˜åŒ–ç‰ˆ - ä¿®å¤TLDé—®é¢˜ + ç²¾ç¡®ç™½åå•åŠŸèƒ½)")
        print("=" * 85)

        try:
            # ç”Ÿæˆè§„åˆ™
            rules = self.generate_rules()

            # æ ¹æ®é…ç½®ä¿å­˜æ–‡ä»¶
            if self.config["output"]["mode"] == "separate_files":
                self.save_separate_files(rules)
            else:
                self.save_single_file(rules)

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.print_statistics(rules)

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
        except Exception as e:
            print(f"\nç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

    def print_statistics(self, rules: Dict[str, any]) -> None:
        """
        è¾“å‡ºç»Ÿè®¡ä¿¡æ¯

        Args:
            rules: ç”Ÿæˆçš„è§„åˆ™
        """
        print("\n" + "=" * 60)
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")

        total_rules = 0
        total_domains = 0

        for rule_type, rule_data in rules.items():
            if isinstance(rule_data, dict):
                rule_count = len(rule_data)
                domain_count = self.category_domain_counts.get(rule_type, 0)
                print(f"  {rule_type} è§„åˆ™: {rule_count} æ¡ (åŒ…å« {domain_count} ä¸ªåŸŸå)")
                total_rules += rule_count
                total_domains += domain_count
            elif isinstance(rule_data, list):
                rule_count = len(rule_data)
                domain_count = self.category_domain_counts.get(rule_type, 0)
                print(f"  {rule_type} è§„åˆ™: {rule_count} æ¡ (åŒ…å« {domain_count} ä¸ªåŸŸå)")
                total_rules += rule_count
                total_domains += domain_count

                # å¦‚æœæ˜¯å•è¡Œæ­£åˆ™æ¨¡å¼ï¼Œæ˜¾ç¤ºè§„åˆ™é•¿åº¦ä¿¡æ¯
                if (self.force_single_regex or self.config["optimization"].get("force_single_regex", False)) and rule_data:
                    for i, rule in enumerate(rule_data, 1):
                        rule_length = len(rule)
                        if rule_length > 1000:
                            print(f"    è§„åˆ™ {i} é•¿åº¦: {rule_length:,} å­—ç¬¦")

        print(f"\nğŸ“ˆ æ€»è®¡: {total_rules} æ¡è§„åˆ™ (åŒ…å« {total_domains} ä¸ªåŸŸå)")

        print(f"\nğŸ” è§£æç»Ÿè®¡:")
        print(f"  - æ€»è¾“å…¥è§„åˆ™: {self.stats['total_rules']:,}")
        print(f"  - æˆåŠŸè§£æåŸŸå: {self.stats['parsed_domains']:,}")
        print(f"  - å¿½ç•¥(ç‰¹å®šè·¯å¾„): {self.stats['ignored_with_path']:,}")
        print(f"  - å¿½ç•¥(æ³¨é‡Š): {self.stats['ignored_comments']:,}")
        print(f"  - å¿½ç•¥(æ— æ•ˆåŸŸå): {self.stats['invalid_domains']:,}")
        print(f"  - é‡å¤åŸŸå: {self.stats['duplicate_domains']:,}")
        print(f"  - ç™½åå•è¿‡æ»¤: {self.stats['whitelist_filtered']:,}")

        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.config['output']['directory']}")

        print(f"\nğŸ“¡ æ•°æ®æº:")
        for source in self.config["sources"]:
            if source.get("enabled", True):
                print(f"  âœ… {source['name']} ({source.get('format', 'domain')})")
            else:
                print(f"  âŒ {source['name']} (å·²ç¦ç”¨)")

        print(f"\nâš™ï¸  é…ç½®:")
        print(f"  - å¿½ç•¥ç‰¹å®šè·¯å¾„è§„åˆ™: {self.config['parsing']['ignore_specific_paths']}")
        print(f"  - ä¸¥æ ¼åŸŸåçº§åˆ«æ£€æŸ¥: {self.config['parsing'].get('strict_domain_level_check', True)}")
        print(f"  - å¿½ç•¥IPåœ°å€: {self.config['parsing']['ignore_ip']}")
        print(f"  - å¿½ç•¥localhost: {self.config['parsing']['ignore_localhost']}")

        # ç™½åå•é…ç½®
        whitelist_config = self.config.get("whitelist", {})
        print(f"\nğŸš« ç™½åå•é…ç½®:")
        if whitelist_config.get("enabled", False):
            print(f"  - çŠ¶æ€: å·²å¯ç”¨ (ç²¾ç¡®åŒ¹é…æ¨¡å¼)")
            print(f"  - æ¨¡å¼: {whitelist_config.get('mode', 'remove_from_all')}")
            print(f"  - ç²¾ç¡®åŸŸå: {len(whitelist_config.get('domains', []))} ä¸ª")
            print(f"  - æ¨¡å¼è§„åˆ™: {len(whitelist_config.get('patterns', []))} ä¸ª")
            print(f"  - é€šé…ç¬¦åŸŸå: {len(whitelist_config.get('wildcard_domains', []))} ä¸ª")
            print(f"  - å¤–éƒ¨æº: {len([s for s in whitelist_config.get('sources', []) if s.get('enabled', True)])} ä¸ª")
            print(f"  - è¿‡æ»¤çš„åŸŸå: {self.stats['whitelist_filtered']:,} ä¸ª")
            print(f"  âš ï¸  æ³¨æ„: ç²¾ç¡®åŒ¹é…åªåŒ¹é…å®Œå…¨ç›¸åŒçš„åŸŸå")
        else:
            print(f"  - çŠ¶æ€: å·²ç¦ç”¨")

        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        opt_config = self.config["optimization"]
        print(f"\nğŸš€ æ€§èƒ½ä¼˜åŒ–:")
        print(f"  - å¯ç”¨åŸŸååˆå¹¶: {opt_config.get('merge_domains', True)}")
        print(f"  - æ™ºèƒ½åŸŸåæ’åº: {opt_config.get('sort_before_merge', True)}")
        print(f"  - é«˜çº§TLDä¼˜åŒ–: {opt_config.get('enable_advanced_tld_merge', True)}")
        print(f"  - å¼ºåˆ¶å•è¡Œæ­£åˆ™: {self.force_single_regex or opt_config.get('force_single_regex', False)}")

        if not (self.force_single_regex or opt_config.get('force_single_regex', False)):
            print(f"  - æ¯è§„åˆ™æœ€å¤§åŸŸåæ•°: {opt_config.get('max_domains_per_rule', 30)}")
            print(f"  - æŒ‰TLDåˆ†ç»„: {opt_config.get('group_by_tld', True)}")
            print(f"  - æœ€å¤§è§„åˆ™é•¿åº¦: {opt_config.get('max_rule_length', 4000):,}")

        print(f"  - å‰ç¼€ä¼˜åŒ–: {opt_config.get('enable_prefix_optimization', True)}")
        print(f"  - åç¼€ä¼˜åŒ–: {opt_config.get('enable_suffix_optimization', True)}")

        print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        if self.config["output"]["mode"] == "separate_files":
            print("åœ¨ SearXNG settings.yml ä¸­æ·»åŠ :")
            print("hostnames:")
            for rule_type, filename in self.config["output"]["files"].items():
                if rule_type != "main_config" and rule_type in rules:
                    print(f"  {rule_type}: '{filename}'")
        else:
            print("å°†ç”Ÿæˆçš„ hostnames.yml å†…å®¹å¤åˆ¶åˆ° SearXNG settings.yml ä¸­")

        print(f"\nâœ¨ ä¼˜åŒ–æ•ˆæœ:")
        if total_domains > 0 and total_rules > 0:
            compression_ratio = (total_rules / total_domains) * 100
            print(f"  - å‹ç¼©æ¯”ç‡: {compression_ratio:.1f}% ({total_domains:,} ä¸ªåŸŸå -> {total_rules} æ¡è§„åˆ™)")
            if compression_ratio < 10:
                print("  - ğŸ‰ å‹ç¼©æ•ˆæœæä½³ï¼å¤§é‡åŸŸåè¢«åˆå¹¶ä¼˜åŒ–")
            elif compression_ratio < 50:
                print("  - ğŸ‘ å‹ç¼©æ•ˆæœè‰¯å¥½")
            else:
                print("  - ğŸ“ è§„åˆ™è¾ƒå¤šï¼Œå¯è€ƒè™‘å¯ç”¨å•è¡Œæ­£åˆ™æ¨¡å¼")

        # æ˜¾ç¤ºå„ç±»åˆ«çš„å‹ç¼©æƒ…å†µ
        print(f"\nğŸ“ˆ å„ç±»åˆ«å‹ç¼©è¯¦æƒ…:")
        for rule_type, rule_data in rules.items():
            if isinstance(rule_data, (list, dict)):
                rule_count = len(rule_data)
                domain_count = self.category_domain_counts.get(rule_type, 0)
                if domain_count > 0 and rule_count > 0:
                    category_ratio = (rule_count / domain_count) * 100
                    print(f"  - {rule_type}: {category_ratio:.1f}% ({domain_count} ä¸ªåŸŸå -> {rule_count} æ¡è§„åˆ™)")

        print(f"\nğŸ”§ æ–°å¢åŠŸèƒ½:")
        print(f"  - âœ… ç²¾ç¡®ç™½åå•ï¼šåªåŒ¹é…å®Œå…¨ç›¸åŒçš„åŸŸåï¼Œé¿å…è¯¯æ”¾è¡Œ")
        print(f"  - âœ… é€šé…ç¬¦æ”¯æŒï¼šä½¿ç”¨ *.baidu.com åŒ¹é…æ‰€æœ‰ç™¾åº¦å­åŸŸå")
        print(f"  - âœ… æ­£åˆ™è¡¨è¾¾å¼ï¼šæ”¯æŒå¤æ‚çš„åŸŸååŒ¹é…æ¨¡å¼")
        print(f"  - âœ… å¤šæºç™½åå•ï¼šæ”¯æŒä»URLå’Œæœ¬åœ°æ–‡ä»¶åŠ è½½ç™½åå•")
        print(f"  - âœ… æºç‰¹å®šç™½åå•ï¼šå¯å¯¹ç‰¹å®šæ•°æ®æºåº”ç”¨ä¸åŒç™½åå•")
        print(f"  - âœ… å®æ—¶è¿‡æ»¤ç»Ÿè®¡ï¼šæ˜¾ç¤ºè¢«ç™½åå•è¿‡æ»¤çš„åŸŸåæ•°é‡")

        print(f"\nğŸ”§ ç™½åå•åŒ¹é…ç¤ºä¾‹:")
        print(f"  - ç²¾ç¡®åŒ¹é…: 'baidu.com' åªåŒ¹é… baidu.comï¼Œä¸åŒ¹é… test.baidu.com")
        print(f"  - é€šé…ç¬¦åŒ¹é…: '*.baidu.com' åŒ¹é… test.baidu.comã€www.baidu.com ç­‰")
        print(f"  - æ­£åˆ™åŒ¹é…: '.*\\.edu\\..*' åŒ¹é…æ‰€æœ‰æ•™è‚²ç½‘ç«™")

        print(f"\nğŸ”§ ä¿®å¤è¯´æ˜:")
        print(f"  - ä¿®å¤äº†TLDä¼˜åŒ–ä¸­ä¸¢å¤±é¡¶çº§åŸŸåçš„é—®é¢˜")
        print(f"  - ç°åœ¨ 'a0cuy6cmk2.pixnet.net' ä¼šæ­£ç¡®ç”Ÿæˆä¸º '*.pixnet.net$' è€Œä¸æ˜¯ '*.pixnet$'")
        print(f"  - æ”¹è¿›äº†å¤šçº§åŸŸåçš„TLDåˆ†ç»„å’Œä¼˜åŒ–é€»è¾‘")
        print(f"  - å¢å¼ºäº†åŸŸåç»“æ„åˆ†æï¼Œç¡®ä¿TLDå®Œæ•´æ€§")
        print(f"  - ç§»é™¤äº†çˆ¶åŸŸååŒ¹é…ï¼Œä½¿ç”¨ç²¾ç¡®åŒ¹é…é¿å…è¯¯æ”¾è¡Œ")


def create_sample_config():
    """
    åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶
    """
    sample_config = {
        "sources": [
            {
                "name": "Google Chinese Results Blocklist",
                "url": "https://raw.githubusercontent.com/cobaltdisco/Google-Chinese-Results-Blocklist/refs/heads/master/GHHbD_perma_ban_list.txt",
                "action": "remove",
                "format": "domain",
                "enabled": True
            },
            {
                "name": "Chinese Internet is Dead",
                "url": "https://raw.githubusercontent.com/obgnail/chinese-internet-is-dead/master/blocklist.txt",
                "action": "remove",
                "format": "ublock",
                "enabled": True
            },
            {
                "name": "Luxirty Search Block List",
                "url": "https://raw.githubusercontent.com/KoriIku/luxirty-search/refs/heads/main/docs/block_list.txt",
                "action": "remove",
                "format": "domain",
                "enabled": True
            },
            {
                "name": "Custom Blocklist",
                "url": "https://example.com/blocklist.txt",
                "action": "remove",
                "format": "domain",
                "enabled": False
            }
        ],
        "whitelist": {
            "enabled": True,
            "mode": "remove_from_all",
            "sources": [
                {
                    "name": "Custom Whitelist",
                    "url": "https://example.com/whitelist.txt",
                    "format": "domain",
                    "enabled": False
                },
                {
                    "name": "Local Whitelist",
                    "file": "./whitelist.txt",
                    "format": "domain",
                    "enabled": False
                }
            ],
            "domains": [
                "baidu.com",
                "google.com",
                "bing.com",
                "github.com",
                "stackoverflow.com"
            ],
            "patterns": [
                r".*\.gov\..*",
                r".*\.edu\..*",
                r".*wikipedia\..*"
            ],
            "wildcard_domains": [
                "*.github.com",
                "*.stackoverflow.com",
                "*.microsoft.com",
                "*.baidu.com"
            ],
            "source_specific": {
                "Chinese Internet is Dead": {
                    "domains": ["zhihu.com", "csdn.net"],
                    "patterns": [r".*\.edu\.cn$"],
                    "wildcard_domains": ["*.tsinghua.edu.cn"]
                }
            }
        },
        "replace_rules": {
            '(.*\.)?youtube\.com$': 'yt.example.com',
            '(.*\.)?youtu\.be$': 'yt.example.com',
            '(.*\.)?reddit\.com$': 'teddit.example.com',
            '(.*\.)?redd\.it$': 'teddit.example.com',
            '(www\.)?twitter\.com$': 'nitter.example.com'
        },
        "fixed_remove": [
            '(.*\.)?facebook.com$'
        ],
        "fixed_low_priority": [
            '(.*\.)?google(\..*)?$'
        ],
        "fixed_high_priority": [
            '(.*\.)?wikipedia.org$'
        ],
        "parsing": {
            "ignore_specific_paths": True,
            "ignore_ip": True,
            "ignore_localhost": True,
            "strict_domain_level_check": True
        },
        "optimization": {
            "merge_domains": True,
            "max_domains_per_rule": 256,
            "group_by_tld": True,
            "use_trie_optimization": True,
            "max_rule_length": 65536,
            "optimize_tld_grouping": True,
            "enable_prefix_optimization": True,
            "enable_suffix_optimization": True,
            "min_common_prefix_length": 3,
            "min_common_suffix_length": 3,
            "force_single_regex": False,
            "sort_before_merge": True,
            "enable_advanced_tld_merge": True
        },
        "request_config": {
            "timeout": 30,
            "retry_count": 3,
            "retry_delay": 1
        },
        "output": {
            "mode": "separate_files",
            "directory": "./rules/",
            "files": {
                "replace": "rewrite-hosts.yml",
                "remove": "remove-hosts.yml",
                "low_priority": "low-priority-hosts.yml",
                "high_priority": "high-priority-hosts.yml",
                "main_config": "hostnames-config.yml"
            }
        }
    }

    with open("config.yaml", "w", encoding="utf-8") as f:
        yaml.dump(sample_config, f, default_flow_style=False, allow_unicode=True, indent=2)

    print("ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²åˆ›å»º: config.yaml")
    print("\nğŸš« ç²¾ç¡®ç™½åå•é…ç½®è¯´æ˜:")
    print("  - domains: ç²¾ç¡®åŒ¹é…çš„åŸŸååˆ—è¡¨ï¼ˆåªåŒ¹é…å®Œå…¨ç›¸åŒçš„åŸŸåï¼‰")
    print("  - patterns: æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åˆ—è¡¨")
    print("  - wildcard_domains: é€šé…ç¬¦åŸŸååˆ—è¡¨ï¼ˆå¦‚ *.baidu.com åŒ¹é…æ‰€æœ‰ç™¾åº¦å­åŸŸåï¼‰")
    print("  - sources: ä»URLæˆ–æœ¬åœ°æ–‡ä»¶åŠ è½½ç™½åå•")
    print("  - source_specific: å¯¹ç‰¹å®šæ•°æ®æºåº”ç”¨çš„ç™½åå•")
    print("\nğŸ’¡ åŒ¹é…ç¤ºä¾‹:")
    print("  - 'baidu.com' åªåŒ¹é… baidu.comï¼Œä¸åŒ¹é… test.baidu.com")
    print("  - '*.baidu.com' åŒ¹é… test.baidu.comã€www.baidu.com ç­‰å­åŸŸå")
    print("  - å¦‚æœè¦å±è”½ test.baidu.com ä½†ä¿ç•™ baidu.comï¼Œä¸è¦åœ¨ç™½åå•ä¸­æ·»åŠ  baidu.com")


def main():
    parser = argparse.ArgumentParser(description="SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨ (é«˜çº§ä¼˜åŒ–ç‰ˆ - ä¿®å¤TLDé—®é¢˜ + ç²¾ç¡®ç™½åå•åŠŸèƒ½)")
    parser.add_argument("-c", "--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--create-config", action="store_true", help="åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶")
    parser.add_argument("--single-regex", action="store_true", help="å¼ºåˆ¶ç”Ÿæˆé«˜çº§TLDä¼˜åŒ–çš„å•è¡Œæ­£åˆ™è¡¨è¾¾å¼ï¼ˆå°†æ‰€æœ‰åŸŸååˆå¹¶ä¸ºä¸€ä¸ªè§„åˆ™ï¼‰")

    args = parser.parse_args()

    if args.create_config:
        create_sample_config()
        return

    generator = SearXNGHostnamesGenerator(args.config, force_single_regex=args.single_regex)
    generator.run()


if __name__ == "__main__":
    main()
