#!/usr/bin/env python3
"""
SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨ - å®Œå–„ç‰ˆ (æ”¯æŒ v2ray æ ¼å¼ - ä¿æŒåŸå§‹ç»“æ„)
- æ”¯æŒä½ä¼˜å…ˆçº§/é«˜ä¼˜å…ˆçº§/æ›¿æ¢è§„åˆ™ä»å¤–éƒ¨æ–‡ä»¶è¯»å–
- ç™½åå•åŠŸèƒ½æ”¹ä¸ºè‡ªåŠ¨åˆ†ç±»è¯­æ³•åŠŸèƒ½ï¼Œæ”¯æŒ remove:baidu.com ç­‰è¯­æ³•
- ä¿®å¤ï¼šskip è§„åˆ™åªå½±å“æ•°æ®æºå¤„ç†ï¼Œä¸é˜»æ­¢æ˜ç¡®çš„è‡ªåŠ¨åˆ†ç±»è§„åˆ™
- æ–°å¢ï¼šæ”¯æŒ v2ray æ ¼å¼ (domain:example.com, full:example.com, domain:example.com:@tag)
- ä¿®æ­£ï¼šä¿æŒåŸå§‹åŸŸåç»“æ„ï¼Œä¸ç§»é™¤ www. ç­‰å‰ç¼€

pip install requests pyyaml argparse
"""

import requests
import yaml
import json
import re
from urllib.parse import urlparse
from typing import Dict, List, Set, Union, Tuple
import argparse
import sys
import time
import os
from collections import OrderedDict, defaultdict

class SearXNGHostnamesGenerator:
    def __init__(self, config_file: str = None, force_single_regex: bool = False):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            force_single_regex: å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™è¡¨è¾¾å¼
        """
        self.config = self.load_config(config_file)
        self.force_single_regex = force_single_regex
        self.domains = set()
        self.auto_classify_rules = []  # è‡ªåŠ¨åˆ†ç±»è§„åˆ™
        self.stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0,
            'auto_classified': 0,  # è‡ªåŠ¨åˆ†ç±»çš„æ•°é‡
            'auto_added': 0,  # ä¸»åŠ¨æ·»åŠ çš„åŸŸåæ•°é‡
            'skipped_from_sources': 0,  # ä»æ•°æ®æºè·³è¿‡çš„åŸŸåæ•°é‡
            'skip_overridden': 0,  # skip è§„åˆ™è¢«å…¶ä»–è§„åˆ™è¦†ç›–çš„æ•°é‡
            'v2ray_with_tags': 0,  # å¸¦æ ‡ç­¾çš„ v2ray è§„åˆ™æ•°é‡
        }
        # è®°å½•æ¯ä¸ªç±»åˆ«çš„åŸŸåæ•°é‡
        self.category_domain_counts = {
            'remove': 0,
            'low_priority': 0,
            'high_priority': 0,
            'replace': 0
        }

        # åŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™
        self.load_auto_classify_rules()

    def load_config(self, config_file: str) -> Dict:
        """
        åŠ è½½é…ç½®æ–‡ä»¶

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„

        Returns:
            é…ç½®å­—å…¸
        """
        default_config = {
            # æ•°æ®æºé…ç½®
            "sources": [
                {
                    "name": "Content Farm Terminator - Content Farm Filters",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/content-farms.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Content Farm Terminator - Nearly Content Farm Filters",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/nearly-content-farms.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Content Farm Terminator - Bad Cloners",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/bad-cloners.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Paxxs - Google Blocklist",
                    "url": "https://raw.githubusercontent.com/Paxxs/Google-Blocklist/refs/heads/develop/uBlacklist_subscription.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "cobaltdisco - Google Chinese Results Blocklist",
                    "url": "https://raw.githubusercontent.com/cobaltdisco/Google-Chinese-Results-Blocklist/refs/heads/master/uBlacklist_subscription.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "obgnail - Chinese Internet is Dead",
                    "url": "https://raw.githubusercontent.com/obgnail/chinese-internet-is-dead/master/blocklist.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "hezhijie0327 - Geosite2Domain - dev",
                    "url": "https://raw.githubusercontent.com/hezhijie0327/Geosite2Domain/refs/heads/main/category/category-dev.txt",
                    "action": "high_priority",
                    "format": "v2ray",
                    "enabled": True
                },
                {
                    "name": "hezhijie0327 - Geosite2Domain - scholar-!cn",
                    "url": "https://raw.githubusercontent.com/hezhijie0327/Geosite2Domain/refs/heads/main/category/category-scholar-!cn.txt",
                    "action": "high_priority",
                    "format": "v2ray",
                    "enabled": True
                },
            ],

            # è‡ªå®šä¹‰è§„åˆ™é…ç½®ï¼ˆä»æ–‡ä»¶è¯»å–ï¼‰
            "custom_rules": {
                "enabled": False,
                "sources": []
            },

            # è‡ªåŠ¨åˆ†ç±»è¯­æ³•é…ç½®ï¼ˆæ›¿æ¢åŸç™½åå•åŠŸèƒ½ï¼‰
            "auto_classify": {
                "enabled": True,
                "sources": [
                    {
                        "name": "Auto Classify Rules",
                        "file": "./auto_classify.txt",
                        "format": "classify",  # æ”¯æŒ action:domain è¯­æ³•
                        "enabled": True
                    },
                ],
                # ç›´æ¥åœ¨é…ç½®ä¸­å®šä¹‰çš„è‡ªåŠ¨åˆ†ç±»è§„åˆ™
                "rules": [
                    # è¯­æ³•ç¤ºä¾‹ï¼š
                    # "remove:example.com",           # å°† example.com æ·»åŠ åˆ°ç§»é™¤åˆ—è¡¨
                    # "low_priority:google.com",      # å°† google.com æ·»åŠ åˆ°ä½ä¼˜å…ˆçº§åˆ—è¡¨
                    # "high_priority:wikipedia.org",  # å°† wikipedia.org æ·»åŠ åˆ°é«˜ä¼˜å…ˆçº§åˆ—è¡¨
                    # "replace:youtube.com=yt.example.com",  # æ›¿æ¢è§„åˆ™
                    # "skip:baidu.com",               # è·³è¿‡ä»æ•°æ®æºå¤„ç†æ­¤åŸŸåï¼ˆä½†ä¸é˜»æ­¢å…¶ä»–è‡ªåŠ¨åˆ†ç±»è§„åˆ™ï¼‰
                ]
            },

            # åŸŸåæ›¿æ¢è§„åˆ™ï¼ˆä¿ç•™åŸé…ç½®æ–¹å¼ï¼‰
            "replace_rules": {
                #'(.*\.)?youtube\.com$': 'yt.example.com',
                #'(.*\.)?youtu\.be$': 'yt.example.com',
                #'(.*\.)?reddit\.com$': 'teddit.example.com',
                #'(.*\.)?redd\.it$': 'teddit.example.com',
                #'(www\.)?twitter\.com$': 'nitter.example.com'
            },

            # å›ºå®šçš„ç§»é™¤è§„åˆ™
            "fixed_remove": [
                #'(.*\.)?facebook.com$'
            ],

            # å›ºå®šçš„ä½ä¼˜å…ˆçº§è§„åˆ™
            "fixed_low_priority": [
                #'(.*\.)?google(\..*)?$'
            ],

            # å›ºå®šçš„é«˜ä¼˜å…ˆçº§è§„åˆ™
            "fixed_high_priority": [
                #'(.*\.)?wikipedia.org$'
            ],

            # è§£æé…ç½®
            "parsing": {
                "ignore_specific_paths": True,  # å¿½ç•¥æŒ‡å‘ç‰¹å®šè·¯å¾„çš„è§„åˆ™
                "ignore_ip": True,     # å¿½ç•¥IPåœ°å€
                "ignore_localhost": True,  # å¿½ç•¥æœ¬åœ°ä¸»æœº
                "strict_domain_level_check": True,  # ä¸¥æ ¼æ£€æŸ¥åŸŸåçº§åˆ«è§„åˆ™
                "preserve_www_prefix": True,  # ä¿æŒ www. å‰ç¼€
                "preserve_original_structure": True  # ä¿æŒåŸå§‹åŸŸåç»“æ„
            },

            # æ€§èƒ½ä¼˜åŒ–é…ç½®
            "optimization": {
                "merge_domains": True,          # å¯ç”¨åŸŸååˆå¹¶ä¼˜åŒ–
                "max_domains_per_rule": 256,     # æ¯ä¸ªåˆå¹¶è§„åˆ™çš„æœ€å¤§åŸŸåæ•°
                "group_by_tld": True,           # æŒ‰é¡¶çº§åŸŸååˆ†ç»„
                "use_trie_optimization": True,  # ä½¿ç”¨å­—å…¸æ ‘ä¼˜åŒ–
                "max_rule_length": 65536,       # å•ä¸ªè§„åˆ™çš„æœ€å¤§é•¿åº¦é™åˆ¶
                "optimize_tld_grouping": True,   # ä¼˜åŒ–TLDåˆ†ç»„ï¼Œé¿å…é‡å¤
                "enable_prefix_optimization": True,  # å¯ç”¨å‰ç¼€ä¼˜åŒ–
                "enable_suffix_optimization": True,  # å¯ç”¨åç¼€ä¼˜åŒ–
                "min_common_prefix_length": 3,      # æœ€å°å…¬å…±å‰ç¼€é•¿åº¦
                "min_common_suffix_length": 3,      # æœ€å°å…¬å…±åç¼€é•¿åº¦
                "force_single_regex": False,         # å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™è¡¨è¾¾å¼
                "sort_before_merge": True,          # åˆå¹¶å‰æ’åºåŸŸå
                "enable_advanced_tld_merge": True   # å¯ç”¨é«˜çº§TLDåˆå¹¶
            },

            # è¯·æ±‚é…ç½®
            "request_config": {
                "timeout": 30,
                "retry_count": 3,
                "retry_delay": 1
            },

            # è¾“å‡ºé…ç½®
            "output": {
                "mode": "separate_files",  # separate_files æˆ– single_file
                "directory": "./rules/",
                "files": {
                    "replace": "rewrite-hosts.yml",
                    "remove": "remove-hosts.yml",
                    "low_priority": "low-priority-hosts.yml",
                    "high_priority": "high-priority-hosts.yml",
                    "main_config": "hostnames-config.yml"
                }
            }
        }

        if config_file:
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    # æ·±åº¦åˆå¹¶é…ç½®
                    self._deep_merge(default_config, user_config)
            except FileNotFoundError:
                print(f"é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            except Exception as e:
                print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                return default_config

        return default_config

    def _deep_merge(self, base_dict: Dict, update_dict: Dict) -> None:
        """
        æ·±åº¦åˆå¹¶å­—å…¸
        """
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_merge(base_dict[key], value)
            else:
                base_dict[key] = value

    def parse_v2ray_rule(self, rule: str) -> Tuple[str, str]:
        """
        è§£æ v2ray æ ¼å¼è§„åˆ™ï¼Œæå–åŸŸåï¼Œä¿æŒåŸå§‹ç»“æ„

        æ”¯æŒçš„æ ¼å¼ï¼š
        - domain:example.com          # åŒ¹é…åŸŸååŠæ‰€æœ‰å­åŸŸå
        - full:example.com            # å®Œå…¨åŒ¹é…åŸŸå
        - domain:example.com:@tag     # å¸¦æ ‡ç­¾çš„åŸŸåè§„åˆ™
        - full:www.example.com:@tag   # ä¿æŒ www å‰ç¼€

        Args:
            rule: v2ray è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            (åŸŸåæˆ– None, å¿½ç•¥åŸå› )
        """
        original_rule = rule  # ä¿å­˜åŸå§‹è§„åˆ™ç”¨äºè°ƒè¯•
        rule = rule.strip()

        if not rule or rule.startswith('#'):
            return None, "æ³¨é‡Šæˆ–ç©ºè¡Œ"

        # å¤„ç†è¡Œæœ«æ³¨é‡Š
        if '#' in rule:
            comment_pos = rule.find('#')
            rule = rule[:comment_pos].strip()
            if not rule:
                return None, "ä»…åŒ…å«æ³¨é‡Š"

        # æ£€æŸ¥æ˜¯å¦æ˜¯ v2ray æ ¼å¼
        if ':' not in rule:
            return None, "é v2ray æ ¼å¼"

        # åˆ†ç¦»å„éƒ¨åˆ†ï¼šprefix:domain[:tag...]
        parts = rule.split(':')
        if len(parts) < 2:
            return None, "æ— æ•ˆçš„ v2ray æ ¼å¼"

        prefix = parts[0].strip().lower()

        # éªŒè¯å‰ç¼€
        if prefix not in ['domain', 'full']:
            return None, f"ä¸æ”¯æŒçš„ v2ray å‰ç¼€: {prefix}"

        # æå–åŸŸåéƒ¨åˆ†
        domain_part = parts[1].strip()

        if not domain_part:
            return None, "åŸŸåéƒ¨åˆ†ä¸ºç©º"

        # å¤„ç†æ ‡ç­¾éƒ¨åˆ†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        tag_info = None
        if len(parts) > 2:
            tag_parts = parts[2:]
            # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡ç­¾
            tags = [part.strip() for part in tag_parts if part.strip()]
            if tags:
                tag_info = ':'.join(tags)
                self.stats['v2ray_with_tags'] += 1
                # æ˜¾ç¤ºæ ‡ç­¾ä¿¡æ¯ç”¨äºè°ƒè¯•
                print(f"    ğŸ“ v2ray å¸¦æ ‡ç­¾: {original_rule} -> åŸŸå: {domain_part}, æ ‡ç­¾: {tag_info}")

        # éªŒè¯å’Œæ¸…ç†åŸŸåï¼ˆä¿æŒåŸå§‹ç»“æ„ï¼‰
        cleaned_domain = self._clean_v2ray_domain(domain_part)
        if not cleaned_domain:
            return None, "æ— æ•ˆåŸŸå"

        return cleaned_domain, None

    def _clean_v2ray_domain(self, domain: str) -> str:
        """
        ä¸“é—¨ä¸º v2ray åŸŸåæ¸…ç†çš„æ–¹æ³•ï¼Œä¿æŒåŸå§‹ç»“æ„

        Args:
            domain: v2ray åŸŸåéƒ¨åˆ†

        Returns:
            æ¸…ç†åçš„åŸŸåï¼ˆä¿æŒåŸå§‹ç»“æ„ï¼‰
        """
        if not domain:
            return None

        # ç§»é™¤åè®®ï¼ˆå¦‚æœæ„å¤–åŒ…å«ï¼‰
        if domain.startswith(('http://', 'https://')):
            parsed = urlparse(domain)
            domain = parsed.netloc
            if parsed.port:
                # å¦‚æœURLä¸­æœ‰ç«¯å£ï¼Œç§»é™¤å®ƒ
                domain = domain.replace(f':{parsed.port}', '')

        # å¯¹äº v2ray æ ¼å¼ï¼Œé€šå¸¸ä¸åº”è¯¥æœ‰ç«¯å£å·
        # ä½†å¦‚æœæœ‰æ˜æ˜¾çš„æ•°å­—ç«¯å£åˆ™ç§»é™¤
        if ':' in domain:
            parts = domain.split(':')
            if len(parts) == 2 and parts[1].isdigit() and int(parts[1]) <= 65535:
                # åªæœ‰å½“ç¬¬äºŒéƒ¨åˆ†æ˜¯æœ‰æ•ˆç«¯å£å·æ—¶æ‰ç§»é™¤
                domain = parts[0]
                print(f"    ğŸ”§ ç§»é™¤ç«¯å£å·: {':'.join(parts)} -> {domain}")

        # ç§»é™¤è·¯å¾„ï¼ˆå¦‚æœæ„å¤–åŒ…å«ï¼‰
        if '/' in domain:
            domain = domain.split('/')[0]

        # **ä¸ç§»é™¤ www. å‰ç¼€ - ä¿æŒåŸå§‹ç»“æ„**
        # è¿™é‡Œæ³¨é‡Šæ‰åŸæ¥çš„ä»£ç ï¼š
        # if domain.startswith('www.'):
        #     domain = domain[4:]

        # åªç§»é™¤æ˜æ˜¾æ— å…³çš„å­—ç¬¦ï¼Œä¿ç•™åŸŸåçš„å®Œæ•´æ€§
        # ä¸ä½¿ç”¨æ¿€è¿›çš„å­—ç¬¦è¿‡æ»¤ï¼Œåªç§»é™¤æ˜æ˜¾çš„ç©ºç™½å­—ç¬¦
        domain = domain.strip()

        # æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€
        if self.config["parsing"]["ignore_ip"] and self.is_ip_address(domain):
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯localhost
        if self.config["parsing"]["ignore_localhost"] and domain in ['localhost', '127.0.0.1', '0.0.0.0']:
            return None

        # éªŒè¯åŸŸåæ ¼å¼
        if self.is_valid_domain(domain):
            return domain.lower()

        return None

    def load_auto_classify_rules(self) -> None:
        """
        åŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™é…ç½®
        """
        if not self.config.get("auto_classify", {}).get("enabled", False):
            print("ğŸ”„ è‡ªåŠ¨åˆ†ç±»åŠŸèƒ½å·²ç¦ç”¨")
            return

        print("ğŸ”„ æ­£åœ¨åŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™...")
        auto_classify_config = self.config["auto_classify"]

        # åŠ è½½ç›´æ¥é…ç½®çš„è§„åˆ™
        rules = auto_classify_config.get("rules", [])
        for rule in rules:
            parsed_rule = self._parse_auto_classify_rule(rule)
            if parsed_rule:
                self.auto_classify_rules.append(parsed_rule)

        if rules:
            print(f"  âœ… åŠ è½½äº† {len(rules)} ä¸ªå†…ç½®è‡ªåŠ¨åˆ†ç±»è§„åˆ™")

        # ä»å¤–éƒ¨æºåŠ è½½è§„åˆ™
        sources = auto_classify_config.get("sources", [])
        for source in sources:
            if not source.get("enabled", True):
                continue

            try:
                if "url" in source:
                    # ä»URLåŠ è½½
                    print(f"  ğŸŒ æ­£åœ¨ä»URLåŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {source['name']}")
                    rules_from_url = self._load_auto_classify_from_url(source)
                    self.auto_classify_rules.extend(rules_from_url)
                elif "file" in source:
                    # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
                    print(f"  ğŸ“ æ­£åœ¨ä»æ–‡ä»¶åŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {source['name']}")
                    rules_from_file = self._load_auto_classify_from_file(source)
                    self.auto_classify_rules.extend(rules_from_file)
            except Exception as e:
                print(f"  âŒ åŠ è½½è‡ªåŠ¨åˆ†ç±»æº '{source['name']}' å¤±è´¥: {e}")

        total_rules = len(self.auto_classify_rules)
        print(f"ğŸ”„ è‡ªåŠ¨åˆ†ç±»è§„åˆ™åŠ è½½å®Œæˆ: {total_rules} ä¸ªè§„åˆ™")

        # æ˜¾ç¤ºè§„åˆ™ç»Ÿè®¡
        if total_rules > 0:
            stats = defaultdict(int)
            for rule in self.auto_classify_rules:
                stats[rule['action']] += 1

            print(f"  ğŸ“Š è§„åˆ™åˆ†å¸ƒ:")
            for action, count in stats.items():
                print(f"    - {action}: {count} ä¸ª")

    def _parse_auto_classify_rule(self, rule_str: str) -> Dict:
        """
        è§£æè‡ªåŠ¨åˆ†ç±»è§„åˆ™

        Args:
            rule_str: è§„åˆ™å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "action:domain" æˆ– "replace:old=new"

        Returns:
            è§£æåçš„è§„åˆ™å­—å…¸
        """
        rule_str = rule_str.strip()
        if not rule_str or rule_str.startswith('#'):
            return None

        if ':' not in rule_str:
            return None

        action, content = rule_str.split(':', 1)
        action = action.strip().lower()
        content = content.strip()

        if not content:
            return None

        # å¤„ç†æ›¿æ¢è§„åˆ™
        if action == 'replace':
            if '=' in content:
                old_domain, new_domain = content.split('=', 1)
                return {
                    'action': 'replace',
                    'old_domain': old_domain.strip(),
                    'new_domain': new_domain.strip()
                }
            else:
                print(f"  âŒ æ— æ•ˆçš„æ›¿æ¢è§„åˆ™æ ¼å¼: {rule_str} (åº”ä¸º replace:old=new)")
                return None

        # å¤„ç†å…¶ä»–åŠ¨ä½œ
        elif action in ['remove', 'low_priority', 'high_priority', 'skip']:
            return {
                'action': action,
                'domain': content
            }
        else:
            print(f"  âŒ æœªçŸ¥çš„åŠ¨ä½œç±»å‹: {action}")
            return None

    def _load_auto_classify_from_url(self, source: dict) -> List[Dict]:
        """
        ä»URLåŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™

        Args:
            source: è§„åˆ™æºé…ç½®

        Returns:
            è§„åˆ™åˆ—è¡¨
        """
        url = source["url"]
        timeout = self.config["request_config"]["timeout"]
        retry_count = self.config["request_config"]["retry_count"]
        retry_delay = self.config["request_config"]["retry_delay"]

        for attempt in range(retry_count):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, timeout=timeout, headers=headers)
                response.raise_for_status()

                return self._parse_auto_classify_content(response.text)

            except requests.RequestException as e:
                print(f"    âŒ è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}")
                if attempt < retry_count - 1:
                    time.sleep(retry_delay)

        return []

    def _load_auto_classify_from_file(self, source: dict) -> List[Dict]:
        """
        ä»æœ¬åœ°æ–‡ä»¶åŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™

        Args:
            source: è§„åˆ™æºé…ç½®

        Returns:
            è§„åˆ™åˆ—è¡¨
        """
        file_path = source["file"]
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return self._parse_auto_classify_content(content)
        except FileNotFoundError:
            print(f"    âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        except Exception as e:
            print(f"    âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

        return []

    def _parse_auto_classify_content(self, content: str) -> List[Dict]:
        """
        è§£æè‡ªåŠ¨åˆ†ç±»è§„åˆ™å†…å®¹

        Args:
            content: æ–‡ä»¶å†…å®¹

        Returns:
            è§„åˆ™åˆ—è¡¨
        """
        rules = []
        for line in content.strip().split('\n'):
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            parsed_rule = self._parse_auto_classify_rule(line)
            if parsed_rule:
                rules.append(parsed_rule)

        return rules

    def load_custom_rules_from_file(self, file_path: str, format_type: str, action: str) -> Tuple[Set[str], Dict[str, str], Dict]:
        """
        ä»æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰è§„åˆ™

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            format_type: æ ¼å¼ç±»å‹ (domain, regex, ublock, v2ray, replace)
            action: åŠ¨ä½œç±»å‹ (remove, low_priority, high_priority, replace)

        Returns:
            (åŸŸåé›†åˆ, æ›¿æ¢è§„åˆ™å­—å…¸, ç»Ÿè®¡ä¿¡æ¯)
        """
        domains = set()
        replace_rules = {}
        stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'invalid_domains': 0,
            'ignored_comments': 0,
            'v2ray_with_tags': 0
        }

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            print(f"  ğŸ“ æ­£åœ¨è§£ææ–‡ä»¶: {file_path} (æ ¼å¼: {format_type})")

            for line_num, line in enumerate(content.strip().split('\n'), 1):
                line = line.strip()
                if not line:
                    continue

                stats['total_rules'] += 1

                # è·³è¿‡æ³¨é‡Š
                if line.startswith('#'):
                    stats['ignored_comments'] += 1
                    continue

                try:
                    if format_type == "replace":
                        # æ›¿æ¢è§„åˆ™æ ¼å¼ï¼šold_domain=new_domain æˆ– old_regex=new_domain
                        if '=' in line:
                            old_pattern, new_domain = line.split('=', 1)
                            old_pattern = old_pattern.strip()
                            new_domain = new_domain.strip()

                            if old_pattern and new_domain:
                                replace_rules[old_pattern] = new_domain
                                stats['parsed_domains'] += 1
                            else:
                                stats['invalid_domains'] += 1
                        else:
                            stats['invalid_domains'] += 1

                    elif format_type == "regex":
                        # æ­£åˆ™è¡¨è¾¾å¼æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                        if line:
                            domains.add(line)
                            stats['parsed_domains'] += 1

                    elif format_type == "ublock":
                        # uBlock æ ¼å¼
                        domain, ignore_reason = self.parse_ublock_rule(line)
                        if domain:
                            domains.add(domain)
                            stats['parsed_domains'] += 1
                        else:
                            stats['invalid_domains'] += 1

                    elif format_type == "v2ray":
                        # v2ray æ ¼å¼
                        domain, ignore_reason = self.parse_v2ray_rule(line)
                        if domain:
                            domains.add(domain)
                            stats['parsed_domains'] += 1
                            if stats['parsed_domains'] <= 3:  # æ˜¾ç¤ºå‰3ä¸ªè§£ææ ·æœ¬
                                print(f"    âœ… v2ray è§£æ: {line} -> {domain}")
                        else:
                            stats['invalid_domains'] += 1
                            if ignore_reason and stats['invalid_domains'] <= 3:
                                print(f"    âŒ v2ray å¿½ç•¥: {line} ({ignore_reason})")

                    else:  # domain æ ¼å¼
                        # å¤„ç†è¡Œæœ«æ³¨é‡Š
                        if '#' in line:
                            line = line[:line.find('#')].strip()
                            if not line:
                                stats['ignored_comments'] += 1
                                continue

                        domain = self.clean_domain(line)
                        if domain:
                            domains.add(domain)
                            stats['parsed_domains'] += 1
                        else:
                            stats['invalid_domains'] += 1

                except Exception as e:
                    print(f"    âŒ è§£æç¬¬ {line_num} è¡Œæ—¶å‡ºé”™: {line[:50]}... - {e}")
                    stats['invalid_domains'] += 1

            # ç´¯åŠ  v2ray æ ‡ç­¾ç»Ÿè®¡
            stats['v2ray_with_tags'] = self.stats.get('v2ray_with_tags', 0)

            print(f"    âœ… è§£æå®Œæˆ: {stats['parsed_domains']} ä¸ªæœ‰æ•ˆè§„åˆ™")
            if format_type == "v2ray" and stats['v2ray_with_tags'] > 0:
                print(f"    ğŸ“ å…¶ä¸­åŒ…å«æ ‡ç­¾çš„è§„åˆ™: {stats['v2ray_with_tags']} ä¸ª")
            if stats['invalid_domains'] > 0:
                print(f"    âš ï¸  å¿½ç•¥äº† {stats['invalid_domains']} ä¸ªæ— æ•ˆè§„åˆ™")

        except FileNotFoundError:
            print(f"    âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        except Exception as e:
            print(f"    âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

        return domains, replace_rules, stats

    def should_skip_domain_from_source(self, domain: str, source_name: str = None) -> Tuple[bool, str]:
        """
        æ£€æŸ¥åŸŸåæ˜¯å¦åº”è¯¥ä»æ•°æ®æºå¤„ç†ä¸­è·³è¿‡
        æ³¨æ„ï¼šè¿™åªå½±å“æ•°æ®æºçš„é»˜è®¤å¤„ç†ï¼Œä¸å½±å“æ˜ç¡®çš„è‡ªåŠ¨åˆ†ç±»è§„åˆ™

        Args:
            domain: è¦æ£€æŸ¥çš„åŸŸå
            source_name: æ•°æ®æºåç§°

        Returns:
            (æ˜¯å¦è·³è¿‡, è·³è¿‡åŸå› )
        """
        if not self.auto_classify_rules:
            return False, ""

        domain_lower = domain.lower()

        for rule in self.auto_classify_rules:
            if rule['action'] == 'skip':
                rule_domain = rule['domain'].lower()

                # æ”¯æŒé€šé…ç¬¦åŒ¹é…
                if rule_domain.startswith('*.'):
                    # é€šé…ç¬¦åŒ¹é…å­åŸŸå
                    pattern_domain = rule_domain[2:]  # ç§»é™¤ *.
                    if domain_lower == pattern_domain or domain_lower.endswith('.' + pattern_domain):
                        return True, f"è‡ªåŠ¨åˆ†ç±»è·³è¿‡è§„åˆ™: {rule['domain']} (ä»…å½±å“æ•°æ®æºå¤„ç†)"
                elif domain_lower == rule_domain:
                    # ç²¾ç¡®åŒ¹é…
                    return True, f"è‡ªåŠ¨åˆ†ç±»è·³è¿‡è§„åˆ™: {rule['domain']} (ä»…å½±å“æ•°æ®æºå¤„ç†)"

        return False, ""

    def get_auto_classify_action(self, domain: str) -> Tuple[str, str]:
        """
        æ ¹æ®è‡ªåŠ¨åˆ†ç±»è§„åˆ™è·å–åŸŸåçš„åŠ¨ä½œ

        Args:
            domain: åŸŸå

        Returns:
            (åŠ¨ä½œç±»å‹, åŒ¹é…åŸå› )
        """
        if not self.auto_classify_rules:
            return None, ""

        domain_lower = domain.lower()

        for rule in self.auto_classify_rules:
            if rule['action'] in ['remove', 'low_priority', 'high_priority']:
                rule_domain = rule['domain'].lower()

                # æ”¯æŒé€šé…ç¬¦åŒ¹é…
                if rule_domain.startswith('*.'):
                    # é€šé…ç¬¦åŒ¹é…å­åŸŸå
                    pattern_domain = rule_domain[2:]  # ç§»é™¤ *.
                    if domain_lower == pattern_domain or domain_lower.endswith('.' + pattern_domain):
                        return rule['action'], f"è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {rule['domain']}"
                elif domain_lower == rule_domain:
                    # ç²¾ç¡®åŒ¹é…
                    return rule['action'], f"è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {rule['domain']}"

        return None, ""

    def get_all_auto_classify_actions_for_domain(self, domain: str) -> List[Tuple[str, str]]:
        """
        è·å–åŸŸåçš„æ‰€æœ‰è‡ªåŠ¨åˆ†ç±»åŠ¨ä½œï¼ˆç”¨äºæ£€æµ‹å†²çªå’Œä¼˜å…ˆçº§å¤„ç†ï¼‰

        Args:
            domain: åŸŸå

        Returns:
            (åŠ¨ä½œç±»å‹, åŒ¹é…åŸå› ) çš„åˆ—è¡¨
        """
        if not self.auto_classify_rules:
            return []

        domain_lower = domain.lower()
        actions = []

        for rule in self.auto_classify_rules:
            if rule['action'] in ['remove', 'low_priority', 'high_priority', 'skip']:
                rule_domain = rule['domain'].lower()

                # æ”¯æŒé€šé…ç¬¦åŒ¹é…
                if rule_domain.startswith('*.'):
                    # é€šé…ç¬¦åŒ¹é…å­åŸŸå
                    pattern_domain = rule_domain[2:]  # ç§»é™¤ *.
                    if domain_lower == pattern_domain or domain_lower.endswith('.' + pattern_domain):
                        actions.append((rule['action'], f"è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {rule['domain']}"))
                elif domain_lower == rule_domain:
                    # ç²¾ç¡®åŒ¹é…
                    actions.append((rule['action'], f"è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {rule['domain']}"))

        return actions

    def apply_auto_classify_rules_directly(self, categorized_domains: Dict[str, Set[str]]) -> None:
        """
        ç›´æ¥åº”ç”¨è‡ªåŠ¨åˆ†ç±»è§„åˆ™ä¸­çš„åŸŸåï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
        ä¸ä»…é‡æ–°åˆ†ç±»ç°æœ‰åŸŸåï¼Œè¿˜ä¼šä¸»åŠ¨æ·»åŠ è§„åˆ™ä¸­å®šä¹‰çš„åŸŸå
        ä¿®å¤ç‰ˆæœ¬ï¼šskip è§„åˆ™ä¸ä¼šé˜»æ­¢å…¶ä»–æ˜ç¡®çš„è‡ªåŠ¨åˆ†ç±»è§„åˆ™

        Args:
            categorized_domains: åˆ†ç±»åçš„åŸŸåå­—å…¸
        """
        if not self.auto_classify_rules:
            return

        print(f"\nğŸ”„ æ­£åœ¨åº”ç”¨è‡ªåŠ¨åˆ†ç±»è§„åˆ™ä¸­çš„åŸŸå...")

        auto_added_count = 0
        auto_added_samples = []
        skip_overridden_count = 0
        skip_overridden_samples = []

        # æ”¶é›†æ‰€æœ‰å·²å­˜åœ¨çš„åŸŸåï¼ˆç”¨äºè·³è¿‡é‡å¤ï¼‰
        all_existing_domains = set()
        for domain_set in categorized_domains.values():
            all_existing_domains.update(d.lower() for d in domain_set)

        # æŒ‰åŸŸååˆ†ç»„å¤„ç†æ‰€æœ‰è§„åˆ™ï¼Œä»¥ä¾¿å¤„ç†å†²çª
        domain_rules_map = defaultdict(list)

        for rule in self.auto_classify_rules:
            if rule['action'] in ['remove', 'low_priority', 'high_priority', 'skip']:
                domain = rule['domain']

                # å¤„ç†é€šé…ç¬¦åŸŸå
                if domain.startswith('*.'):
                    # å¯¹äºé€šé…ç¬¦è§„åˆ™ï¼Œæˆ‘ä»¬ä¸ç›´æ¥æ·»åŠ ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŒ¹é…è§„åˆ™è€Œä¸æ˜¯å…·ä½“åŸŸå
                    continue

                # æ¸…ç†åŸŸåï¼ˆä½¿ç”¨ä¿æŒåŸå§‹ç»“æ„çš„æ¸…ç†æ–¹æ³•ï¼‰
                cleaned_domain = self.clean_domain_preserve_structure(domain)
                if not cleaned_domain:
                    continue

                domain_rules_map[cleaned_domain].append(rule)

            elif rule['action'] == 'replace':
                # å¤„ç†æ›¿æ¢è§„åˆ™
                old_domain = rule['old_domain']
                new_domain = rule['new_domain']

                cleaned_old_domain = self.clean_domain_preserve_structure(old_domain)
                if cleaned_old_domain and new_domain:
                    # ç”Ÿæˆæ­£åˆ™è¡¨è¾¾å¼æ ¼å¼çš„é”®
                    old_regex = f"(.*\\.)?{re.escape(cleaned_old_domain)}$"
                    self.config["replace_rules"][old_regex] = new_domain
                    auto_added_count += 1

                    if len(auto_added_samples) < 5:
                        auto_added_samples.append(f"{cleaned_old_domain} -> {new_domain} (æ›¿æ¢)")

        # å¤„ç†æ¯ä¸ªåŸŸåçš„è§„åˆ™
        for domain, rules in domain_rules_map.items():
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if domain.lower() in all_existing_domains:
                continue

            # åˆ†æè§„åˆ™ä¼˜å…ˆçº§
            has_skip = any(r['action'] == 'skip' for r in rules)
            non_skip_rules = [r for r in rules if r['action'] != 'skip']

            if non_skip_rules:
                # æœ‰é skip çš„è§„åˆ™ï¼Œä¼˜å…ˆå¤„ç†è¿™äº›è§„åˆ™
                # å¦‚æœæœ‰å¤šä¸ªé skip è§„åˆ™ï¼Œå–æœ€åä¸€ä¸ªï¼ˆæˆ–è€…å¯ä»¥æ ¹æ®ä¼˜å…ˆçº§æ’åºï¼‰
                effective_rule = non_skip_rules[-1]  # å–æœ€åä¸€ä¸ªè§„åˆ™

                action = effective_rule['action']
                if action in categorized_domains:
                    categorized_domains[action].add(domain)
                    all_existing_domains.add(domain.lower())
                    auto_added_count += 1

                    if len(auto_added_samples) < 5:
                        auto_added_samples.append(f"{domain} -> {action}")

                    # å¦‚æœåŒæ—¶æœ‰ skip è§„åˆ™ï¼Œè®°å½•è¦†ç›–æƒ…å†µ
                    if has_skip:
                        skip_overridden_count += 1
                        if len(skip_overridden_samples) < 3:
                            skip_overridden_samples.append(f"{domain} (skip è¢« {action} è¦†ç›–)")

            # å¦‚æœåªæœ‰ skip è§„åˆ™ï¼Œä¸åšä»»ä½•å¤„ç†ï¼ˆä½†ä¸æ˜¯é”™è¯¯ï¼‰

        if auto_added_count > 0:
            print(f"  âœ… ä¸»åŠ¨æ·»åŠ äº† {auto_added_count} ä¸ªåŸŸååˆ°ç›¸åº”ç±»åˆ«")
            self.stats['auto_added'] = auto_added_count

            print(f"  ğŸ“ æ·»åŠ çš„åŸŸåæ ·æœ¬:")
            for sample in auto_added_samples:
                print(f"    + {sample}")

        if skip_overridden_count > 0:
            print(f"  ğŸ”„ skip è§„åˆ™è¢«è¦†ç›–: {skip_overridden_count} ä¸ªåŸŸå")
            self.stats['skip_overridden'] = skip_overridden_count

            print(f"  ğŸ“ è¦†ç›–æƒ…å†µæ ·æœ¬:")
            for sample in skip_overridden_samples:
                print(f"    âš¡ {sample}")

        if auto_added_count == 0 and skip_overridden_count == 0:
            print(f"  â„¹ï¸  æ²¡æœ‰éœ€è¦ä¸»åŠ¨æ·»åŠ çš„åŸŸå")

    def clean_domain_preserve_structure(self, domain: str) -> str:
        """
        æ¸…ç†åŸŸåä½†ä¿æŒåŸå§‹ç»“æ„ï¼ˆç”¨äºè‡ªåŠ¨åˆ†ç±»è§„åˆ™ï¼‰

        Args:
            domain: åŸå§‹åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ¸…ç†åçš„åŸŸåï¼ˆä¿æŒç»“æ„ï¼‰
        """
        if not domain:
            return None

        # ç§»é™¤åè®®
        if domain.startswith(('http://', 'https://')):
            parsed = urlparse(domain)
            domain = parsed.netloc

        # ç§»é™¤æ˜æ˜¾çš„ç«¯å£å·
        if ':' in domain:
            parts = domain.split(':')
            if len(parts) == 2 and parts[1].isdigit():
                domain = parts[0]

        # ç§»é™¤è·¯å¾„
        if '/' in domain:
            domain = domain.split('/')[0]

        # **ä¿æŒ www. å‰ç¼€**
        # ä¸åšä»»ä½•å‰ç¼€ç§»é™¤

        # åªç§»é™¤ç©ºç™½å­—ç¬¦
        domain = domain.strip()

        # æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€
        if self.config["parsing"]["ignore_ip"] and self.is_ip_address(domain):
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯localhost
        if self.config["parsing"]["ignore_localhost"] and domain in ['localhost', '127.0.0.1', '0.0.0.0']:
            return None

        # éªŒè¯åŸŸåæ ¼å¼
        if self.is_valid_domain(domain):
            return domain.lower()

        return None

    def is_domain_level_rule(self, url_string: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™ï¼ˆè€Œéç‰¹å®šè·¯å¾„è§„åˆ™ï¼‰

        Args:
            url_string: URLå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦æ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™
        """
        url_string = url_string.strip()

        # ä¸¥æ ¼æ£€æŸ¥æ¨¡å¼
        if self.config["parsing"].get("strict_domain_level_check", True):
            # è¿™äº›æ¨¡å¼è¢«è®¤ä¸ºæ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™ï¼š
            domain_level_patterns = [
                # uBlock åŸŸåçº§åˆ«æ¨¡å¼ - ç²¾ç¡®æ¨¡å¼
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/?$',                    # *://*.example.com æˆ– *://*.example.com/
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/\*?$',                 # *://*.example.com/*
                r'^\*://[a-zA-Z0-9.-]+/?$',                         # *://example.com æˆ– *://example.com/
                r'^\*://[a-zA-Z0-9.-]+/\*?$',                       # *://example.com/*
                r'^\|\|[a-zA-Z0-9.-]+\^?$',                         # ||example.com^
                r'^[a-zA-Z0-9.-]+/?$',                              # example.com æˆ– example.com/
                r'^[a-zA-Z0-9.-]+/\*?$',                            # example.com/* æˆ– example.com/
            ]

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…åŸŸåçº§åˆ«æ¨¡å¼
            for pattern in domain_level_patterns:
                if re.match(pattern, url_string):
                    # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœåŒ…å«å…·ä½“è·¯å¾„ï¼ˆé™¤äº†/å’Œ/*ï¼‰ï¼Œåˆ™ä¸æ˜¯åŸŸåçº§åˆ«
                    if self._has_specific_path(url_string):
                        return False
                    return True

            return False
        else:
            # å…¼å®¹æ¨¡å¼ï¼ˆåŸæ¥çš„é€»è¾‘ï¼‰
            domain_level_patterns = [
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/?$',
                r'^\*://\*?\.?[a-zA-Z0-9.-]+/\*$',
                r'^\|\|[a-zA-Z0-9.-]+\^?$',
                r'^[a-zA-Z0-9.-]+/?$',
                r'^[a-zA-Z0-9.-]+/\*$',
            ]

            for pattern in domain_level_patterns:
                if re.match(pattern, url_string):
                    return True

            return False

    def _has_specific_path(self, url_string: str) -> bool:
        """
        æ£€æŸ¥URLæ˜¯å¦åŒ…å«å…·ä½“çš„è·¯å¾„ï¼ˆéåŸŸåçº§åˆ«ï¼‰

        Args:
            url_string: URLå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦åŒ…å«å…·ä½“è·¯å¾„
        """
        # ç§»é™¤åè®®éƒ¨åˆ†
        if url_string.startswith('*://'):
            url_part = url_string[4:]
        elif url_string.startswith('||'):
            url_part = url_string[2:].rstrip('^')
        else:
            url_part = url_string

        # æ£€æŸ¥æ˜¯å¦æœ‰è·¯å¾„éƒ¨åˆ†
        if '/' in url_part:
            domain_and_path = url_part.split('/', 1)
            if len(domain_and_path) > 1:
                path_part = domain_and_path[1]
                # å¦‚æœè·¯å¾„ä¸æ˜¯ç©ºã€å•ä¸ª*æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™è®¤ä¸ºæ˜¯å…·ä½“è·¯å¾„
                if path_part and path_part not in ['', '*']:
                    return True

        return False

    def extract_domain_from_rule(self, rule: str) -> str:
        """
        ä»è§„åˆ™ä¸­æå–åŸŸå

        Args:
            rule: è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            åŸŸåæˆ– None
        """
        rule = rule.strip()

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“è·¯å¾„
        if self._has_specific_path(rule):
            # å¯¹äºåŒ…å«å…·ä½“è·¯å¾„çš„è§„åˆ™ï¼Œéœ€è¦æ›´è°¨æ…åœ°æå–åŸŸå
            return self._extract_domain_from_path_rule(rule)

        # uBlock è¯­æ³•æ¨¡å¼ - ä»…ç”¨äºåŸŸåçº§åˆ«è§„åˆ™
        patterns = [
            # *://*.domain.com/* æˆ– *://*.domain.com (é€šé…ç¬¦å­åŸŸå)
            r'^\*://\*\.([a-zA-Z0-9.-]+)(?:/\*?)?$',
            # *://domain.com/* æˆ– *://domain.com (æ— é€šé…ç¬¦)
            r'^\*://([a-zA-Z0-9.-]+)(?:/\*?)?$',
            # ||domain.com^
            r'^\|\|([a-zA-Z0-9.-]+)\^?$',
            # æ™®é€šåŸŸåæ ¼å¼
            r'^([a-zA-Z0-9.-]+)(?:/\*?)?$',
        ]

        for pattern in patterns:
            match = re.match(pattern, rule)
            if match:
                return match.group(1)

        # é€šç”¨åŸŸåæå–ï¼ˆæœ€åçš„åå¤‡æ–¹æ¡ˆï¼‰
        domain_match = re.search(r'([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', rule)
        if domain_match:
            candidate = domain_match.group(1)
            # éªŒè¯è¿™ä¸ªåŸŸåæ˜¯å¦åˆç†
            if self.is_valid_domain(candidate):
                return candidate

        return None

    def _extract_domain_from_path_rule(self, rule: str) -> str:
        """
        ä»åŒ…å«è·¯å¾„çš„è§„åˆ™ä¸­æå–åŸŸå

        Args:
            rule: åŒ…å«è·¯å¾„çš„è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            åŸŸåæˆ– None
        """
        rule = rule.strip()

        # å¯¹äºåŒ…å«å…·ä½“è·¯å¾„çš„è§„åˆ™ï¼Œæˆ‘ä»¬é€šå¸¸ä¸æå–åŸŸå
        # é™¤éç”¨æˆ·æ˜ç¡®é…ç½®å…è®¸
        if self.config["parsing"]["ignore_specific_paths"]:
            return None

        # å¦‚æœç”¨æˆ·å…è®¸å¤„ç†è·¯å¾„è§„åˆ™ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼
        path_patterns = [
            # *://*.subdomain.domain.com/path/* -> æå– subdomain.domain.com
            r'^\*://\*\.([a-zA-Z0-9.-]+)/[^/]+',
            # *://subdomain.domain.com/path/* -> æå– subdomain.domain.com
            r'^\*://([a-zA-Z0-9.-]+)/[^/]+',
        ]

        for pattern in path_patterns:
            match = re.match(pattern, rule)
            if match:
                domain = match.group(1)
                # åªæœ‰å½“è¿™æ˜¯ä¸€ä¸ªå­åŸŸåæ—¶æ‰è¿”å›ï¼Œé¿å…æå–ä¸»åŸŸå
                if '.' in domain and len(domain.split('.')) >= 2:
                    return domain

        return None

    def parse_ublock_rule(self, rule: str) -> Tuple[str, str]:
        """
        è§£æ uBlock Origin è¯­æ³•è§„åˆ™ï¼Œæå–åŸŸå

        Args:
            rule: uBlock è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            (åŸŸåæˆ– None, å¿½ç•¥åŸå› )
        """
        rule = rule.strip()
        if not rule or rule.startswith('!') or rule.startswith('#'):
            return None, "æ³¨é‡Šæˆ–ç©ºè¡Œ"

        # å¤„ç†è¡Œæœ«æ³¨é‡Š - ç§»é™¤ # åé¢çš„æ‰€æœ‰å†…å®¹
        if '#' in rule:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª # çš„ä½ç½®ï¼Œç§»é™¤å®ƒåŠåé¢çš„å†…å®¹
            comment_pos = rule.find('#')
            rule = rule[:comment_pos].strip()

            # å¦‚æœç§»é™¤æ³¨é‡Šåè§„åˆ™ä¸ºç©ºï¼Œåˆ™å¿½ç•¥
            if not rule:
                return None, "ä»…åŒ…å«æ³¨é‡Š"

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯åŸŸåçº§åˆ«çš„è§„åˆ™
        if not self.is_domain_level_rule(rule):
            if self.config["parsing"]["ignore_specific_paths"]:
                return None, "æŒ‡å‘ç‰¹å®šè·¯å¾„"

        # æå–åŸŸå
        domain = self.extract_domain_from_rule(rule)

        if domain:
            cleaned_domain = self.clean_domain(domain)
            return cleaned_domain, None if cleaned_domain else "æ— æ•ˆåŸŸå"

        return None, "æ— æ³•è§£æè§„åˆ™æ ¼å¼"

    def fetch_domain_list(self, url: str, format_type: str = "domain", source_name: str = None) -> Tuple[Set[str], Dict]:
        """
        ä»URLè·å–åŸŸååˆ—è¡¨

        Args:
            url: åŸŸååˆ—è¡¨URL
            format_type: æ ¼å¼ç±»å‹ï¼Œ"domain", "ublock", æˆ– "v2ray"
            source_name: æ•°æ®æºåç§°ï¼ˆç”¨äºè‡ªåŠ¨åˆ†ç±»ï¼‰

        Returns:
            (åŸŸåé›†åˆ, ç»Ÿè®¡ä¿¡æ¯)
        """
        domains = set()
        stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0,
            'auto_classified': 0,  # è‡ªåŠ¨åˆ†ç±»å¤„ç†çš„æ•°é‡
            'skipped_domains': 0,   # è·³è¿‡çš„åŸŸåæ•°é‡
            'v2ray_with_tags': 0   # v2ray å¸¦æ ‡ç­¾çš„è§„åˆ™æ•°é‡
        }

        retry_count = self.config["request_config"]["retry_count"]
        timeout = self.config["request_config"]["timeout"]
        retry_delay = self.config["request_config"]["retry_delay"]

        for attempt in range(retry_count):
            try:
                print(f"æ­£åœ¨è·å– {url} (å°è¯• {attempt + 1}/{retry_count}) - æ ¼å¼: {format_type}")

                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }

                response = requests.get(url, timeout=timeout, headers=headers)
                response.raise_for_status()

                # è®°å½•ä¸€äº›è¢«å¿½ç•¥çš„è§„åˆ™ç”¨äºè°ƒè¯•
                ignored_samples = []
                accepted_samples = []
                comment_samples = []
                path_samples = []  # è·¯å¾„è§„åˆ™æ ·æœ¬
                skip_samples = []  # è·³è¿‡çš„åŸŸåæ ·æœ¬

                # é‡ç½® v2ray æ ‡ç­¾è®¡æ•°å™¨
                initial_v2ray_tags = self.stats.get('v2ray_with_tags', 0)

                # è§£æåŸŸå
                for line_num, line in enumerate(response.text.strip().split('\n'), 1):
                    line = line.strip()
                    if not line:
                        continue

                    stats['total_rules'] += 1

                    try:
                        if format_type == "ublock":
                            # ä½¿ç”¨ uBlock è¯­æ³•è§£æ
                            domain, ignore_reason = self.parse_ublock_rule(line)
                        elif format_type == "v2ray":
                            # ä½¿ç”¨ v2ray è¯­æ³•è§£æ
                            domain, ignore_reason = self.parse_v2ray_rule(line)
                            # æ˜¾ç¤º v2ray è§£ææ ·æœ¬
                            if domain and len(accepted_samples) < 3:
                                accepted_samples.append(f"v2ray: {line} -> {domain}")
                            elif ignore_reason and len(ignored_samples) < 3:
                                ignored_samples.append(f"v2ray: {line} ({ignore_reason})")
                        else:
                            # æ™®é€šåŸŸåæ ¼å¼ - ä¹Ÿéœ€è¦å¤„ç†è¡Œæœ«æ³¨é‡Š
                            cleaned_line = line
                            if '#' in line:
                                cleaned_line = line[:line.find('#')].strip()
                                if not cleaned_line:
                                    domain, ignore_reason = None, "ä»…åŒ…å«æ³¨é‡Š"
                                else:
                                    if self.config["parsing"]["ignore_specific_paths"] and not self.is_domain_level_rule(cleaned_line):
                                        domain, ignore_reason = None, "æŒ‡å‘ç‰¹å®šè·¯å¾„"
                                    else:
                                        domain = self.clean_domain(self.extract_domain_from_rule(cleaned_line))
                                        ignore_reason = None if domain else "æ— æ•ˆåŸŸå"
                            else:
                                if self.config["parsing"]["ignore_specific_paths"] and not self.is_domain_level_rule(cleaned_line):
                                    domain, ignore_reason = None, "æŒ‡å‘ç‰¹å®šè·¯å¾„"
                                else:
                                    domain = self.clean_domain(self.extract_domain_from_rule(cleaned_line))
                                    ignore_reason = None if domain else "æ— æ•ˆåŸŸå"

                        if domain:
                            # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä»æ•°æ®æºè·³è¿‡æ­¤åŸŸå
                            should_skip, skip_reason = self.should_skip_domain_from_source(domain, source_name)
                            if should_skip:
                                stats['skipped_domains'] += 1
                                if len(skip_samples) < 3:
                                    skip_samples.append(f"{line} -> {domain} ({skip_reason})")
                            else:
                                if domain in domains:
                                    stats['duplicate_domains'] += 1
                                else:
                                    domains.add(domain)
                                    stats['parsed_domains'] += 1
                                    # è®°å½•ä¸€äº›è¢«æ¥å—çš„è§„åˆ™æ ·æœ¬ï¼ˆé v2ray æ ¼å¼çš„ï¼‰
                                    if format_type != "v2ray" and len(accepted_samples) < 3:
                                        accepted_samples.append(f"{line} -> {domain}")
                        else:
                            # ç»Ÿè®¡å¿½ç•¥åŸå› 
                            if ignore_reason == "æŒ‡å‘ç‰¹å®šè·¯å¾„":
                                stats['ignored_with_path'] += 1
                                # è®°å½•ä¸€äº›è¢«å¿½ç•¥çš„è·¯å¾„è§„åˆ™æ ·æœ¬
                                if len(path_samples) < 3:
                                    path_samples.append(line)
                            elif ignore_reason in ["æ³¨é‡Šæˆ–ç©ºè¡Œ", "ä»…åŒ…å«æ³¨é‡Š"]:
                                stats['ignored_comments'] += 1
                                if len(comment_samples) < 3:
                                    comment_samples.append(line)
                            elif ignore_reason == "æ— æ•ˆåŸŸå":
                                stats['invalid_domains'] += 1
                                if format_type != "v2ray" and len(ignored_samples) < 3:
                                    ignored_samples.append(line)

                    except Exception as e:
                        print(f"è§£æç¬¬ {line_num} è¡Œæ—¶å‡ºé”™: {line[:50]}... - {e}")
                        stats['invalid_domains'] += 1
                        continue

                # è®¡ç®—æœ¬æ¬¡è¯·æ±‚ä¸­çš„ v2ray æ ‡ç­¾æ•°é‡
                current_v2ray_tags = self.stats.get('v2ray_with_tags', 0) - initial_v2ray_tags
                stats['v2ray_with_tags'] = current_v2ray_tags

                print(f"æˆåŠŸè·å– {len(domains)} ä¸ªåŸŸå")
                print(f"  - æ€»è§„åˆ™: {stats['total_rules']}")
                print(f"  - æˆåŠŸè§£æ: {stats['parsed_domains']}")
                print(f"  - å¿½ç•¥(ç‰¹å®šè·¯å¾„): {stats['ignored_with_path']}")
                print(f"  - å¿½ç•¥(æ³¨é‡Š): {stats['ignored_comments']}")
                print(f"  - å¿½ç•¥(æ— æ•ˆåŸŸå): {stats['invalid_domains']}")
                print(f"  - é‡å¤åŸŸå: {stats['duplicate_domains']}")
                print(f"  - è·³è¿‡åŸŸå: {stats['skipped_domains']}")
                if format_type == "v2ray" and stats['v2ray_with_tags'] > 0:
                    print(f"  - v2ray å¸¦æ ‡ç­¾è§„åˆ™: {stats['v2ray_with_tags']}")

                # æ˜¾ç¤ºæ ·æœ¬
                if accepted_samples:
                    print(f"  - æ¥å—çš„è§„åˆ™æ ·æœ¬:")
                    for sample in accepted_samples:
                        print(f"    âœ“ {sample}")

                if skip_samples:
                    print(f"  - è·³è¿‡çš„åŸŸåæ ·æœ¬:")
                    for sample in skip_samples:
                        print(f"    â­ï¸ {sample}")

                if path_samples:
                    print(f"  - å¿½ç•¥çš„è·¯å¾„è§„åˆ™æ ·æœ¬:")
                    for sample in path_samples:
                        print(f"    ğŸ›¤ï¸  {sample}")

                if comment_samples:
                    print(f"  - å¿½ç•¥çš„æ³¨é‡Šè§„åˆ™æ ·æœ¬:")
                    for sample in comment_samples:
                        print(f"    # {sample}")

                if ignored_samples:
                    print(f"  - å…¶ä»–å¿½ç•¥çš„è§„åˆ™æ ·æœ¬:")
                    for sample in ignored_samples:
                        print(f"    âœ— {sample}")

                return domains, stats

            except requests.RequestException as e:
                print(f"è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}")
                if attempt < retry_count - 1:
                    time.sleep(retry_delay)
                else:
                    print(f"æ”¾å¼ƒè·å– {url}")

        return domains, stats

    def clean_domain(self, domain: str) -> str:
        """
        æ¸…ç†åŸŸåå­—ç¬¦ä¸²
        æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿æŒåŸå§‹ç»“æ„

        Args:
            domain: åŸå§‹åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ¸…ç†åçš„åŸŸå
        """
        if not domain:
            return None

        # å¦‚æœé…ç½®ä¸ºä¿æŒåŸå§‹ç»“æ„ï¼Œä½¿ç”¨ä¸“é—¨çš„æ–¹æ³•
        if self.config["parsing"].get("preserve_original_structure", True):
            return self.clean_domain_preserve_structure(domain)

        # åŸæ¥çš„æ¸…ç†é€»è¾‘ï¼ˆå¯èƒ½ç§»é™¤ www. å‰ç¼€ï¼‰
        return self._clean_domain_legacy(domain)

    def _clean_domain_legacy(self, domain: str) -> str:
        """
        ä¼ ç»Ÿçš„åŸŸåæ¸…ç†æ–¹æ³•ï¼ˆå¯èƒ½ç§»é™¤ www. å‰ç¼€ï¼‰

        Args:
            domain: åŸå§‹åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ¸…ç†åçš„åŸŸå
        """
        if not domain:
            return None

        # ç§»é™¤åè®®
        if domain.startswith(('http://', 'https://')):
            domain = urlparse(domain).netloc

        # ç§»é™¤ç«¯å£
        if ':' in domain:
            domain = domain.split(':')[0]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è·¯å¾„ï¼ˆè¿™é‡Œä¸åº”è¯¥æœ‰ï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
        if '/' in domain:
            domain = domain.split('/')[0]

        # ç§»é™¤ www. å‰ç¼€ï¼ˆä¼ ç»Ÿè¡Œä¸ºï¼‰
        if not self.config["parsing"].get("preserve_www_prefix", True):
            if domain.startswith('www.'):
                domain = domain[4:]

        # ç§»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        domain = re.sub(r'[^\w.-]', '', domain)

        # æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€
        if self.config["parsing"]["ignore_ip"] and self.is_ip_address(domain):
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯localhost
        if self.config["parsing"]["ignore_localhost"] and domain in ['localhost', '127.0.0.1', '0.0.0.0']:
            return None

        # éªŒè¯åŸŸåæ ¼å¼
        if self.is_valid_domain(domain):
            return domain.lower()

        return None

    def is_ip_address(self, domain: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€

        Args:
            domain: åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦æ˜¯IPåœ°å€
        """
        ip_pattern = re.compile(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')
        return bool(ip_pattern.match(domain))

    def is_valid_domain(self, domain: str) -> bool:
        """
        éªŒè¯åŸŸåæ ¼å¼

        Args:
            domain: åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦ä¸ºæœ‰æ•ˆåŸŸå
        """
        if not domain or len(domain) > 255:
            return False

        # åŸºæœ¬çš„åŸŸåæ ¼å¼éªŒè¯
        domain_pattern = re.compile(
            r'^(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)*[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$'
        )

        return bool(domain_pattern.match(domain))

    def domain_to_regex(self, domain: str) -> str:
        """
        å°†åŸŸåè½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼

        Args:
            domain: åŸŸå

        Returns:
            æ­£åˆ™è¡¨è¾¾å¼å­—ç¬¦ä¸²
        """
        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
        escaped_domain = re.escape(domain)
        # æ·»åŠ å­åŸŸååŒ¹é…
        return f'(.*\.)?{escaped_domain}$'

    def smart_sort_domains(self, domains: Set[str]) -> List[str]:
        """
        æ™ºèƒ½æ’åºåŸŸåï¼Œä¾¿äºåç»­åˆå¹¶
        å…ˆæŒ‰TLDæ’åºï¼Œå†æŒ‰åŸŸåä¸»ä½“æ’åº

        Args:
            domains: åŸŸåé›†åˆ

        Returns:
            æ’åºåçš„åŸŸååˆ—è¡¨
        """
        def domain_sort_key(domain: str) -> Tuple[str, str]:
            """
            ç”ŸæˆåŸŸåæ’åºé”®ï¼š(TLD, åå‘åŸŸåä¸»ä½“)
            è¿™æ ·å¯ä»¥å°†åŒTLDçš„åŸŸåèšé›†åœ¨ä¸€èµ·ï¼Œä¾¿äºåˆå¹¶
            """
            parts = domain.split('.')
            if len(parts) >= 2:
                # TLD ä½œä¸ºä¸»è¦æ’åºé”®
                tld = parts[-1]
                # åŸŸåä¸»ä½“ä½œä¸ºæ¬¡è¦æ’åºé”®ï¼Œåå‘æ’åºä¾¿äºæ‰¾åˆ°å…¬å…±åç¼€
                base = '.'.join(parts[:-1])
                return (tld, base)
            else:
                return (domain, '')

        if self.config["optimization"].get("sort_before_merge", True):
            sorted_domains = sorted(list(domains), key=domain_sort_key)
            print(f"  ğŸ”„ åŸŸåå·²æŒ‰TLDæ™ºèƒ½æ’åºï¼Œä¾¿äºåˆå¹¶ä¼˜åŒ–")
            return sorted_domains
        else:
            return list(domains)

    def group_domains_by_tld(self, domains: Union[Set[str], List[str]]) -> Dict[str, List[str]]:
        """
        æŒ‰é¡¶çº§åŸŸååˆ†ç»„åŸŸåï¼Œå¹¶ä¿æŒæ’åº

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            æŒ‰TLDåˆ†ç»„çš„åŸŸåå­—å…¸ï¼Œå€¼ä¸ºæ’åºåçš„åˆ—è¡¨
        """
        tld_groups = defaultdict(list)

        # å¦‚æœè¾“å…¥æ˜¯é›†åˆï¼Œå…ˆè½¬æ¢ä¸ºæ™ºèƒ½æ’åºçš„åˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        for domain in domains:
            parts = domain.split('.')
            if len(parts) >= 2:
                # è·å–é¡¶çº§åŸŸåï¼ˆå¦‚ .com, .orgï¼‰
                tld = parts[-1]
                tld_groups[tld].append(domain)
            else:
                # å¤„ç†æ— æ•ˆåŸŸå
                tld_groups['other'].append(domain)

        return dict(tld_groups)

    def get_domain_base_and_tld(self, domain: str) -> Tuple[str, str]:
        """
        æå–åŸŸåçš„ä¸»ä½“éƒ¨åˆ†å’ŒTLD

        Args:
            domain: å®Œæ•´åŸŸå

        Returns:
            (åŸŸåä¸»ä½“, TLD)
        """
        parts = domain.split('.')
        if len(parts) >= 2:
            tld = parts[-1]
            base = '.'.join(parts[:-1])
            return base, tld
        return domain, ''

    def find_common_prefix(self, strings: List[str]) -> str:
        """
        æ‰¾åˆ°å­—ç¬¦ä¸²åˆ—è¡¨çš„å…¬å…±å‰ç¼€

        Args:
            strings: å­—ç¬¦ä¸²åˆ—è¡¨

        Returns:
            å…¬å…±å‰ç¼€
        """
        if not strings:
            return ""

        strings = [s for s in strings if s]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not strings:
            return ""

        min_len = min(len(s) for s in strings)
        prefix = ""

        for i in range(min_len):
            char = strings[0][i]
            if all(s[i] == char for s in strings):
                prefix += char
            else:
                break

        return prefix

    def find_common_suffix(self, strings: List[str]) -> str:
        """
        æ‰¾åˆ°å­—ç¬¦ä¸²åˆ—è¡¨çš„å…¬å…±åç¼€

        Args:
            strings: å­—ç¬¦ä¸²åˆ—è¡¨

        Returns:
            å…¬å…±åç¼€
        """
        if not strings:
            return ""

        strings = [s for s in strings if s]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not strings:
            return ""

        # åè½¬å­—ç¬¦ä¸²ï¼Œæ‰¾å‰ç¼€ï¼Œå†åè½¬å›æ¥
        reversed_strings = [s[::-1] for s in strings]
        reversed_suffix = self.find_common_prefix(reversed_strings)
        return reversed_suffix[::-1]

    def create_advanced_tld_regex(self, tld_domains: List[str], tld: str) -> str:
        """
        ä¸ºåŒä¸€TLDçš„åŸŸååˆ›å»ºé«˜çº§ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼
        ä¿®å¤ç‰ˆæœ¬ï¼šç¡®ä¿TLDä¸ä¼šä¸¢å¤±

        Args:
            tld_domains: åŒä¸€TLDçš„åŸŸååˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
            tld: é¡¶çº§åŸŸå

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        if len(tld_domains) == 1:
            return re.escape(tld_domains[0])

        # æå–åŸŸåä¸»ä½“éƒ¨åˆ†
        domain_bases = []
        for domain in tld_domains:
            base, domain_tld = self.get_domain_base_and_tld(domain)
            if domain_tld == tld:
                domain_bases.append(base)
            else:
                # TLDä¸åŒ¹é…çš„æƒ…å†µï¼Œä½¿ç”¨å®Œæ•´åŸŸå
                domain_bases.append(domain)

        if not domain_bases:
            return '|'.join(re.escape(d) for d in tld_domains)

        # å°è¯•æ‰¾åˆ°å…¬å…±æ¨¡å¼
        optimized_pattern = self.optimize_domain_bases(domain_bases)

        # æ£€æŸ¥åŸŸååŸºç¡€éƒ¨åˆ†çš„ç»“æ„
        simple_domains = [base for base in domain_bases if '.' not in base]  # äºŒçº§åŸŸå
        complex_domains = [base for base in domain_bases if '.' in base]     # å¤šçº§åŸŸå

        if len(simple_domains) == len(domain_bases):
            # æ‰€æœ‰éƒ½æ˜¯äºŒçº§åŸŸåï¼Œå¯ä»¥è¿›è¡ŒTLDä¼˜åŒ–
            return f"({optimized_pattern})\\.{re.escape(tld)}"
        elif len(complex_domains) == len(domain_bases):
            # æ‰€æœ‰éƒ½æ˜¯å¤šçº§åŸŸåï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰å…¬å…±çš„äºŒçº§+TLDåç¼€
            return self._optimize_complex_domains_with_tld(domain_bases, tld)
        else:
            # æ··åˆæƒ…å†µï¼šäºŒçº§åŸŸå+å¤šçº§åŸŸå
            return self._optimize_mixed_domains_with_tld(simple_domains, complex_domains, tld)

    def _optimize_complex_domains_with_tld(self, domain_bases: List[str], tld: str) -> str:
        """
        ä¼˜åŒ–å¤šçº§åŸŸåï¼Œç¡®ä¿ä¿ç•™TLD

        Args:
            domain_bases: åŸŸååŸºç¡€éƒ¨åˆ†åˆ—è¡¨ï¼ˆéƒ½æ˜¯å¤šçº§åŸŸåï¼‰
            tld: é¡¶çº§åŸŸå

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¬å…±çš„äºŒçº§åŸŸå+TLDæ¨¡å¼
        # ä¾‹å¦‚ï¼ša.pixnet.net, b.pixnet.net -> (a|b).pixnet.net

        # æ‰¾åˆ°æ‰€æœ‰åŸŸåçš„å…¬å…±åç¼€ï¼ˆä¸åŒ…æ‹¬ç¬¬ä¸€éƒ¨åˆ†ï¼‰
        if len(domain_bases) <= 1:
            if domain_bases:
                return f"{re.escape(domain_bases[0])}\\.{re.escape(tld)}"
            return f".*\\.{re.escape(tld)}"

        # åˆ†æç»“æ„ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰åŸŸåéƒ½æœ‰ç›¸åŒçš„åç¼€ç»“æ„
        common_suffix_parts = None
        prefixes = []

        for base in domain_bases:
            parts = base.split('.')
            if common_suffix_parts is None:
                # ç¬¬ä¸€ä¸ªåŸŸåï¼Œè®¾ç½®å…¬å…±åç¼€å€™é€‰
                if len(parts) >= 2:
                    common_suffix_parts = parts[1:]  # é™¤äº†ç¬¬ä¸€éƒ¨åˆ†çš„å…¶ä½™éƒ¨åˆ†
                    prefixes.append(parts[0])
                else:
                    # å¤„ç†å¼‚å¸¸æƒ…å†µ
                    common_suffix_parts = []
                    prefixes.append(base)
            else:
                # æ£€æŸ¥æ˜¯å¦ä¸å…¬å…±åç¼€åŒ¹é…
                if len(parts) >= len(common_suffix_parts) + 1:
                    current_suffix = parts[-(len(common_suffix_parts)):]
                    if current_suffix == common_suffix_parts:
                        prefixes.append(parts[0])
                    else:
                        # åç¼€ä¸åŒ¹é…ï¼Œæ— æ³•ä¼˜åŒ–ï¼Œç›´æ¥è¿”å›å®Œæ•´åŸŸååˆ—è¡¨
                        escaped_bases = [re.escape(base) for base in domain_bases]
                        return f"({'|'.join(escaped_bases)})\\.{re.escape(tld)}"
                else:
                    # é•¿åº¦ä¸å¤Ÿï¼Œæ— æ³•ä¼˜åŒ–
                    escaped_bases = [re.escape(base) for base in domain_bases]
                    return f"({'|'.join(escaped_bases)})\\.{re.escape(tld)}"

        # å¦‚æœæ‰¾åˆ°äº†å…¬å…±åç¼€ï¼Œè¿›è¡Œä¼˜åŒ–
        if common_suffix_parts and len(set(prefixes)) > 1:
            # ä¼˜åŒ–å‰ç¼€éƒ¨åˆ†
            optimized_prefixes = self.optimize_domain_bases(prefixes)
            escaped_suffix = '\\.'.join(re.escape(part) for part in common_suffix_parts)
            return f"({optimized_prefixes})\\.{escaped_suffix}\\.{re.escape(tld)}"
        else:
            # æ— æ³•æ‰¾åˆ°å…¬å…±æ¨¡å¼ï¼Œä½¿ç”¨åŸºç¡€ä¼˜åŒ–
            optimized_pattern = self.optimize_domain_bases(domain_bases)
            return f"({optimized_pattern})\\.{re.escape(tld)}"

    def _optimize_mixed_domains_with_tld(self, simple_domains: List[str], complex_domains: List[str], tld: str) -> str:
        """
        ä¼˜åŒ–æ··åˆåŸŸåï¼ˆäºŒçº§+å¤šçº§ï¼‰ï¼Œç¡®ä¿ä¿ç•™TLD

        Args:
            simple_domains: äºŒçº§åŸŸååˆ—è¡¨
            complex_domains: å¤šçº§åŸŸååˆ—è¡¨
            tld: é¡¶çº§åŸŸå

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        patterns = []

        # å¤„ç†äºŒçº§åŸŸå
        if simple_domains:
            if len(simple_domains) == 1:
                patterns.append(f"{re.escape(simple_domains[0])}\\.{re.escape(tld)}")
            else:
                optimized_simple = self.optimize_domain_bases(simple_domains)
                patterns.append(f"({optimized_simple})\\.{re.escape(tld)}")

        # å¤„ç†å¤šçº§åŸŸå
        if complex_domains:
            complex_pattern = self._optimize_complex_domains_with_tld(complex_domains, tld)
            patterns.append(complex_pattern)

        # åˆå¹¶æ‰€æœ‰æ¨¡å¼
        if len(patterns) == 1:
            return patterns[0]
        else:
            return f"({'|'.join(patterns)})"

    def optimize_domain_bases(self, domain_bases: List[str]) -> str:
        """
        ä¼˜åŒ–åŸŸååŸºç¡€éƒ¨åˆ†åˆ—è¡¨

        Args:
            domain_bases: åŸŸååŸºç¡€éƒ¨åˆ†åˆ—è¡¨

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        """
        if len(domain_bases) <= 1:
            return '|'.join(re.escape(base) for base in domain_bases)

        optimization_config = self.config["optimization"]

        # å°è¯•å‰ç¼€ä¼˜åŒ–
        if optimization_config.get("enable_prefix_optimization", True):
            common_prefix = self.find_common_prefix(domain_bases)
            min_prefix_len = optimization_config.get("min_common_prefix_length", 3)

            if len(common_prefix) >= min_prefix_len:
                # ç§»é™¤å…¬å…±å‰ç¼€
                suffixes = [base[len(common_prefix):] for base in domain_bases]
                suffixes = [s for s in suffixes if s]  # è¿‡æ»¤ç©ºåç¼€
                if suffixes and len(set(suffixes)) > 1:  # ç¡®ä¿æœ‰ä¸åŒçš„åç¼€
                    suffix_pattern = self.optimize_domain_bases(suffixes)
                    return f"{re.escape(common_prefix)}({suffix_pattern})"

        # å°è¯•åç¼€ä¼˜åŒ–
        if optimization_config.get("enable_suffix_optimization", True):
            common_suffix = self.find_common_suffix(domain_bases)
            min_suffix_len = optimization_config.get("min_common_suffix_length", 3)

            if len(common_suffix) >= min_suffix_len:
                # ç§»é™¤å…¬å…±åç¼€
                prefixes = [base[:-len(common_suffix)] for base in domain_bases]
                prefixes = [p for p in prefixes if p]  # è¿‡æ»¤ç©ºå‰ç¼€
                if prefixes and len(set(prefixes)) > 1:  # ç¡®ä¿æœ‰ä¸åŒçš„å‰ç¼€
                    prefix_pattern = self.optimize_domain_bases(prefixes)
                    return f"({prefix_pattern}){re.escape(common_suffix)}"

        # æ²¡æœ‰æ‰¾åˆ°ä¼˜åŒ–æ¨¡å¼ï¼Œç›´æ¥è¿æ¥
        return '|'.join(re.escape(base) for base in domain_bases)

    def create_single_regex_rule(self, domains: Union[Set[str], List[str]]) -> str:
        """
        åˆ›å»ºåŒ…å«æ‰€æœ‰åŸŸåçš„å•è¡Œæ­£åˆ™è¡¨è¾¾å¼ï¼ˆé«˜çº§TLDä¼˜åŒ–ï¼‰

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            å•è¡Œæ­£åˆ™è¡¨è¾¾å¼
        """
        if not domains:
            return ""

        # è½¬æ¢ä¸ºæ’åºåˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"

        print(f"ğŸš€ æ­£åœ¨ç”Ÿæˆé«˜çº§TLDä¼˜åŒ–å•è¡Œæ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ…å« {len(domains)} ä¸ªåŸŸå")

        # å¯ç”¨é«˜çº§TLDåˆå¹¶
        if self.config["optimization"].get("enable_advanced_tld_merge", True):
            # æŒ‰TLDåˆ†ç»„
            tld_groups = self.group_domains_by_tld(domains)
            tld_patterns = []

            print(f"  ğŸ“Š TLDåˆ†å¸ƒæƒ…å†µ:")
            for tld, tld_domains in sorted(tld_groups.items(), key=lambda x: len(x[1]), reverse=True):
                print(f"    .{tld}: {len(tld_domains)} ä¸ªåŸŸå")
                # æ˜¾ç¤ºä¸€äº›åŸŸåæ ·æœ¬
                if len(tld_domains) <= 3:
                    for domain in tld_domains:
                        print(f"      - {domain}")
                else:
                    for domain in tld_domains[:3]:
                        print(f"      - {domain}")
                    print(f"      - ... è¿˜æœ‰ {len(tld_domains)-3} ä¸ªåŸŸå")

            for tld, tld_domains in tld_groups.items():
                if len(tld_domains) == 1:
                    # å•ä¸ªåŸŸåç›´æ¥å¤„ç†
                    domain = tld_domains[0]
                    tld_patterns.append(re.escape(domain))
                else:
                    # å¤šä¸ªåŸŸåè¿›è¡Œé«˜çº§ä¼˜åŒ–
                    optimized_pattern = self.create_advanced_tld_regex(tld_domains, tld)
                    tld_patterns.append(optimized_pattern)
                    print(f"  âœ… TLD .{tld}: {len(tld_domains)} ä¸ªåŸŸåå·²ä¼˜åŒ–åˆå¹¶")

            # åˆå¹¶æ‰€æœ‰TLDç»„çš„æ¨¡å¼
            if len(tld_patterns) == 1:
                combined_pattern = tld_patterns[0]
            else:
                combined_pattern = f"({'|'.join(tld_patterns)})"

            single_regex = f"(.*\\.)?{combined_pattern}$"
        else:
            # ç®€å•åˆå¹¶æ¨¡å¼
            escaped_domains = [re.escape(d) for d in domains]
            combined_pattern = '|'.join(escaped_domains)
            single_regex = f"(.*\\.)?({combined_pattern})$"

        # æ˜¾ç¤ºè§„åˆ™é•¿åº¦ä¿¡æ¯
        rule_length = len(single_regex)
        print(f"  ğŸ“ ç”Ÿæˆçš„å•è¡Œè§„åˆ™é•¿åº¦: {rule_length:,} å­—ç¬¦")

        if rule_length > 100000:
            print("  âš ï¸  è§„åˆ™æé•¿ï¼Œå¯èƒ½ä¸¥é‡å½±å“æ€§èƒ½ï¼Œå»ºè®®åˆ†å‰²")
        elif rule_length > 50000:
            print("  âš ï¸  è§„åˆ™å¾ˆé•¿ï¼Œå¯èƒ½å½±å“åŒ¹é…æ€§èƒ½")
        elif rule_length > 10000:
            print("  âš ï¸  è§„åˆ™è¾ƒé•¿ï¼Œè¯·æ³¨æ„æ€§èƒ½")
        else:
            print("  âœ… è§„åˆ™é•¿åº¦é€‚ä¸­")

        return single_regex

    def create_multiple_optimized_rules(self, domains: Union[Set[str], List[str]]) -> List[str]:
        """
        åˆ›å»ºå¤šä¸ªä¼˜åŒ–çš„è§„åˆ™ï¼ˆéå•è¡Œæ¨¡å¼ï¼‰

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            ä¼˜åŒ–åçš„è§„åˆ™åˆ—è¡¨
        """
        if not domains:
            return []

        # è½¬æ¢ä¸ºæ’åºåˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        optimization_config = self.config["optimization"]
        max_domains_per_rule = optimization_config.get("max_domains_per_rule", 30)
        max_rule_length = optimization_config.get("max_rule_length", 4000)

        # æŒ‰TLDåˆ†ç»„å¤„ç†
        if optimization_config.get("group_by_tld", True):
            tld_groups = self.group_domains_by_tld(domains)
            rules = []

            for tld, tld_domains in tld_groups.items():
                if len(tld_domains) <= 1:
                    # å•ä¸ªåŸŸåç›´æ¥è½¬æ¢
                    rules.extend([self.domain_to_regex(domain) for domain in tld_domains])
                else:
                    # å¤šä¸ªåŸŸååˆ†æ‰¹å¤„ç†
                    tld_rules = self._create_batched_rules(tld_domains, tld, max_domains_per_rule, max_rule_length)
                    rules.extend(tld_rules)
                    print(f"  ğŸ“¦ TLD .{tld}: {len(tld_domains)} ä¸ªåŸŸå -> {len(tld_rules)} ä¸ªè§„åˆ™")

            return rules
        else:
            # ä¸åˆ†ç»„ï¼Œç›´æ¥åˆ†æ‰¹å¤„ç†
            return self._create_batched_rules(domains, None, max_domains_per_rule, max_rule_length)

    def _create_batched_rules(self, domains: List[str], tld: str = None, max_domains_per_rule: int = 30, max_rule_length: int = 4000) -> List[str]:
        """
        ä¸ºåŸŸååˆ—è¡¨åˆ›å»ºåˆ†æ‰¹çš„è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨
            tld: é¡¶çº§åŸŸåï¼ˆå¯é€‰ï¼Œç”¨äºä¼˜åŒ–ï¼‰
            max_domains_per_rule: æ¯ä¸ªè§„åˆ™çš„æœ€å¤§åŸŸåæ•°
            max_rule_length: æœ€å¤§è§„åˆ™é•¿åº¦

        Returns:
            åˆ†æ‰¹åçš„è§„åˆ™åˆ—è¡¨
        """
        rules = []
        current_batch = []

        for domain in domains:
            test_batch = current_batch + [domain]

            # åˆ›å»ºæµ‹è¯•è§„åˆ™
            if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                test_rule = self._create_tld_optimized_rule(test_batch, tld)
            else:
                test_rule = self._create_simple_rule(test_batch)

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if (len(test_batch) > max_domains_per_rule or
                len(test_rule) > max_rule_length):

                # ä¿å­˜å½“å‰æ‰¹æ¬¡
                if current_batch:
                    if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                        rules.append(self._create_tld_optimized_rule(current_batch, tld))
                    else:
                        rules.append(self._create_simple_rule(current_batch))

                # å¼€å§‹æ–°æ‰¹æ¬¡
                current_batch = [domain]
            else:
                current_batch.append(domain)

        # å¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                rules.append(self._create_tld_optimized_rule(current_batch, tld))
            else:
                rules.append(self._create_simple_rule(current_batch))

        return rules

    def _create_tld_optimized_rule(self, domains: List[str], tld: str) -> str:
        """
        ä¸ºåŒTLDåŸŸååˆ›å»ºä¼˜åŒ–è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨
            tld: é¡¶çº§åŸŸå

        Returns:
            TLDä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™
        """
        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"

        optimized_pattern = self.create_advanced_tld_regex(domains, tld)
        return f"(.*\\.)?{optimized_pattern}$"

    def _create_simple_rule(self, domains: List[str]) -> str:
        """
        ä¸ºåŸŸååˆ—è¡¨åˆ›å»ºç®€å•è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨

        Returns:
            ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™
        """
        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"
        else:
            pattern = self.optimize_domain_bases(domains)
            return f"(.*\\.)?({pattern})$"

    def merge_domains_to_regex(self, domains: Set[str]) -> List[str]:
        """
        å°†å¤šä¸ªåŸŸååˆå¹¶ä¸ºä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨

        Args:
            domains: åŸŸåé›†åˆ

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨
        """
        if not domains:
            return []

        # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™
        if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
            print(f"ğŸš€ å¯ç”¨å¼ºåˆ¶å•è¡Œæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼")
            single_rule = self.create_single_regex_rule(domains)
            return [single_rule] if single_rule else []

        optimization_config = self.config["optimization"]

        # å¦‚æœæœªå¯ç”¨åˆå¹¶ä¼˜åŒ–ï¼Œè¿”å›å•ç‹¬çš„è§„åˆ™
        if not optimization_config.get("merge_domains", True):
            return [self.domain_to_regex(domain) for domain in domains]

        # ä½¿ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼
        print(f"ğŸ”§ å¯ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼ï¼Œå¤„ç† {len(domains)} ä¸ªåŸŸå")
        rules = self.create_multiple_optimized_rules(domains)

        print(f"  âœ… ä¼˜åŒ–å®Œæˆ: {len(domains)} ä¸ªåŸŸå -> {len(rules)} ä¸ªè§„åˆ™")
        return rules

    def collect_domains(self) -> Dict[str, Set[str]]:
        """
        ä»æ‰€æœ‰é…ç½®çš„æºæ”¶é›†åŸŸå

        Returns:
            æŒ‰åŠ¨ä½œåˆ†ç±»çš„åŸŸåé›†åˆ
        """
        categorized_domains = {
            'remove': set(),
            'low_priority': set(),
            'high_priority': set()
        }

        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0,
            'auto_classified': 0,
            'skipped_domains': 0,
            'auto_added': 0,
            'skipped_from_sources': 0,
            'skip_overridden': 0,
            'v2ray_with_tags': 0
        }

        # ä»åœ¨çº¿æºæ”¶é›†åŸŸå
        for source in self.config["sources"]:
            if not source.get("enabled", True):
                continue

            print(f"\nå¤„ç†æ•°æ®æº: {source['name']}")
            format_type = source.get("format", "domain")
            print(f"æ ¼å¼ç±»å‹: {format_type}")

            domains, source_stats = self.fetch_domain_list(source["url"], format_type, source["name"])

            # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
            for key in self.stats:
                if key in source_stats:
                    self.stats[key] += source_stats[key]

            # å¤„ç†åŸŸååˆ†ç±»
            source_action = source.get("action", "remove")
            auto_classified_count = 0

            for domain in list(domains):
                # æ£€æŸ¥è‡ªåŠ¨åˆ†ç±»è§„åˆ™
                auto_action, reason = self.get_auto_classify_action(domain)
                if auto_action:
                    # ä»åŸå§‹é›†åˆä¸­ç§»é™¤ï¼Œæ·»åŠ åˆ°ç›¸åº”ç±»åˆ«
                    domains.remove(domain)
                    categorized_domains[auto_action].add(domain)
                    auto_classified_count += 1
                    if auto_classified_count <= 5:  # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬
                        print(f"  ğŸ”„ è‡ªåŠ¨åˆ†ç±»: {domain} -> {auto_action} ({reason})")
                else:
                    # ä½¿ç”¨æºçš„é»˜è®¤åŠ¨ä½œ
                    if source_action in categorized_domains:
                        categorized_domains[source_action].add(domain)

            if auto_classified_count > 0:
                print(f"  âœ… è‡ªåŠ¨åˆ†ç±»å¤„ç†: {auto_classified_count} ä¸ªåŸŸå")
                self.stats['auto_classified'] += auto_classified_count

            # è®°å½•ä»æ•°æ®æºè·³è¿‡çš„åŸŸåæ•°é‡
            self.stats['skipped_from_sources'] += source_stats.get('skipped_domains', 0)

            print(f"å·²æ·»åŠ  {len(domains)} ä¸ªåŸŸååˆ° {source_action} ç±»åˆ«")

        # ä»è‡ªå®šä¹‰è§„åˆ™æ–‡ä»¶åŠ è½½
        print(f"\nå¤„ç†è‡ªå®šä¹‰è§„åˆ™æ–‡ä»¶...")
        if self.config.get("custom_rules", {}).get("enabled", False):
            custom_sources = self.config["custom_rules"]["sources"]

            for source in custom_sources:
                if not source.get("enabled", True):
                    continue

                print(f"\nå¤„ç†è‡ªå®šä¹‰è§„åˆ™: {source['name']}")
                file_path = source["file"]
                format_type = source.get("format", "domain")
                action = source.get("action", "remove")

                if not os.path.exists(file_path):
                    print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    continue

                domains, replace_rules, source_stats = self.load_custom_rules_from_file(
                    file_path, format_type, action
                )

                # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
                self.stats['total_rules'] += source_stats['total_rules']
                self.stats['parsed_domains'] += source_stats['parsed_domains']
                self.stats['invalid_domains'] += source_stats['invalid_domains']
                self.stats['ignored_comments'] += source_stats['ignored_comments']

                # å°†åŸŸåæ·»åŠ åˆ°ç›¸åº”ç±»åˆ«
                if action in categorized_domains:
                    categorized_domains[action].update(domains)

                # å¤„ç†æ›¿æ¢è§„åˆ™
                if action == "replace" and replace_rules:
                    # æ›´æ–°é…ç½®ä¸­çš„æ›¿æ¢è§„åˆ™
                    self.config["replace_rules"].update(replace_rules)

                print(f"  âœ… ä»æ–‡ä»¶åŠ è½½äº† {len(domains) + len(replace_rules)} ä¸ªè§„åˆ™åˆ° {action} ç±»åˆ«")

        # ğŸ”¥ æ–°å¢åŠŸèƒ½ï¼šä¸»åŠ¨åº”ç”¨è‡ªåŠ¨åˆ†ç±»è§„åˆ™ä¸­çš„åŸŸå
        self.apply_auto_classify_rules_directly(categorized_domains)

        return categorized_domains

    def sort_rules(self, rules: Union[Dict, List]) -> Union[Dict, List]:
        """
        å¯¹è§„åˆ™è¿›è¡Œæ’åº

        Args:
            rules: è§„åˆ™æ•°æ®

        Returns:
            æ’åºåçš„è§„åˆ™
        """
        if isinstance(rules, dict):
            # å¯¹å­—å…¸æŒ‰é”®æ’åº
            return OrderedDict(sorted(rules.items()))
        elif isinstance(rules, list):
            # å¯¹åˆ—è¡¨æŒ‰å€¼æ’åº
            return sorted(rules)
        else:
            return rules

    def generate_rules(self) -> Dict[str, any]:
        """
        ç”Ÿæˆå„ç±»è§„åˆ™

        Returns:
            è§„åˆ™å­—å…¸
        """
        print("\nå¼€å§‹ç”Ÿæˆ SearXNG hostnames è§„åˆ™...")

        # æ˜¾ç¤ºä¼˜åŒ–æ¨¡å¼ä¿¡æ¯
        if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
            print("ğŸš€ å·²å¯ç”¨é«˜çº§TLDä¼˜åŒ–å•è¡Œæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼")
            print("   æ¯ä¸ªç±»åˆ«å°†ç”Ÿæˆå•ä¸ªåŒ…å«æ‰€æœ‰åŸŸåçš„é«˜çº§ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼")
        else:
            print("ğŸ”§ å·²å¯ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼")
            print("   å°†ç”Ÿæˆå¤šä¸ªæ€§èƒ½ä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™")

        # æ˜¾ç¤ºè§£æé…ç½®
        parsing_config = self.config["parsing"]
        print(f"ğŸ“ è§£æé…ç½®:")
        print(f"   - å¿½ç•¥ç‰¹å®šè·¯å¾„è§„åˆ™: {parsing_config.get('ignore_specific_paths', True)}")
        print(f"   - ä¸¥æ ¼åŸŸåçº§åˆ«æ£€æŸ¥: {parsing_config.get('strict_domain_level_check', True)}")
        print(f"   - ä¿æŒåŸå§‹ç»“æ„: {parsing_config.get('preserve_original_structure', True)}")
        print(f"   - ä¿æŒ www. å‰ç¼€: {parsing_config.get('preserve_www_prefix', True)}")

        # æ˜¾ç¤ºè‡ªåŠ¨åˆ†ç±»é…ç½®
        auto_classify_config = self.config.get("auto_classify", {})
        if auto_classify_config.get("enabled", False):
            print(f"ğŸ”„ è‡ªåŠ¨åˆ†ç±»é…ç½®:")
            print(f"   - å†…ç½®è§„åˆ™: {len(auto_classify_config.get('rules', []))} ä¸ª")
            print(f"   - å¤–éƒ¨æº: {len([s for s in auto_classify_config.get('sources', []) if s.get('enabled', True)])} ä¸ª")
            print(f"   - æ€»è®¡è§„åˆ™: {len(self.auto_classify_rules)} ä¸ª")
        else:
            print(f"ğŸ”„ è‡ªåŠ¨åˆ†ç±»åŠŸèƒ½å·²ç¦ç”¨")

        # æ˜¾ç¤ºè‡ªå®šä¹‰è§„åˆ™é…ç½®
        custom_rules_config = self.config.get("custom_rules", {})
        if custom_rules_config.get("enabled", False):
            enabled_sources = [s for s in custom_rules_config.get("sources", []) if s.get("enabled", True)]
            print(f"ğŸ“ è‡ªå®šä¹‰è§„åˆ™æ–‡ä»¶:")
            print(f"   - å¯ç”¨çš„æ–‡ä»¶æº: {len(enabled_sources)} ä¸ª")
            for source in enabled_sources:
                print(f"     â€¢ {source['name']}: {source['file']} ({source.get('action', 'remove')}) - æ ¼å¼: {source.get('format', 'domain')}")
        else:
            print(f"ğŸ“ è‡ªå®šä¹‰è§„åˆ™æ–‡ä»¶åŠŸèƒ½å·²ç¦ç”¨")

        # æ”¶é›†åŸŸå
        categorized_domains = self.collect_domains()

        # å¤„ç†è‡ªåŠ¨åˆ†ç±»æ›¿æ¢è§„åˆ™
        auto_replace_rules = {}
        for rule in self.auto_classify_rules:
            if rule['action'] == 'replace':
                old_regex = f"(.*\\.)?{re.escape(rule['old_domain'])}$"
                auto_replace_rules[old_regex] = rule['new_domain']

        rules = {}

        # æ›¿æ¢è§„åˆ™ (å­—å…¸æ ¼å¼)
        all_replace_rules = {}
        all_replace_rules.update(self.config["replace_rules"])
        all_replace_rules.update(auto_replace_rules)

        if all_replace_rules:
            rules["replace"] = all_replace_rules
            # è®°å½•æ›¿æ¢è§„åˆ™çš„åŸŸåæ•°é‡
            self.category_domain_counts["replace"] = len(all_replace_rules)
        else:
            # å³ä½¿æ²¡æœ‰æ›¿æ¢è§„åˆ™ï¼Œä¹Ÿåˆ›å»ºç©ºè§„åˆ™ä»¥ç¡®ä¿æ–‡ä»¶è¢«åˆ›å»º
            self.category_domain_counts["replace"] = 0

        # ç§»é™¤è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆç§»é™¤è§„åˆ™...")
        remove_rules = []
        remove_rules.extend(self.config["fixed_remove"])

        # è®°å½•å›ºå®šç§»é™¤è§„åˆ™æ•°é‡
        fixed_remove_count = len(self.config["fixed_remove"])

        if categorized_domains["remove"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['remove'])} ä¸ªç§»é™¤åŸŸå...")
            self.category_domain_counts["remove"] = len(categorized_domains["remove"]) + fixed_remove_count
            merged_remove_rules = self.merge_domains_to_regex(categorized_domains["remove"])
            remove_rules.extend(merged_remove_rules)
        else:
            self.category_domain_counts["remove"] = fixed_remove_count

        if remove_rules:
            rules["remove"] = remove_rules
        else:
            # åˆ›å»ºç©ºç§»é™¤è§„åˆ™åˆ—è¡¨ï¼Œç¡®ä¿æ–‡ä»¶è¢«åˆ›å»º
            rules["remove"] = []

        # ä½ä¼˜å…ˆçº§è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆä½ä¼˜å…ˆçº§è§„åˆ™...")
        low_priority_rules = []
        low_priority_rules.extend(self.config["fixed_low_priority"])

        # è®°å½•å›ºå®šä½ä¼˜å…ˆçº§è§„åˆ™æ•°é‡
        fixed_low_priority_count = len(self.config["fixed_low_priority"])

        if categorized_domains["low_priority"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['low_priority'])} ä¸ªä½ä¼˜å…ˆçº§åŸŸå...")
            self.category_domain_counts["low_priority"] = len(categorized_domains["low_priority"]) + fixed_low_priority_count
            merged_low_priority_rules = self.merge_domains_to_regex(categorized_domains["low_priority"])
            low_priority_rules.extend(merged_low_priority_rules)
        else:
            self.category_domain_counts["low_priority"] = fixed_low_priority_count

        if low_priority_rules:
            rules["low_priority"] = low_priority_rules
        else:
            # åˆ›å»ºç©ºä½ä¼˜å…ˆçº§è§„åˆ™åˆ—è¡¨ï¼Œç¡®ä¿æ–‡ä»¶è¢«åˆ›å»º
            rules["low_priority"] = []

        # é«˜ä¼˜å…ˆçº§è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆé«˜ä¼˜å…ˆçº§è§„åˆ™...")
        high_priority_rules = []
        high_priority_rules.extend(self.config["fixed_high_priority"])

        # è®°å½•å›ºå®šé«˜ä¼˜å…ˆçº§è§„åˆ™æ•°é‡
        fixed_high_priority_count = len(self.config["fixed_high_priority"])

        if categorized_domains["high_priority"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['high_priority'])} ä¸ªé«˜ä¼˜å…ˆçº§åŸŸå...")
            self.category_domain_counts["high_priority"] = len(categorized_domains["high_priority"]) + fixed_high_priority_count
            merged_high_priority_rules = self.merge_domains_to_regex(categorized_domains["high_priority"])
            high_priority_rules.extend(merged_high_priority_rules)
        else:
            self.category_domain_counts["high_priority"] = fixed_high_priority_count

        if high_priority_rules:
            rules["high_priority"] = high_priority_rules
        else:
            # åˆ›å»ºç©ºé«˜ä¼˜å…ˆçº§è§„åˆ™åˆ—è¡¨ï¼Œç¡®ä¿æ–‡ä»¶è¢«åˆ›å»º
            rules["high_priority"] = []

        # å¯¹æ‰€æœ‰è§„åˆ™è¿›è¡Œæ’åºå’Œå»é‡
        for rule_type in rules:
            if rule_type == "replace":
                # æ›¿æ¢è§„åˆ™æŒ‰é”®æ’åº
                rules[rule_type] = self.sort_rules(rules[rule_type])
            else:
                # åˆ—è¡¨è§„åˆ™å»é‡å¹¶æ’åº
                rules[rule_type] = self.sort_rules(list(set(rules[rule_type])))

        return rules

    def save_separate_files(self, rules: Dict[str, any]) -> None:
        """
        ä¿å­˜ä¸ºåˆ†ç¦»çš„æ–‡ä»¶ - ç®€åŒ–ç‰ˆæ–‡ä»¶å¤´

        Args:
            rules: è§„åˆ™å­—å…¸
        """
        output_dir = self.config["output"]["directory"]
        files_config = self.config["output"]["files"]

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # ç”Ÿæˆä¸»é…ç½®æ–‡ä»¶ (ç”¨äºå¼•ç”¨å¤–éƒ¨æ–‡ä»¶)
        main_config = {"hostnames": {}}

        # ä¿å­˜å„ç±»è§„åˆ™åˆ°å•ç‹¬æ–‡ä»¶ - ç¡®ä¿æ‰€æœ‰ç±»åˆ«çš„æ–‡ä»¶éƒ½è¢«åˆ›å»º
        expected_rule_types = ["replace", "remove", "low_priority", "high_priority"]

        for rule_type in expected_rule_types:
            if rule_type in files_config:
                filename = files_config[rule_type]
                filepath = os.path.join(output_dir, filename)

                # è·å–è§„åˆ™æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç©ºæ•°æ®
                rule_data = rules.get(rule_type, [] if rule_type != "replace" else {})

                try:
                    with open(filepath, 'w', encoding='utf-8') as f:
                        # ç®€åŒ–çš„æ–‡ä»¶å¤´æ³¨é‡Š
                        rule_count = len(rule_data) if isinstance(rule_data, (list, dict)) else 0
                        domain_count = self.category_domain_counts.get(rule_type, 0)

                        f.write(f"# SearXNG {rule_type} rules\n")
                        f.write(f"# Total rules: {rule_count}, Total domains: {domain_count}\n")
                        f.write("\n")

                        # ç›´æ¥å†™å…¥è§„åˆ™å†…å®¹ï¼Œä¸åŒ…å«é¡¶çº§é”®
                        if rule_data or rule_type in rules:  # åªæœ‰å½“æœ‰æ•°æ®æˆ–åŸæœ¬å°±åœ¨rulesä¸­æ‰å†™å…¥å†…å®¹
                            yaml.dump(rule_data, f, default_flow_style=False, allow_unicode=True, indent=2)
                        else:
                            # å†™å…¥ç©ºå†…å®¹æ ‡è®°
                            if rule_type == "replace":
                                f.write("{}\n")  # ç©ºå­—å…¸
                            else:
                                f.write("[]\n")  # ç©ºåˆ—è¡¨

                    print(f"å·²ä¿å­˜ {rule_type} è§„åˆ™åˆ°: {filepath} ({rule_count} æ¡è§„åˆ™)")

                    # åœ¨ä¸»é…ç½®ä¸­å¼•ç”¨å¤–éƒ¨æ–‡ä»¶
                    main_config["hostnames"][rule_type] = filename

                except Exception as e:
                    print(f"ä¿å­˜ {rule_type} è§„åˆ™å¤±è´¥: {e}")

        # ä¿å­˜ä¸»é…ç½®æ–‡ä»¶
        if main_config["hostnames"]:
            main_config_path = os.path.join(output_dir, files_config["main_config"])
            try:
                with open(main_config_path, 'w', encoding='utf-8') as f:
                    # ç®€åŒ–çš„ä¸»é…ç½®æ–‡ä»¶å¤´
                    f.write("# SearXNG hostnames configuration\n")
                    f.write("# This file references external rule files\n")
                    f.write("\n")
                    yaml.dump(main_config, f, default_flow_style=False, allow_unicode=True, indent=2)
                print(f"å·²ä¿å­˜ä¸»é…ç½®åˆ°: {main_config_path}")
            except Exception as e:
                print(f"ä¿å­˜ä¸»é…ç½®å¤±è´¥: {e}")

    def save_single_file(self, rules: Dict[str, any]) -> None:
        """
        ä¿å­˜ä¸ºå•ä¸ªæ–‡ä»¶ - ç®€åŒ–ç‰ˆæ–‡ä»¶å¤´

        Args:
            rules: è§„åˆ™å­—å…¸
        """
        output_dir = self.config["output"]["directory"]
        os.makedirs(output_dir, exist_ok=True)

        # ç¡®ä¿æ‰€æœ‰ç±»åˆ«éƒ½åœ¨è§„åˆ™ä¸­
        expected_rule_types = ["replace", "remove", "low_priority", "high_priority"]
        for rule_type in expected_rule_types:
            if rule_type not in rules:
                if rule_type == "replace":
                    rules[rule_type] = {}
                else:
                    rules[rule_type] = []

        # æ„å»ºå®Œæ•´çš„ hostnames é…ç½®
        hostnames_config = {"hostnames": rules}

        filepath = os.path.join(output_dir, "hostnames.yml")

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                # ç®€åŒ–çš„æ–‡ä»¶å¤´æ³¨é‡Š
                total_rules = sum(len(rule_data) if isinstance(rule_data, (list, dict)) else 0 for rule_data in rules.values())
                total_domains = sum(self.category_domain_counts.values())

                f.write("# SearXNG hostnames configuration\n")
                f.write(f"# Total rules: {total_rules}, Total domains: {total_domains}\n")
                f.write("\n")

                yaml.dump(hostnames_config, f, default_flow_style=False, allow_unicode=True, indent=2)

            print(f"å·²ä¿å­˜å®Œæ•´é…ç½®åˆ°: {filepath}")

        except Exception as e:
            print(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")

    def run(self) -> None:
        """
        è¿è¡Œç”Ÿæˆå™¨
        """
        print("SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨å¯åŠ¨ (å®Œæ•´ç‰ˆ - è‡ªåŠ¨åˆ†ç±» + è‡ªå®šä¹‰æ–‡ä»¶ + TLDä¼˜åŒ– + v2ray æ ¼å¼ - ä¿æŒåŸå§‹ç»“æ„)")
        print("ğŸ”§ ä¿®å¤ç‰ˆæœ¬ï¼šskip è§„åˆ™åªå½±å“æ•°æ®æºå¤„ç†ï¼Œä¸é˜»æ­¢æ˜ç¡®çš„è‡ªåŠ¨åˆ†ç±»è§„åˆ™")
        print("ğŸ†• æ–°å¢åŠŸèƒ½ï¼šæ”¯æŒ v2ray æ ¼å¼ (domain:example.com, full:example.com, domain:example.com:@tag)")
        print("ğŸ”§ ä¿®æ­£åŠŸèƒ½ï¼šä¿æŒåŸå§‹åŸŸåç»“æ„ï¼Œä¸ç§»é™¤ www. ç­‰å‰ç¼€")
        print("=" * 90)

        try:
            # ç”Ÿæˆè§„åˆ™
            rules = self.generate_rules()

            # æ ¹æ®é…ç½®ä¿å­˜æ–‡ä»¶
            if self.config["output"]["mode"] == "separate_files":
                self.save_separate_files(rules)
            else:
                self.save_single_file(rules)

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.print_statistics(rules)

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
        except Exception as e:
            print(f"\nç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

    def print_statistics(self, rules: Dict[str, any]) -> None:
        """
        è¾“å‡ºç»Ÿè®¡ä¿¡æ¯

        Args:
            rules: ç”Ÿæˆçš„è§„åˆ™
        """
        print("\n" + "=" * 70)
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")

        total_rules = 0
        total_domains = 0

        for rule_type, rule_data in rules.items():
            if isinstance(rule_data, dict):
                rule_count = len(rule_data)
                domain_count = self.category_domain_counts.get(rule_type, 0)
                print(f"  {rule_type} è§„åˆ™: {rule_count} æ¡ (åŒ…å« {domain_count} ä¸ªåŸŸå)")
                total_rules += rule_count
                total_domains += domain_count
            elif isinstance(rule_data, list):
                rule_count = len(rule_data)
                domain_count = self.category_domain_counts.get(rule_type, 0)
                print(f"  {rule_type} è§„åˆ™: {rule_count} æ¡ (åŒ…å« {domain_count} ä¸ªåŸŸå)")
                total_rules += rule_count
                total_domains += domain_count

                # å¦‚æœæ˜¯å•è¡Œæ­£åˆ™æ¨¡å¼ï¼Œæ˜¾ç¤ºè§„åˆ™é•¿åº¦ä¿¡æ¯
                if (self.force_single_regex or self.config["optimization"].get("force_single_regex", False)) and rule_data:
                    for i, rule in enumerate(rule_data, 1):
                        rule_length = len(rule)
                        if rule_length > 1000:
                            print(f"    è§„åˆ™ {i} é•¿åº¦: {rule_length:,} å­—ç¬¦")

        print(f"\nğŸ“ˆ æ€»è®¡: {total_rules} æ¡è§„åˆ™ (åŒ…å« {total_domains} ä¸ªåŸŸå)")

        print(f"\nğŸ” è§£æç»Ÿè®¡:")
        print(f"  - æ€»è¾“å…¥è§„åˆ™: {self.stats['total_rules']:,}")
        print(f"  - æˆåŠŸè§£æåŸŸå: {self.stats['parsed_domains']:,}")
        print(f"  - å¿½ç•¥(ç‰¹å®šè·¯å¾„): {self.stats['ignored_with_path']:,}")
        print(f"  - å¿½ç•¥(æ³¨é‡Š): {self.stats['ignored_comments']:,}")
        print(f"  - å¿½ç•¥(æ— æ•ˆåŸŸå): {self.stats['invalid_domains']:,}")
        print(f"  - é‡å¤åŸŸå: {self.stats['duplicate_domains']:,}")
        print(f"  - è‡ªåŠ¨åˆ†ç±»å¤„ç†: {self.stats.get('auto_classified', 0):,}")
        print(f"  - ğŸ†• ä¸»åŠ¨æ·»åŠ åŸŸå: {self.stats.get('auto_added', 0):,}")
        print(f"  - ğŸ”„ ä»æ•°æ®æºè·³è¿‡: {self.stats.get('skipped_from_sources', 0):,}")
        print(f"  - ğŸ”„ skip è§„åˆ™è¢«è¦†ç›–: {self.stats.get('skip_overridden', 0):,}")
        if self.stats.get('v2ray_with_tags', 0) > 0:
            print(f"  - ğŸ“ v2ray å¸¦æ ‡ç­¾è§„åˆ™: {self.stats.get('v2ray_with_tags', 0):,}")

        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.config['output']['directory']}")

        print(f"\nğŸ“¡ æ•°æ®æº:")
        for source in self.config["sources"]:
            if source.get("enabled", True):
                print(f"  âœ… {source['name']} ({source.get('format', 'domain')})")
            else:
                print(f"  âŒ {source['name']} (å·²ç¦ç”¨)")

        print(f"\nâš™ï¸  é…ç½®:")
        print(f"  - å¿½ç•¥ç‰¹å®šè·¯å¾„è§„åˆ™: {self.config['parsing']['ignore_specific_paths']}")
        print(f"  - ä¸¥æ ¼åŸŸåçº§åˆ«æ£€æŸ¥: {self.config['parsing'].get('strict_domain_level_check', True)}")
        print(f"  - å¿½ç•¥IPåœ°å€: {self.config['parsing']['ignore_ip']}")
        print(f"  - å¿½ç•¥localhost: {self.config['parsing']['ignore_localhost']}")
        print(f"  - ä¿æŒåŸå§‹ç»“æ„: {self.config['parsing'].get('preserve_original_structure', True)}")
        print(f"  - ä¿æŒ www. å‰ç¼€: {self.config['parsing'].get('preserve_www_prefix', True)}")

        # è‡ªåŠ¨åˆ†ç±»é…ç½®
        auto_classify_config = self.config.get("auto_classify", {})
        print(f"\nğŸ”„ è‡ªåŠ¨åˆ†ç±»é…ç½®:")
        if auto_classify_config.get("enabled", False):
            print(f"  - çŠ¶æ€: å·²å¯ç”¨")
            print(f"  - å†…ç½®è§„åˆ™: {len(auto_classify_config.get('rules', []))} ä¸ª")
            print(f"  - å¤–éƒ¨æº: {len([s for s in auto_classify_config.get('sources', []) if s.get('enabled', True)])} ä¸ª")
            print(f"  - æ€»è®¡è§„åˆ™: {len(self.auto_classify_rules)} ä¸ª")
            print(f"  - é‡æ–°åˆ†ç±»åŸŸå: {self.stats.get('auto_classified', 0):,} ä¸ª")
            print(f"  - ğŸ†• ä¸»åŠ¨æ·»åŠ åŸŸå: {self.stats.get('auto_added', 0):,} ä¸ª")
            print(f"  - ğŸ”„ ä»æ•°æ®æºè·³è¿‡: {self.stats.get('skipped_from_sources', 0):,} ä¸ª")
            print(f"  - ğŸ”„ skip è§„åˆ™è¦†ç›–: {self.stats.get('skip_overridden', 0):,} ä¸ª")
        else:
            print(f"  - çŠ¶æ€: å·²ç¦ç”¨")

        # è‡ªå®šä¹‰è§„åˆ™é…ç½®
        custom_rules_config = self.config.get("custom_rules", {})
        print(f"\nğŸ“ è‡ªå®šä¹‰è§„åˆ™æ–‡ä»¶:")
        if custom_rules_config.get("enabled", False):
            print(f"  - çŠ¶æ€: å·²å¯ç”¨")
            enabled_sources = [s for s in custom_rules_config.get("sources", []) if s.get("enabled", True)]
            print(f"  - å¯ç”¨çš„æ–‡ä»¶æº: {len(enabled_sources)} ä¸ª")
            for source in enabled_sources:
                file_exists = "âœ…" if os.path.exists(source['file']) else "âŒ"
                format_info = f" - æ ¼å¼: {source.get('format', 'domain')}"
                print(f"    {file_exists} {source['name']}: {source['file']} ({source.get('action', 'remove')}){format_info}")
        else:
            print(f"  - çŠ¶æ€: å·²ç¦ç”¨")

        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        opt_config = self.config["optimization"]
        print(f"\nğŸš€ æ€§èƒ½ä¼˜åŒ–:")
        print(f"  - å¯ç”¨åŸŸååˆå¹¶: {opt_config.get('merge_domains', True)}")
        print(f"  - æ™ºèƒ½åŸŸåæ’åº: {opt_config.get('sort_before_merge', True)}")
        print(f"  - é«˜çº§TLDä¼˜åŒ–: {opt_config.get('enable_advanced_tld_merge', True)}")
        print(f"  - å¼ºåˆ¶å•è¡Œæ­£åˆ™: {self.force_single_regex or opt_config.get('force_single_regex', False)}")

        if not (self.force_single_regex or opt_config.get('force_single_regex', False)):
            print(f"  - æ¯è§„åˆ™æœ€å¤§åŸŸåæ•°: {opt_config.get('max_domains_per_rule', 30)}")
            print(f"  - æŒ‰TLDåˆ†ç»„: {opt_config.get('group_by_tld', True)}")
            print(f"  - æœ€å¤§è§„åˆ™é•¿åº¦: {opt_config.get('max_rule_length', 4000):,}")

        print(f"  - å‰ç¼€ä¼˜åŒ–: {opt_config.get('enable_prefix_optimization', True)}")
        print(f"  - åç¼€ä¼˜åŒ–: {opt_config.get('enable_suffix_optimization', True)}")

        print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        if self.config["output"]["mode"] == "separate_files":
            print("åœ¨ SearXNG settings.yml ä¸­æ·»åŠ :")
            print("hostnames:")
            for rule_type, filename in self.config["output"]["files"].items():
                if rule_type != "main_config" and rule_type in ["replace", "remove", "low_priority", "high_priority"]:
                    print(f"  {rule_type}: '{filename}'")
        else:
            print("å°†ç”Ÿæˆçš„ hostnames.yml å†…å®¹å¤åˆ¶åˆ° SearXNG settings.yml ä¸­")

        print(f"\nâœ¨ ä¼˜åŒ–æ•ˆæœ:")
        if total_domains > 0 and total_rules > 0:
            compression_ratio = (total_rules / total_domains) * 100
            print(f"  - å‹ç¼©æ¯”ç‡: {compression_ratio:.1f}% ({total_domains:,} ä¸ªåŸŸå -> {total_rules} æ¡è§„åˆ™)")
            if compression_ratio < 10:
                print("  - ğŸ‰ å‹ç¼©æ•ˆæœæä½³ï¼å¤§é‡åŸŸåè¢«åˆå¹¶ä¼˜åŒ–")
            elif compression_ratio < 50:
                print("  - ğŸ‘ å‹ç¼©æ•ˆæœè‰¯å¥½")
            else:
                print("  - ğŸ“ è§„åˆ™è¾ƒå¤šï¼Œå¯è€ƒè™‘å¯ç”¨å•è¡Œæ­£åˆ™æ¨¡å¼")

        # æ˜¾ç¤ºå„ç±»åˆ«çš„å‹ç¼©æƒ…å†µ
        print(f"\nğŸ“ˆ å„ç±»åˆ«å‹ç¼©è¯¦æƒ…:")
        for rule_type in ["replace", "remove", "low_priority", "high_priority"]:
            if rule_type in rules:
                rule_data = rules[rule_type]
                rule_count = len(rule_data) if isinstance(rule_data, (list, dict)) else 0
                domain_count = self.category_domain_counts.get(rule_type, 0)
                if domain_count > 0 and rule_count > 0:
                    category_ratio = (rule_count / domain_count) * 100
                    print(f"  - {rule_type}: {category_ratio:.1f}% ({domain_count} ä¸ªåŸŸå -> {rule_count} æ¡è§„åˆ™)")
                elif domain_count == 0 and rule_count == 0:
                    print(f"  - {rule_type}: ç©º (0 ä¸ªåŸŸå -> 0 æ¡è§„åˆ™)")
                else:
                    print(f"  - {rule_type}: {domain_count} ä¸ªåŸŸå -> {rule_count} æ¡è§„åˆ™")

        print(f"\nğŸ†• v2ray æ ¼å¼æ”¯æŒ:")
        print(f"  - domain:example.com         # åŒ¹é…åŸŸååŠå…¶æ‰€æœ‰å­åŸŸå")
        print(f"  - full:example.com           # å®Œå…¨åŒ¹é…æŒ‡å®šåŸŸå")
        print(f"  - domain:example.com:@tag    # å¸¦æ ‡ç­¾çš„åŸŸåè§„åˆ™")
        print(f"  - æ ‡ç­¾ä¿¡æ¯ä¼šè¢«è®°å½•ä½†ä¸å½±å“åŸŸååŒ¹é…")
        print(f"  - åªæœ‰æ˜ç¡®çš„ç«¯å£å·(çº¯æ•°å­—)æ‰ä¼šè¢«ç§»é™¤")
        if self.stats.get('v2ray_with_tags', 0) > 0:
            print(f"  - æœ¬æ¬¡å¤„ç†äº† {self.stats.get('v2ray_with_tags', 0)} ä¸ªå¸¦æ ‡ç­¾çš„ v2ray è§„åˆ™")

        print(f"\nğŸ”§ åŸå§‹ç»“æ„ä¿æŒ:")
        print(f"  - ä¿æŒ www.example.com çš„ www. å‰ç¼€")
        print(f"  - ä¿æŒå­åŸŸåçš„å®Œæ•´ç»“æ„")
        print(f"  - åªç§»é™¤æ˜ç¡®çš„åè®®å’Œç«¯å£ä¿¡æ¯")
        print(f"  - v2ray æ ¼å¼åŸŸåå®Œå…¨ä¿æŒåŸå§‹ç»“æ„")

        print(f"\nğŸ“ æ”¯æŒçš„æ–‡ä»¶æ ¼å¼:")
        print(f"  - domain: çº¯åŸŸåæ ¼å¼ (æ¯è¡Œä¸€ä¸ªåŸŸå)")
        print(f"  - regex: æ­£åˆ™è¡¨è¾¾å¼æ ¼å¼ (ç›´æ¥ä½¿ç”¨çš„æ­£åˆ™)")
        print(f"  - ublock: uBlock Origin æ ¼å¼")
        print(f"  - v2ray: v2ray æ ¼å¼ (domain:example.com, full:example.com, domain:example.com:@tag)")
        print(f"  - replace: æ›¿æ¢æ ¼å¼ (old_domain=new_domain)")
        print(f"  - classify: è‡ªåŠ¨åˆ†ç±»æ ¼å¼ (action:domain)")

        print(f"\nğŸ”§ è‡ªåŠ¨åˆ†ç±»è¯­æ³•ç¤ºä¾‹:")
        print(f"  - skip:csdn.net              # ä»æ•°æ®æºè·³è¿‡ï¼ˆä½†ä¸é˜»æ­¢å…¶ä»–è§„åˆ™ï¼‰")
        print(f"  - low_priority:csdn.net      # æ·»åŠ åˆ°ä½ä¼˜å…ˆçº§ï¼ˆä¼šè¦†ç›– skipï¼‰")
        print(f"  - remove:baidu.com           # å°† baidu.com æ·»åŠ åˆ°ç§»é™¤åˆ—è¡¨")
        print(f"  - high_priority:wikipedia.org # å°† wikipedia.org æ·»åŠ åˆ°é«˜ä¼˜å…ˆçº§åˆ—è¡¨")
        print(f"  - replace:youtube.com=yt.example.com # æ›¿æ¢è§„åˆ™")

        print(f"\nğŸ”§ ä¿®å¤åçš„ Skip è§„åˆ™è¡Œä¸º:")
        print(f"  - skip:csdn.net - åªä¼šä»æ•°æ®æºçš„é»˜è®¤å¤„ç†ä¸­è·³è¿‡ csdn.net")
        print(f"  - low_priority:csdn.net - ä¼šä¸»åŠ¨å°† csdn.net æ·»åŠ åˆ°ä½ä¼˜å…ˆçº§åˆ—è¡¨")
        print(f"  - å¦‚æœåŒæ—¶å­˜åœ¨ï¼Œlow_priority è§„åˆ™ä¼šç”Ÿæ•ˆï¼Œskip è¢«è¦†ç›–")

        if self.stats.get('skip_overridden', 0) > 0:
            print(f"\nğŸ”„ Skip è§„åˆ™è¦†ç›–è¯¦æƒ…:")
            print(f"  - æœ‰ {self.stats.get('skip_overridden', 0)} ä¸ªåŸŸåçš„ skip è§„åˆ™è¢«å…¶ä»–è‡ªåŠ¨åˆ†ç±»è§„åˆ™è¦†ç›–")
            print(f"  - è¿™æ„å‘³ç€è¿™äº›åŸŸåä¸ä¼šä»æ•°æ®æºè·³è¿‡ï¼Œä½†ä¼šè¢«æ·»åŠ åˆ°æŒ‡å®šç±»åˆ«")
            print(f"  - è¿™æ­£æ˜¯æœŸæœ›çš„è¡Œä¸ºï¼šæ˜ç¡®çš„åˆ†ç±»è§„åˆ™ä¼˜å…ˆçº§é«˜äº skip è§„åˆ™")


def create_sample_config():
    """
    åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶ (æ”¯æŒ v2ray æ ¼å¼ï¼Œä¿æŒåŸå§‹ç»“æ„)
    """
    sample_config = {
        "sources": [
            {
                "name": "Google Chinese Results Blocklist",
                "url": "https://raw.githubusercontent.com/cobaltdisco/Google-Chinese-Results-Blocklist/refs/heads/master/GHHbD_perma_ban_list.txt",
                "action": "remove",
                "format": "domain",
                "enabled": True
            },
            {
                "name": "Chinese Internet is Dead",
                "url": "https://raw.githubusercontent.com/obgnail/chinese-internet-is-dead/master/blocklist.txt",
                "action": "remove",
                "format": "ublock",
                "enabled": True
            },
            {
                "name": "Luxirty Search Block List",
                "url": "https://raw.githubusercontent.com/KoriIku/luxirty-search/refs/heads/main/docs/block_list.txt",
                "action": "remove",
                "format": "domain",
                "enabled": True
            },
            {
                "name": "Example v2ray Rules",
                "url": "https://example.com/v2ray-rules.txt",
                "action": "remove",
                "format": "v2ray",
                "enabled": False
            }
        ],

        "custom_rules": {
            "enabled": True,
            "sources": [
                {
                    "name": "Custom Remove Rules",
                    "file": "./custom_remove.txt",
                    "action": "remove",
                    "format": "domain",
                    "enabled": False
                },
                {
                    "name": "Custom Low Priority Rules",
                    "file": "./custom_low_priority.txt",
                    "action": "low_priority",
                    "format": "domain",
                    "enabled": False
                },
                {
                    "name": "Custom High Priority Rules",
                    "file": "./custom_high_priority.txt",
                    "action": "high_priority",
                    "format": "domain",
                    "enabled": False
                },
                {
                    "name": "Custom Replace Rules",
                    "file": "./custom_replace.txt",
                    "action": "replace",
                    "format": "replace",
                    "enabled": False
                },
                {
                    "name": "Custom v2ray Rules",
                    "file": "./custom_v2ray.txt",
                    "action": "remove",
                    "format": "v2ray",
                    "enabled": False
                }
            ]
        },

        "auto_classify": {
            "enabled": True,
            "sources": [
                {
                    "name": "Auto Classify Rules",
                    "file": "./auto_classify.txt",
                    "format": "classify",
                    "enabled": True
                }
            ],
            "rules": [
                "remove:example.com",
                "low_priority:google.com",
                "high_priority:wikipedia.org",
                "replace:youtube.com=yt.example.com",
                "skip:github.com"
            ]
        },

        "replace_rules": {
            '(.*\.)?youtube\.com$': 'yt.example.com',
            '(.*\.)?youtu\.be$': 'yt.example.com',
            '(.*\.)?reddit\.com$': 'teddit.example.com'
        },

        "fixed_remove": [
            '(.*\.)?facebook.com$'
        ],

        "fixed_low_priority": [
            '(.*\.)?google(\..*)?$'
        ],

        "fixed_high_priority": [
            '(.*\.)?wikipedia.org$'
        ],

        "parsing": {
            "ignore_specific_paths": True,
            "ignore_ip": True,
            "ignore_localhost": True,
            "strict_domain_level_check": True,
            "preserve_www_prefix": True,  # ä¿æŒ www. å‰ç¼€
            "preserve_original_structure": True  # ä¿æŒåŸå§‹åŸŸåç»“æ„
        },

        "optimization": {
            "merge_domains": True,
            "max_domains_per_rule": 256,
            "group_by_tld": True,
            "use_trie_optimization": True,
            "max_rule_length": 65536,
            "optimize_tld_grouping": True,
            "enable_prefix_optimization": True,
            "enable_suffix_optimization": True,
            "min_common_prefix_length": 3,
            "min_common_suffix_length": 3,
            "force_single_regex": False,
            "sort_before_merge": True,
            "enable_advanced_tld_merge": True
        },

        "request_config": {
            "timeout": 30,
            "retry_count": 3,
            "retry_delay": 1
        },

        "output": {
            "mode": "separate_files",
            "directory": "./rules/",
            "files": {
                "replace": "rewrite-hosts.yml",
                "remove": "remove-hosts.yml",
                "low_priority": "low-priority-hosts.yml",
                "high_priority": "high-priority-hosts.yml",
                "main_config": "hostnames-config.yml"
            }
        }
    }

    with open("config.yaml", "w", encoding="utf-8") as f:
        yaml.dump(sample_config, f, default_flow_style=False, allow_unicode=True, indent=2)

    # åˆ›å»ºç¤ºä¾‹è‡ªåŠ¨åˆ†ç±»æ–‡ä»¶
    with open("auto_classify.txt", "w", encoding="utf-8") as f:
        f.write("""# è‡ªåŠ¨åˆ†ç±»è§„åˆ™ç¤ºä¾‹æ–‡ä»¶
# è¯­æ³•ï¼šaction:domain
# æ”¯æŒçš„ actionï¼šremove, low_priority, high_priority, replace, skip

# ç§»é™¤è§„åˆ™ - å°†åŸŸåæ·»åŠ åˆ°ç§»é™¤åˆ—è¡¨
remove:baidu.com
remove:*.csdn.net
remove:zhihu.com

# ä½ä¼˜å…ˆçº§è§„åˆ™
low_priority:google.com
low_priority:*.google.com

# é«˜ä¼˜å…ˆçº§è§„åˆ™
high_priority:wikipedia.org
high_priority:*.wikipedia.org
high_priority:www.wikipedia.org  # ä¼šä¿æŒ www. å‰ç¼€

# æ›¿æ¢è§„åˆ™ - æ ¼å¼ï¼šreplace:old_domain=new_domain
replace:youtube.com=yt.example.com
replace:www.youtube.com=yt.example.com  # www å‰ç¼€ä¼šè¢«ä¿ç•™åœ¨åŒ¹é…ä¸­
replace:twitter.com=nitter.example.com

# è·³è¿‡è§„åˆ™ - åªè·³è¿‡æ•°æ®æºå¤„ç†ï¼Œä¸é˜»æ­¢æ˜ç¡®çš„è‡ªåŠ¨åˆ†ç±»è§„åˆ™
skip:github.com
skip:*.github.com
skip:stackoverflow.com

# ç¤ºä¾‹ï¼šåŒæ—¶æœ‰ skip å’Œ low_priority è§„åˆ™
# skip è§„åˆ™ä¼šè¢« low_priority è¦†ç›–ï¼ŒåŸŸåä¼šè¢«æ·»åŠ åˆ°ä½ä¼˜å…ˆçº§åˆ—è¡¨
skip:csdn.net
low_priority:csdn.net
""")

    # åˆ›å»ºç¤ºä¾‹è‡ªå®šä¹‰è§„åˆ™æ–‡ä»¶
    with open("custom_remove.txt", "w", encoding="utf-8") as f:
        f.write("""# è‡ªå®šä¹‰ç§»é™¤è§„åˆ™ç¤ºä¾‹
# æ¯è¡Œä¸€ä¸ªåŸŸåï¼Œæ”¯æŒæ³¨é‡Š

example1.com  # ç¤ºä¾‹åŸŸå1
example2.com  # ç¤ºä¾‹åŸŸå2
www.example3.com  # www å‰ç¼€ä¼šè¢«ä¿æŒ
*.example4.com  # ç¤ºä¾‹é€šé…ç¬¦åŸŸå
""")

    with open("custom_replace.txt", "w", encoding="utf-8") as f:
        f.write("""# è‡ªå®šä¹‰æ›¿æ¢è§„åˆ™ç¤ºä¾‹
# æ ¼å¼ï¼šold_domain=new_domain

old.example.com=new.example.com
www.old.example.com=new.example.com
another.old.com=another.new.com
""")

    # åˆ›å»ºç¤ºä¾‹ v2ray è§„åˆ™æ–‡ä»¶
    with open("custom_v2ray.txt", "w", encoding="utf-8") as f:
        f.write("""# v2ray æ ¼å¼è§„åˆ™ç¤ºä¾‹æ–‡ä»¶
# æ”¯æŒçš„æ ¼å¼ï¼š
# domain:example.com     - åŒ¹é…åŸŸååŠå…¶æ‰€æœ‰å­åŸŸå
# full:example.com       - å®Œå…¨åŒ¹é…æŒ‡å®šåŸŸå
# domain:example.com:@tag - å¸¦æ ‡ç­¾çš„åŸŸåè§„åˆ™

# åŸŸåçº§åˆ«åŒ¹é…ï¼ˆåŒ…æ‹¬å­åŸŸåï¼‰
domain:scopus.com
domain:researchgate.net
domain:academia.edu
domain:www.researchkit.cn  # www å‰ç¼€ä¼šè¢«ä¿æŒ

# å®Œå…¨åŒ¹é…
full:scholar.google.ae
full:scholar.google.com.hk
full:pubmed.ncbi.nlm.nih.gov
full:www.scholar.google.com  # www å‰ç¼€ä¼šè¢«ä¿æŒ

# å†…å®¹å†œåœºåŸŸå
domain:csdn.net
domain:jianshu.com
domain:zhihu.com
domain:www.cnblogs.com  # www å‰ç¼€ä¼šè¢«ä¿æŒ

# å¸¦æ ‡ç­¾çš„è§„åˆ™ç¤ºä¾‹
domain:researchkit.cn:@cn  # å¸¦åœ°åŒºæ ‡ç­¾
full:www.example.com:@test:@demo  # å¤šä¸ªæ ‡ç­¾
domain:academic.example.com:@academic:@high_priority  # å¤åˆæ ‡ç­¾

# æ³¨é‡Šç¤ºä¾‹
# domain:example.com  # è¿™æ˜¯æ³¨é‡Š
""")

    print("ç¤ºä¾‹é…ç½®æ–‡ä»¶å·²åˆ›å»º: config.yaml")
    print("ç¤ºä¾‹è‡ªåŠ¨åˆ†ç±»æ–‡ä»¶å·²åˆ›å»º: auto_classify.txt")
    print("ç¤ºä¾‹è‡ªå®šä¹‰è§„åˆ™æ–‡ä»¶å·²åˆ›å»º: custom_remove.txt, custom_replace.txt")
    print("ğŸ†• ç¤ºä¾‹ v2ray è§„åˆ™æ–‡ä»¶å·²åˆ›å»º: custom_v2ray.txt")

    print("\nğŸ†• v2ray æ ¼å¼è¯´æ˜:")
    print("  - domain:example.com         # åŒ¹é…åŸŸååŠå…¶æ‰€æœ‰å­åŸŸå")
    print("  - full:example.com           # å®Œå…¨åŒ¹é…æŒ‡å®šåŸŸå")
    print("  - domain:example.com:@tag    # å¸¦æ ‡ç­¾çš„åŸŸåè§„åˆ™")
    print("  - æ ‡ç­¾ä¿¡æ¯ä¼šè¢«æ˜¾ç¤ºä½†ä¸å½±å“åŸŸåå¤„ç†")
    print("  - åŸŸåçš„åŸå§‹ç»“æ„(åŒ…æ‹¬wwwå‰ç¼€)å®Œå…¨ä¿æŒ")

    print("\nğŸ”§ åŸå§‹ç»“æ„ä¿æŒè¯´æ˜:")
    print("  - www.example.com ä¼šä¿æŒ www. å‰ç¼€")
    print("  - sub.example.com ä¼šä¿æŒå®Œæ•´çš„å­åŸŸåç»“æ„")
    print("  - åªæœ‰æ˜ç¡®çš„åè®®(http://)å’Œç«¯å£å·(:8080)æ‰ä¼šè¢«ç§»é™¤")
    print("  - v2ray æ ¼å¼ä¸­çš„æ ‡ç­¾(:@tag)ä¼šè¢«è¯†åˆ«ä½†ä¸å½±å“åŸŸåæœ¬èº«")

    print("\nğŸ”„ ä¿®å¤åçš„è‡ªåŠ¨åˆ†ç±»è¯­æ³•è¯´æ˜:")
    print("  - skip:domain.com            # åªä»æ•°æ®æºè·³è¿‡ï¼Œä¸é˜»æ­¢å…¶ä»–è§„åˆ™")
    print("  - remove:domain.com          # å°†åŸŸåæ·»åŠ åˆ°ç§»é™¤åˆ—è¡¨")
    print("  - low_priority:domain.com    # å°†åŸŸåæ·»åŠ åˆ°ä½ä¼˜å…ˆçº§åˆ—è¡¨")
    print("  - high_priority:domain.com   # å°†åŸŸåæ·»åŠ åˆ°é«˜ä¼˜å…ˆçº§åˆ—è¡¨")
    print("  - replace:old.com=new.com    # æ›¿æ¢è§„åˆ™")
    print("  - remove:*.domain.com        # æ”¯æŒé€šé…ç¬¦åŒ¹é…å­åŸŸå")

    print("\nğŸ”„ Skip è§„åˆ™è¡Œä¸ºè¯´æ˜:")
    print("  - skip åªå½±å“ä»æ•°æ®æºçš„é»˜è®¤å¤„ç†")
    print("  - å¦‚æœåŒä¸€åŸŸåæœ‰å¤šä¸ªè§„åˆ™ï¼Œæ˜ç¡®çš„åˆ†ç±»è§„åˆ™ä¼šè¦†ç›– skip")
    print("  - ä¾‹å¦‚ï¼šskip:csdn.net + low_priority:csdn.net = csdn.net è¢«æ·»åŠ åˆ°ä½ä¼˜å…ˆçº§")

    print("\nğŸ“ æ”¯æŒçš„æ–‡ä»¶æ ¼å¼:")
    print("  - domain: çº¯åŸŸåæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªåŸŸå")
    print("  - regex: æ­£åˆ™è¡¨è¾¾å¼æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨")
    print("  - ublock: uBlock Origin æ ¼å¼")
    print("  - v2ray: v2ray æ ¼å¼ (domain:example.com, full:example.com, domain:example.com:@tag)")
    print("  - replace: æ›¿æ¢æ ¼å¼ï¼Œold_domain=new_domain")
    print("  - classify: è‡ªåŠ¨åˆ†ç±»æ ¼å¼ï¼Œaction:domain")


def main():
    parser = argparse.ArgumentParser(description="SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨ (å®Œæ•´ç‰ˆ - è‡ªåŠ¨åˆ†ç±» + è‡ªå®šä¹‰æ–‡ä»¶ + TLDä¼˜åŒ– + v2rayæ ¼å¼ - ä¿æŒåŸå§‹ç»“æ„) - Skip ä¿®å¤ç‰ˆ")
    parser.add_argument("-c", "--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--create-config", action="store_true", help="åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶å’Œç¤ºä¾‹è§„åˆ™æ–‡ä»¶")
    parser.add_argument("--single-regex", action="store_true", help="å¼ºåˆ¶ç”Ÿæˆé«˜çº§TLDä¼˜åŒ–çš„å•è¡Œæ­£åˆ™è¡¨è¾¾å¼")

    args = parser.parse_args()

    if args.create_config:
        create_sample_config()
        return

    generator = SearXNGHostnamesGenerator(args.config, force_single_regex=args.single_regex)
    generator.run()


if __name__ == "__main__":
    main()
