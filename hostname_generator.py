#!/usr/bin/env python3
"""
SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨

pip install requests pyyaml argparse
"""

import requests
import yaml
import json
import re
import csv
from urllib.parse import urlparse
from typing import Dict, List, Set, Union, Tuple
import argparse
import sys
import time
import os
from collections import OrderedDict, defaultdict

class SearXNGHostnamesGenerator:
    def __init__(self, config_file: str = None, force_single_regex: bool = False):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            force_single_regex: å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™è¡¨è¾¾å¼
        """
        self.config = self.load_config(config_file)
        self.force_single_regex = force_single_regex
        self.domains = set()
        self.auto_classify_rules = []  # è‡ªåŠ¨åˆ†ç±»è§„åˆ™
        self.stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'path_to_low_priority': 0,  # ç‰¹å®šè·¯å¾„è®¾ç½®ä¸ºä½ä¼˜å…ˆçº§çš„æ•°é‡
            'path_kept_action': 0,      # ğŸ”§ ç‰¹å®šè·¯å¾„ä¿æŒåŸåŠ¨ä½œçš„æ•°é‡
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0,
            'auto_classified': 0,  # è‡ªåŠ¨åˆ†ç±»çš„æ•°é‡
            'auto_added': 0,  # ä¸»åŠ¨æ·»åŠ çš„åŸŸåæ•°é‡
            'skipped_from_sources': 0,  # ä»æ•°æ®æºè·³è¿‡çš„åŸŸåæ•°é‡
            'skip_overridden': 0,  # skip è§„åˆ™è¢«å…¶ä»–è§„åˆ™è¦†ç›–çš„æ•°é‡
            'v2ray_with_tags': 0,  # å¸¦æ ‡ç­¾çš„ v2ray è§„åˆ™æ•°é‡
            'csv_parsed_rows': 0,  # CSV è§£æçš„è¡Œæ•°
            'csv_invalid_urls': 0,  # CSV ä¸­æ— æ•ˆ URL çš„æ•°é‡
            'csv_extracted_domains': 0,  # CSV ä¸­æˆåŠŸæå–çš„åŸŸåæ•°é‡
            'wildcard_rules_processed': 0,  # ğŸ”§ å¤„ç†çš„é€šé…ç¬¦è§„åˆ™æ•°é‡
        }
        # è®°å½•æ¯ä¸ªç±»åˆ«çš„åŸŸåæ•°é‡
        self.category_domain_counts = {
            'remove': 0,
            'low_priority': 0,
            'high_priority': 0,
            'replace': 0
        }

        # åŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™
        self.load_auto_classify_rules()

    def load_config(self, config_file: str) -> Dict:
        """
        åŠ è½½é…ç½®æ–‡ä»¶

        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„

        Returns:
            é…ç½®å­—å…¸
        """
        default_config = {
            # æ•°æ®æºé…ç½®
            "sources": [
                {
                    "name": "Content Farm Terminator - Content Farm Filters",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/content-farms.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Content Farm Terminator - Nearly Content Farm Filters",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/nearly-content-farms.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Content Farm Terminator - Bad Cloners",
                    "url": "https://danny0838.github.io/content-farm-terminator/files/blocklist-ublacklist/bad-cloners.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "Paxxs - Google Blocklist",
                    "url": "https://raw.githubusercontent.com/Paxxs/Google-Blocklist/refs/heads/develop/uBlacklist_subscription.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "cobaltdisco - Google Chinese Results Blocklist",
                    "url": "https://raw.githubusercontent.com/cobaltdisco/Google-Chinese-Results-Blocklist/refs/heads/master/uBlacklist_subscription.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "obgnail - Chinese Internet is Dead",
                    "url": "https://raw.githubusercontent.com/obgnail/chinese-internet-is-dead/master/blocklist.txt",
                    "action": "remove",
                    "format": "ublock",
                    "enabled": True
                },
                {
                    "name": "timqian - Chinese Independent Blogs",
                    "url": "https://raw.githubusercontent.com/timqian/chinese-independent-blogs/refs/heads/master/blogs-original.csv",
                    "action": "high_priority",
                    "format": "csv",
                    "csv_config": {
                        "column": "Address",
                        "has_header": True,
                        "delimiter": ",",
                        "encoding": "utf-8"
                    },
                    "enabled": True
                },
                {
                    "name": "bcaso - Computer Science Whitelist",
                    "url": "https://raw.githubusercontent.com/bcaso/Computer-Science-Whitelist/refs/heads/main/whitelists/domain_name.txt",
                    "action": "high_priority",
                    "format": "ublock",
                    "enabled": True
                },
            ],

            # è‡ªå®šä¹‰è§„åˆ™é…ç½®ï¼ˆä»æ–‡ä»¶è¯»å–ï¼‰
            "custom_rules": {
                "enabled": False,
                "sources": []
            },

            # è‡ªåŠ¨åˆ†ç±»è¯­æ³•é…ç½®ï¼ˆæ›¿æ¢åŸç™½åå•åŠŸèƒ½ï¼‰
            "auto_classify": {
                "enabled": True,
                "sources": [
                    {
                        "name": "Auto Classify Rules",
                        "file": "./auto_classify.txt",
                        "format": "classify",  # æ”¯æŒ action:domain è¯­æ³•
                        "enabled": True
                    },
                ],
                # ç›´æ¥åœ¨é…ç½®ä¸­å®šä¹‰çš„è‡ªåŠ¨åˆ†ç±»è§„åˆ™
                "rules": []
            },

            # åŸŸåæ›¿æ¢è§„åˆ™ï¼ˆä¿ç•™åŸé…ç½®æ–¹å¼ï¼‰
            "replace_rules": {},

            # å›ºå®šçš„ç§»é™¤è§„åˆ™
            "fixed_remove": [],

            # å›ºå®šçš„ä½ä¼˜å…ˆçº§è§„åˆ™
            "fixed_low_priority": [],

            # å›ºå®šçš„é«˜ä¼˜å…ˆçº§è§„åˆ™
            "fixed_high_priority": [],

            # è§£æé…ç½®
            "parsing": {
                "ignore_specific_paths": False,     # ä¸å¿½ç•¥ç‰¹å®šè·¯å¾„è§„åˆ™
                # ğŸ”§ ä¿®å¤ï¼šç‰¹å®šè·¯å¾„è§„åˆ™å¤„ç†æ–¹å¼çš„è¯¦ç»†è¯´æ˜å’Œé»˜è®¤å€¼ä¿®æ”¹
                "specific_path_action": "smart",  # ç‰¹å®šè·¯å¾„è§„åˆ™å¤„ç†æ–¹å¼ï¼š
                                                        # - "keep_action": ä¿æŒæºçš„åŸå§‹åŠ¨ä½œ (æ¨è)
                                                        # - "low_priority": å¼ºåˆ¶è®¾ä¸ºä½ä¼˜å…ˆçº§
                                                        # - "ignore": å®Œå…¨å¿½ç•¥
                                                        # - "smart": æ™ºèƒ½å¤„ç† (remove->low_priority, å…¶ä»–ä¿æŒ)
                "ignore_ip": True,     # å¿½ç•¥IPåœ°å€
                "ignore_localhost": True,  # å¿½ç•¥æœ¬åœ°ä¸»æœº
                "strict_domain_level_check": True,  # ä¸¥æ ¼æ£€æŸ¥åŸŸåçº§åˆ«è§„åˆ™
                "preserve_www_prefix": True,  # ä¿æŒ www. å‰ç¼€
                "preserve_original_structure": True  # ä¿æŒåŸå§‹åŸŸåç»“æ„
            },

            # æ€§èƒ½ä¼˜åŒ–é…ç½®
            "optimization": {
                "merge_domains": True,          # å¯ç”¨åŸŸååˆå¹¶ä¼˜åŒ–
                "max_domains_per_rule": 256,     # æ¯ä¸ªåˆå¹¶è§„åˆ™çš„æœ€å¤§åŸŸåæ•°
                "group_by_tld": True,           # æŒ‰é¡¶çº§åŸŸååˆ†ç»„
                "use_trie_optimization": True,  # ä½¿ç”¨å­—å…¸æ ‘ä¼˜åŒ–
                "max_rule_length": 65536,       # å•ä¸ªè§„åˆ™çš„æœ€å¤§é•¿åº¦é™åˆ¶
                "optimize_tld_grouping": True,   # ä¼˜åŒ–TLDåˆ†ç»„ï¼Œé¿å…é‡å¤
                "enable_prefix_optimization": True,  # å¯ç”¨å‰ç¼€ä¼˜åŒ–
                "enable_suffix_optimization": True,  # å¯ç”¨åç¼€ä¼˜åŒ–
                "min_common_prefix_length": 3,      # æœ€å°å…¬å…±å‰ç¼€é•¿åº¦
                "min_common_suffix_length": 3,      # æœ€å°å…¬å…±åç¼€é•¿åº¦
                "force_single_regex": False,         # å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™è¡¨è¾¾å¼
                "sort_before_merge": True,          # åˆå¹¶å‰æ’åºåŸŸå
                "enable_advanced_tld_merge": True   # å¯ç”¨é«˜çº§TLDåˆå¹¶
            },

            # è¯·æ±‚é…ç½®
            "request_config": {
                "timeout": 30,
                "retry_count": 3,
                "retry_delay": 1
            },

            # è¾“å‡ºé…ç½®
            "output": {
                "mode": "separate_files",  # separate_files æˆ– single_file
                "directory": "./rules/",
                "files": {
                    "replace": "rewrite-hosts.yml",
                    "remove": "remove-hosts.yml",
                    "low_priority": "low-priority-hosts.yml",
                    "high_priority": "high-priority-hosts.yml",
                    "main_config": "hostnames-config.yml"
                }
            }
        }

        if config_file:
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    # æ·±åº¦åˆå¹¶é…ç½®
                    self._deep_merge(default_config, user_config)
            except FileNotFoundError:
                print(f"é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            except Exception as e:
                print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                return default_config

        return default_config

    def _deep_merge(self, base_dict: Dict, update_dict: Dict) -> None:
        """
        æ·±åº¦åˆå¹¶å­—å…¸
        """
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_merge(base_dict[key], value)
            else:
                base_dict[key] = value

    def extract_hostname_from_url(self, url_string: str) -> str:
        """
        ä» URL å­—ç¬¦ä¸²ä¸­æå– hostname

        Args:
            url_string: URL å­—ç¬¦ä¸²

        Returns:
            æå–çš„ hostnameï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        if not url_string:
            return None

        url_string = url_string.strip()

        # å¦‚æœæ²¡æœ‰åè®®ï¼Œå°è¯•æ·»åŠ  http://
        if not url_string.startswith(('http://', 'https://', 'ftp://')):
            # æ£€æŸ¥æ˜¯å¦çœ‹èµ·æ¥åƒä¸€ä¸ªå®Œæ•´çš„åŸŸå
            if '.' in url_string and not url_string.startswith('/'):
                url_string = 'http://' + url_string
            else:
                return None

        try:
            parsed = urlparse(url_string)
            hostname = parsed.netloc

            if not hostname:
                return None

            # ç§»é™¤ç«¯å£å·
            if ':' in hostname:
                hostname = hostname.split(':')[0]

            # éªŒè¯åŸŸåæ ¼å¼
            if self.is_valid_domain(hostname):
                return hostname.lower()

        except Exception as e:
            print(f"  âŒ URL è§£æå¤±è´¥: {url_string} - {e}")

        return None

    def parse_csv_rule(self, csv_row: List[str], csv_config: Dict, row_num: int) -> Tuple[str, str]:
        """
        è§£æ CSV è¡Œï¼Œæå–åŸŸå

        Args:
            csv_row: CSV è¡Œæ•°æ®åˆ—è¡¨
            csv_config: CSV é…ç½®
            row_num: è¡Œå·ï¼ˆç”¨äºé”™è¯¯ä¿¡æ¯ï¼‰

        Returns:
            (åŸŸåæˆ– None, å¿½ç•¥åŸå› )
        """
        try:
            # è·å–ç›®æ ‡åˆ—çš„å€¼
            column = csv_config.get("column")
            column_index = csv_config.get("column_index")

            target_value = None

            if column_index is not None:
                # ä½¿ç”¨åˆ—ç´¢å¼•
                if 0 <= column_index < len(csv_row):
                    target_value = csv_row[column_index].strip()
                else:
                    return None, f"åˆ—ç´¢å¼• {column_index} è¶…å‡ºèŒƒå›´ (è¡Œ {row_num})"
            elif column:
                # ä½¿ç”¨åˆ—åï¼ˆéœ€è¦ headersï¼‰
                return None, "ä½¿ç”¨åˆ—åéœ€è¦åœ¨å¤´éƒ¨ä¿¡æ¯ä¸­æŸ¥æ‰¾ï¼Œè¿™åº”è¯¥åœ¨è°ƒç”¨æ–¹å¤„ç†"
            else:
                return None, "æœªæŒ‡å®šåˆ—åæˆ–åˆ—ç´¢å¼•"

            if not target_value:
                return None, "ç›®æ ‡åˆ—å€¼ä¸ºç©º"

            # ä» URL ä¸­æå– hostname
            hostname = self.extract_hostname_from_url(target_value)
            if hostname:
                return hostname, None
            else:
                return None, f"æ— æ³•ä» URL æå–åŸŸå: {target_value}"

        except Exception as e:
            return None, f"è§£æ CSV è¡Œæ—¶å‡ºé”™ (è¡Œ {row_num}): {e}"

    def load_csv_rules_from_file(self, file_path: str, csv_config: Dict, action: str) -> Tuple[Set[str], Dict[str, str], Dict]:
        """
        ä» CSV æ–‡ä»¶åŠ è½½è§„åˆ™

        Args:
            file_path: CSV æ–‡ä»¶è·¯å¾„
            csv_config: CSV é…ç½®
            action: åŠ¨ä½œç±»å‹

        Returns:
            (åŸŸåé›†åˆ, æ›¿æ¢è§„åˆ™å­—å…¸, ç»Ÿè®¡ä¿¡æ¯)
        """
        domains = set()
        replace_rules = {}
        stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'invalid_domains': 0,
            'ignored_comments': 0,
            'csv_parsed_rows': 0,
            'csv_invalid_urls': 0,
            'csv_extracted_domains': 0
        }

        # CSV é…ç½®é»˜è®¤å€¼
        has_header = csv_config.get("has_header", True)
        delimiter = csv_config.get("delimiter", ",")
        encoding = csv_config.get("encoding", "utf-8")
        column = csv_config.get("column")
        column_index = csv_config.get("column_index")

        try:
            with open(file_path, 'r', encoding=encoding) as f:
                csv_reader = csv.reader(f, delimiter=delimiter)

                headers = None
                actual_column_index = None

                for row_num, row in enumerate(csv_reader, 1):
                    if not row or all(cell.strip() == '' for cell in row):
                        continue  # è·³è¿‡ç©ºè¡Œ

                    stats['total_rules'] += 1

                    # å¤„ç†å¤´éƒ¨è¡Œ
                    if has_header and row_num == 1:
                        headers = [cell.strip() for cell in row]

                        # å¦‚æœæŒ‡å®šäº†åˆ—åï¼Œæ‰¾åˆ°å¯¹åº”çš„ç´¢å¼•
                        if column:
                            try:
                                actual_column_index = headers.index(column)
                                print(f"  ğŸ“ æ‰¾åˆ°ç›®æ ‡åˆ— '{column}' ä½äºç´¢å¼• {actual_column_index}")
                            except ValueError:
                                print(f"  âŒ æœªæ‰¾åˆ°æŒ‡å®šçš„åˆ—å '{column}'")
                                print(f"  ğŸ“‹ å¯ç”¨çš„åˆ—å: {', '.join(headers)}")
                                return domains, replace_rules, stats
                        elif column_index is not None:
                            actual_column_index = column_index
                            if actual_column_index < len(headers):
                                print(f"  ğŸ“ ä½¿ç”¨åˆ—ç´¢å¼• {actual_column_index}: '{headers[actual_column_index]}'")
                            else:
                                print(f"  âŒ åˆ—ç´¢å¼• {actual_column_index} è¶…å‡ºèŒƒå›´")
                                return domains, replace_rules, stats

                        continue  # è·³è¿‡å¤´éƒ¨è¡Œï¼Œä¸è§£ææ•°æ®

                    # å¦‚æœæ²¡æœ‰è®¾ç½®å®é™…åˆ—ç´¢å¼•ï¼Œä½¿ç”¨é…ç½®çš„åˆ—ç´¢å¼•
                    if actual_column_index is None and column_index is not None:
                        actual_column_index = column_index

                    stats['csv_parsed_rows'] += 1

                    try:
                        # è§£æ CSV è¡Œ
                        if actual_column_index is not None:
                            if actual_column_index < len(row):
                                url_value = row[actual_column_index].strip()
                                if url_value:
                                    domain = self.extract_hostname_from_url(url_value)
                                    if domain:
                                        domains.add(domain)
                                        stats['parsed_domains'] += 1
                                        stats['csv_extracted_domains'] += 1

                                        # æ˜¾ç¤ºä¸€äº›è§£ææ ·æœ¬
                                        if stats['csv_extracted_domains'] <= 5:
                                            print(f"    âœ… CSV è§£æ: {url_value} -> {domain}")
                                    else:
                                        stats['csv_invalid_urls'] += 1
                                        if stats['csv_invalid_urls'] <= 3:
                                            print(f"    âŒ æ— æ•ˆ URL: {url_value}")
                                else:
                                    stats['invalid_domains'] += 1
                            else:
                                stats['invalid_domains'] += 1
                                if stats['invalid_domains'] <= 3:
                                    print(f"    âŒ è¡Œ {row_num} åˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´")
                        else:
                            stats['invalid_domains'] += 1
                            print(f"    âŒ æœªè®¾ç½®æœ‰æ•ˆçš„åˆ—ç´¢å¼•")

                    except Exception as e:
                        print(f"    âŒ è§£æç¬¬ {row_num} è¡Œæ—¶å‡ºé”™: {e}")
                        stats['invalid_domains'] += 1

                print(f"    âœ… CSV è§£æå®Œæˆ: {stats['csv_extracted_domains']} ä¸ªæœ‰æ•ˆåŸŸå")
                if stats['csv_invalid_urls'] > 0:
                    print(f"    âš ï¸  å¿½ç•¥äº† {stats['csv_invalid_urls']} ä¸ªæ— æ•ˆ URL")

        except FileNotFoundError:
            print(f"    âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        except Exception as e:
            print(f"    âŒ è¯»å– CSV æ–‡ä»¶å¤±è´¥: {e}")

        return domains, replace_rules, stats

    def parse_v2ray_rule(self, rule: str) -> Tuple[str, str]:
        """
        è§£æ v2ray æ ¼å¼è§„åˆ™ï¼Œæå–åŸŸåï¼Œä¿æŒåŸå§‹ç»“æ„

        æ”¯æŒçš„æ ¼å¼ï¼š
        - domain:example.com          # åŒ¹é…åŸŸååŠæ‰€æœ‰å­åŸŸå
        - full:example.com            # å®Œå…¨åŒ¹é…åŸŸå
        - domain:example.com:@tag     # å¸¦æ ‡ç­¾çš„åŸŸåè§„åˆ™
        - full:www.example.com:@tag   # ä¿æŒ www å‰ç¼€

        Args:
            rule: v2ray è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            (åŸŸåæˆ– None, å¿½ç•¥åŸå› )
        """
        original_rule = rule  # ä¿å­˜åŸå§‹è§„åˆ™ç”¨äºè°ƒè¯•
        rule = rule.strip()

        if not rule or rule.startswith('#'):
            return None, "æ³¨é‡Šæˆ–ç©ºè¡Œ"

        # å¤„ç†è¡Œæœ«æ³¨é‡Š
        if '#' in rule:
            comment_pos = rule.find('#')
            rule = rule[:comment_pos].strip()
            if not rule:
                return None, "ä»…åŒ…å«æ³¨é‡Š"

        # æ£€æŸ¥æ˜¯å¦æ˜¯ v2ray æ ¼å¼
        if ':' not in rule:
            return None, "é v2ray æ ¼å¼"

        # åˆ†ç¦»å„éƒ¨åˆ†ï¼šprefix:domain[:tag...]
        parts = rule.split(':')
        if len(parts) < 2:
            return None, "æ— æ•ˆçš„ v2ray æ ¼å¼"

        prefix = parts[0].strip().lower()

        # éªŒè¯å‰ç¼€
        if prefix not in ['domain', 'full']:
            return None, f"ä¸æ”¯æŒçš„ v2ray å‰ç¼€: {prefix}"

        # æå–åŸŸåéƒ¨åˆ†
        domain_part = parts[1].strip()

        if not domain_part:
            return None, "åŸŸåéƒ¨åˆ†ä¸ºç©º"

        # å¤„ç†æ ‡ç­¾éƒ¨åˆ†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        tag_info = None
        if len(parts) > 2:
            tag_parts = parts[2:]
            # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡ç­¾
            tags = [part.strip() for part in tag_parts if part.strip()]
            if tags:
                tag_info = ':'.join(tags)
                self.stats['v2ray_with_tags'] += 1
                # æ˜¾ç¤ºæ ‡ç­¾ä¿¡æ¯ç”¨äºè°ƒè¯•
                print(f"    ğŸ“ v2ray å¸¦æ ‡ç­¾: {original_rule} -> åŸŸå: {domain_part}, æ ‡ç­¾: {tag_info}")

        # éªŒè¯å’Œæ¸…ç†åŸŸåï¼ˆä¿æŒåŸå§‹ç»“æ„ï¼‰
        cleaned_domain = self._clean_v2ray_domain(domain_part)
        if not cleaned_domain:
            return None, "æ— æ•ˆåŸŸå"

        return cleaned_domain, None

    def _clean_v2ray_domain(self, domain: str) -> str:
        """
        ä¸“é—¨ä¸º v2ray åŸŸåæ¸…ç†çš„æ–¹æ³•ï¼Œä¿æŒåŸå§‹ç»“æ„

        Args:
            domain: v2ray åŸŸåéƒ¨åˆ†

        Returns:
            æ¸…ç†åçš„åŸŸåï¼ˆä¿æŒåŸå§‹ç»“æ„ï¼‰
        """
        if not domain:
            return None

        # ç§»é™¤åè®®ï¼ˆå¦‚æœæ„å¤–åŒ…å«ï¼‰
        if domain.startswith(('http://', 'https://')):
            parsed = urlparse(domain)
            domain = parsed.netloc
            if parsed.port:
                # å¦‚æœURLä¸­æœ‰ç«¯å£ï¼Œç§»é™¤å®ƒ
                domain = domain.replace(f':{parsed.port}', '')

        # å¯¹äº v2ray æ ¼å¼ï¼Œé€šå¸¸ä¸åº”è¯¥æœ‰ç«¯å£å·
        # ä½†å¦‚æœæœ‰æ˜æ˜¾çš„æ•°å­—ç«¯å£åˆ™ç§»é™¤
        if ':' in domain:
            parts = domain.split(':')
            if len(parts) == 2 and parts[1].isdigit() and int(parts[1]) <= 65535:
                # åªæœ‰å½“ç¬¬äºŒéƒ¨åˆ†æ˜¯æœ‰æ•ˆç«¯å£å·æ—¶æ‰ç§»é™¤
                domain = parts[0]
                print(f"    ğŸ”§ ç§»é™¤ç«¯å£å·: {':'.join(parts)} -> {domain}")

        # ç§»é™¤è·¯å¾„ï¼ˆå¦‚æœæ„å¤–åŒ…å«ï¼‰
        if '/' in domain:
            domain = domain.split('/')[0]

        # **ä¸ç§»é™¤ www. å‰ç¼€ - ä¿æŒåŸå§‹ç»“æ„**
        # è¿™é‡Œæ³¨é‡Šæ‰åŸæ¥çš„ä»£ç ï¼š
        # if domain.startswith('www.'):
        #     domain = domain[4:]

        # åªç§»é™¤æ˜æ˜¾æ— å…³çš„å­—ç¬¦ï¼Œä¿ç•™åŸŸåçš„å®Œæ•´æ€§
        # ä¸ä½¿ç”¨æ¿€è¿›çš„å­—ç¬¦è¿‡æ»¤ï¼Œåªç§»é™¤æ˜æ˜¾çš„ç©ºç™½å­—ç¬¦
        domain = domain.strip()

        # æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€
        if self.config["parsing"]["ignore_ip"] and self.is_ip_address(domain):
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯localhost
        if self.config["parsing"]["ignore_localhost"] and domain in ['localhost', '127.0.0.1', '0.0.0.0']:
            return None

        # éªŒè¯åŸŸåæ ¼å¼
        if self.is_valid_domain(domain):
            return domain.lower()

        return None

    def load_auto_classify_rules(self) -> None:
        """
        åŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™é…ç½®
        """
        if not self.config.get("auto_classify", {}).get("enabled", False):
            print("ğŸ”„ è‡ªåŠ¨åˆ†ç±»åŠŸèƒ½å·²ç¦ç”¨")
            return

        print("ğŸ”„ æ­£åœ¨åŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™...")
        auto_classify_config = self.config["auto_classify"]

        # åŠ è½½ç›´æ¥é…ç½®çš„è§„åˆ™
        rules = auto_classify_config.get("rules", [])
        for rule in rules:
            parsed_rule = self._parse_auto_classify_rule(rule)
            if parsed_rule:
                self.auto_classify_rules.append(parsed_rule)

        if rules:
            print(f"  âœ… åŠ è½½äº† {len(rules)} ä¸ªå†…ç½®è‡ªåŠ¨åˆ†ç±»è§„åˆ™")

        # ä»å¤–éƒ¨æºåŠ è½½è§„åˆ™
        sources = auto_classify_config.get("sources", [])
        for source in sources:
            if not source.get("enabled", True):
                continue

            try:
                if "url" in source:
                    # ä»URLåŠ è½½
                    print(f"  ğŸŒ æ­£åœ¨ä»URLåŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {source['name']}")
                    rules_from_url = self._load_auto_classify_from_url(source)
                    self.auto_classify_rules.extend(rules_from_url)
                elif "file" in source:
                    # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
                    print(f"  ğŸ“ æ­£åœ¨ä»æ–‡ä»¶åŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {source['name']}")
                    rules_from_file = self._load_auto_classify_from_file(source)
                    self.auto_classify_rules.extend(rules_from_file)
            except Exception as e:
                print(f"  âŒ åŠ è½½è‡ªåŠ¨åˆ†ç±»æº '{source['name']}' å¤±è´¥: {e}")

        total_rules = len(self.auto_classify_rules)
        print(f"ğŸ”„ è‡ªåŠ¨åˆ†ç±»è§„åˆ™åŠ è½½å®Œæˆ: {total_rules} ä¸ªè§„åˆ™")

        # æ˜¾ç¤ºè§„åˆ™ç»Ÿè®¡
        if total_rules > 0:
            stats = defaultdict(int)
            for rule in self.auto_classify_rules:
                stats[rule['action']] += 1

            print(f"  ğŸ“Š è§„åˆ™åˆ†å¸ƒ:")
            for action, count in stats.items():
                print(f"    - {action}: {count} ä¸ª")

    def _parse_auto_classify_rule(self, rule_str: str) -> Dict:
        """
        è§£æè‡ªåŠ¨åˆ†ç±»è§„åˆ™

        Args:
            rule_str: è§„åˆ™å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "action:domain" æˆ– "replace:old=new"

        Returns:
            è§£æåçš„è§„åˆ™å­—å…¸
        """
        rule_str = rule_str.strip()
        if not rule_str or rule_str.startswith('#'):
            return None

        if ':' not in rule_str:
            return None

        action, content = rule_str.split(':', 1)
        action = action.strip().lower()
        content = content.strip()

        if not content:
            return None

        # å¤„ç†æ›¿æ¢è§„åˆ™
        if action == 'replace':
            if '=' in content:
                old_domain, new_domain = content.split('=', 1)
                return {
                    'action': 'replace',
                    'old_domain': old_domain.strip(),
                    'new_domain': new_domain.strip()
                }
            else:
                print(f"  âŒ æ— æ•ˆçš„æ›¿æ¢è§„åˆ™æ ¼å¼: {rule_str} (åº”ä¸º replace:old=new)")
                return None

        # å¤„ç†å…¶ä»–åŠ¨ä½œ
        elif action in ['remove', 'low_priority', 'high_priority', 'skip']:
            return {
                'action': action,
                'domain': content
            }
        else:
            print(f"  âŒ æœªçŸ¥çš„åŠ¨ä½œç±»å‹: {action}")
            return None

    def _load_auto_classify_from_url(self, source: dict) -> List[Dict]:
        """
        ä»URLåŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™

        Args:
            source: è§„åˆ™æºé…ç½®

        Returns:
            è§„åˆ™åˆ—è¡¨
        """
        url = source["url"]
        timeout = self.config["request_config"]["timeout"]
        retry_count = self.config["request_config"]["retry_count"]
        retry_delay = self.config["request_config"]["retry_delay"]

        for attempt in range(retry_count):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                response = requests.get(url, timeout=timeout, headers=headers)
                response.raise_for_status()

                return self._parse_auto_classify_content(response.text)

            except requests.RequestException as e:
                print(f"    âŒ è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}")
                if attempt < retry_count - 1:
                    time.sleep(retry_delay)

        return []

    def _load_auto_classify_from_file(self, source: dict) -> List[Dict]:
        """
        ä»æœ¬åœ°æ–‡ä»¶åŠ è½½è‡ªåŠ¨åˆ†ç±»è§„åˆ™

        Args:
            source: è§„åˆ™æºé…ç½®

        Returns:
            è§„åˆ™åˆ—è¡¨
        """
        file_path = source["file"]
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return self._parse_auto_classify_content(content)
        except FileNotFoundError:
            print(f"    âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        except Exception as e:
            print(f"    âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

        return []

    def _parse_auto_classify_content(self, content: str) -> List[Dict]:
        """
        è§£æè‡ªåŠ¨åˆ†ç±»è§„åˆ™å†…å®¹

        Args:
            content: æ–‡ä»¶å†…å®¹

        Returns:
            è§„åˆ™åˆ—è¡¨
        """
        rules = []
        for line in content.strip().split('\n'):
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            parsed_rule = self._parse_auto_classify_rule(line)
            if parsed_rule:
                rules.append(parsed_rule)

        return rules

    def load_custom_rules_from_file(self, file_path: str, format_type: str, action: str, csv_config: Dict = None) -> Tuple[Set[str], Dict[str, str], Dict]:
        """
        ä»æ–‡ä»¶åŠ è½½è‡ªå®šä¹‰è§„åˆ™ï¼Œæ”¯æŒ CSV æ ¼å¼

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            format_type: æ ¼å¼ç±»å‹ (domain, regex, ublock, v2ray, replace, csv)
            action: åŠ¨ä½œç±»å‹ (remove, low_priority, high_priority, replace)
            csv_config: CSV é…ç½®ï¼ˆå½“ format_type ä¸º csv æ—¶ä½¿ç”¨ï¼‰

        Returns:
            (åŸŸåé›†åˆ, æ›¿æ¢è§„åˆ™å­—å…¸, ç»Ÿè®¡ä¿¡æ¯)
        """
        domains = set()
        replace_rules = {}
        stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'invalid_domains': 0,
            'ignored_comments': 0,
            'v2ray_with_tags': 0,
            'csv_parsed_rows': 0,
            'csv_invalid_urls': 0,
            'csv_extracted_domains': 0
        }

        try:
            # CSV æ ¼å¼ç‰¹æ®Šå¤„ç†
            if format_type == "csv":
                if not csv_config:
                    print(f"  âŒ CSV æ ¼å¼éœ€è¦ csv_config é…ç½®")
                    return domains, replace_rules, stats

                print(f"  ğŸ“ æ­£åœ¨è§£æ CSV æ–‡ä»¶: {file_path}")
                return self.load_csv_rules_from_file(file_path, csv_config, action)

            # å…¶ä»–æ ¼å¼çš„å¤„ç†é€»è¾‘ä¿æŒä¸å˜
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            print(f"  ğŸ“ æ­£åœ¨è§£ææ–‡ä»¶: {file_path} (æ ¼å¼: {format_type})")

            for line_num, line in enumerate(content.strip().split('\n'), 1):
                line = line.strip()
                if not line:
                    continue

                stats['total_rules'] += 1

                # è·³è¿‡æ³¨é‡Š
                if line.startswith('#'):
                    stats['ignored_comments'] += 1
                    continue

                try:
                    if format_type == "replace":
                        # æ›¿æ¢è§„åˆ™æ ¼å¼ï¼šold_domain=new_domain æˆ– old_regex=new_domain
                        if '=' in line:
                            old_pattern, new_domain = line.split('=', 1)
                            old_pattern = old_pattern.strip()
                            new_domain = new_domain.strip()

                            if old_pattern and new_domain:
                                replace_rules[old_pattern] = new_domain
                                stats['parsed_domains'] += 1
                            else:
                                stats['invalid_domains'] += 1
                        else:
                            stats['invalid_domains'] += 1

                    elif format_type == "regex":
                        # æ­£åˆ™è¡¨è¾¾å¼æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                        if line:
                            domains.add(line)
                            stats['parsed_domains'] += 1

                    elif format_type == "ublock":
                        # uBlock æ ¼å¼
                        domain, ignore_reason, is_path_rule = self.parse_ublock_rule(line)
                        if domain:
                            domains.add(domain)
                            stats['parsed_domains'] += 1
                        else:
                            stats['invalid_domains'] += 1

                    elif format_type == "v2ray":
                        # v2ray æ ¼å¼
                        domain, ignore_reason = self.parse_v2ray_rule(line)
                        if domain:
                            domains.add(domain)
                            stats['parsed_domains'] += 1
                            if stats['parsed_domains'] <= 3:  # æ˜¾ç¤ºå‰3ä¸ªè§£ææ ·æœ¬
                                print(f"    âœ… v2ray è§£æ: {line} -> {domain}")
                        else:
                            stats['invalid_domains'] += 1
                            if ignore_reason and stats['invalid_domains'] <= 3:
                                print(f"    âŒ v2ray å¿½ç•¥: {line} ({ignore_reason})")

                    else:  # domain æ ¼å¼
                        # å¤„ç†è¡Œæœ«æ³¨é‡Š
                        if '#' in line:
                            line = line[:line.find('#')].strip()
                            if not line:
                                stats['ignored_comments'] += 1
                                continue

                        domain = self.clean_domain(line)
                        if domain:
                            domains.add(domain)
                            stats['parsed_domains'] += 1
                        else:
                            stats['invalid_domains'] += 1

                except Exception as e:
                    print(f"    âŒ è§£æç¬¬ {line_num} è¡Œæ—¶å‡ºé”™: {line[:50]}... - {e}")
                    stats['invalid_domains'] += 1

            # ç´¯åŠ  v2ray æ ‡ç­¾ç»Ÿè®¡
            stats['v2ray_with_tags'] = self.stats.get('v2ray_with_tags', 0)

            print(f"    âœ… è§£æå®Œæˆ: {stats['parsed_domains']} ä¸ªæœ‰æ•ˆè§„åˆ™")
            if format_type == "v2ray" and stats['v2ray_with_tags'] > 0:
                print(f"    ğŸ“ å…¶ä¸­åŒ…å«æ ‡ç­¾çš„è§„åˆ™: {stats['v2ray_with_tags']} ä¸ª")
            if stats['invalid_domains'] > 0:
                print(f"    âš ï¸  å¿½ç•¥äº† {stats['invalid_domains']} ä¸ªæ— æ•ˆè§„åˆ™")

        except FileNotFoundError:
            print(f"    âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        except Exception as e:
            print(f"    âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

        return domains, replace_rules, stats

    def should_skip_domain_from_source(self, domain: str, source_name: str = None) -> Tuple[bool, str]:
        """
        æ£€æŸ¥åŸŸåæ˜¯å¦åº”è¯¥ä»æ•°æ®æºå¤„ç†ä¸­è·³è¿‡
        æ³¨æ„ï¼šè¿™åªå½±å“æ•°æ®æºçš„é»˜è®¤å¤„ç†ï¼Œä¸å½±å“æ˜ç¡®çš„è‡ªåŠ¨åˆ†ç±»è§„åˆ™

        Args:
            domain: è¦æ£€æŸ¥çš„åŸŸå
            source_name: æ•°æ®æºåç§°

        Returns:
            (æ˜¯å¦è·³è¿‡, è·³è¿‡åŸå› )
        """
        if not self.auto_classify_rules:
            return False, ""

        domain_lower = domain.lower()

        for rule in self.auto_classify_rules:
            if rule['action'] == 'skip':
                rule_domain = rule['domain'].lower()

                # æ”¯æŒé€šé…ç¬¦åŒ¹é…
                if rule_domain.startswith('*.'):
                    # é€šé…ç¬¦åŒ¹é…å­åŸŸå
                    pattern_domain = rule_domain[2:]  # ç§»é™¤ *.
                    if domain_lower == pattern_domain or domain_lower.endswith('.' + pattern_domain):
                        return True, f"è‡ªåŠ¨åˆ†ç±»è·³è¿‡è§„åˆ™: {rule['domain']} (ä»…å½±å“æ•°æ®æºå¤„ç†)"
                elif domain_lower == rule_domain:
                    # ç²¾ç¡®åŒ¹é…
                    return True, f"è‡ªåŠ¨åˆ†ç±»è·³è¿‡è§„åˆ™: {rule['domain']} (ä»…å½±å“æ•°æ®æºå¤„ç†)"

        return False, ""

    def get_auto_classify_action(self, domain: str) -> Tuple[str, str]:
        """
        æ ¹æ®è‡ªåŠ¨åˆ†ç±»è§„åˆ™è·å–åŸŸåçš„åŠ¨ä½œ

        Args:
            domain: åŸŸå

        Returns:
            (åŠ¨ä½œç±»å‹, åŒ¹é…åŸå› )
        """
        if not self.auto_classify_rules:
            return None, ""

        domain_lower = domain.lower()

        for rule in self.auto_classify_rules:
            if rule['action'] in ['remove', 'low_priority', 'high_priority']:
                rule_domain = rule['domain'].lower()

                # æ”¯æŒé€šé…ç¬¦åŒ¹é…
                if rule_domain.startswith('*.'):
                    # é€šé…ç¬¦åŒ¹é…å­åŸŸå
                    pattern_domain = rule_domain[2:]  # ç§»é™¤ *.
                    if domain_lower == pattern_domain or domain_lower.endswith('.' + pattern_domain):
                        return rule['action'], f"è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {rule['domain']}"
                elif domain_lower == rule_domain:
                    # ç²¾ç¡®åŒ¹é…
                    return rule['action'], f"è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {rule['domain']}"

        return None, ""

    def get_all_auto_classify_actions_for_domain(self, domain: str) -> List[Tuple[str, str]]:
        """
        è·å–åŸŸåçš„æ‰€æœ‰è‡ªåŠ¨åˆ†ç±»åŠ¨ä½œï¼ˆç”¨äºæ£€æµ‹å†²çªå’Œä¼˜å…ˆçº§å¤„ç†ï¼‰

        Args:
            domain: åŸŸå

        Returns:
            (åŠ¨ä½œç±»å‹, åŒ¹é…åŸå› ) çš„åˆ—è¡¨
        """
        if not self.auto_classify_rules:
            return []

        domain_lower = domain.lower()
        actions = []

        for rule in self.auto_classify_rules:
            if rule['action'] in ['remove', 'low_priority', 'high_priority', 'skip']:
                rule_domain = rule['domain'].lower()

                # æ”¯æŒé€šé…ç¬¦åŒ¹é…
                if rule_domain.startswith('*.'):
                    # é€šé…ç¬¦åŒ¹é…å­åŸŸå
                    pattern_domain = rule_domain[2:]  # ç§»é™¤ *.
                    if domain_lower == pattern_domain or domain_lower.endswith('.' + pattern_domain):
                        actions.append((rule['action'], f"è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {rule['domain']}"))
                elif domain_lower == rule_domain:
                    # ç²¾ç¡®åŒ¹é…
                    actions.append((rule['action'], f"è‡ªåŠ¨åˆ†ç±»è§„åˆ™: {rule['domain']}"))

        return actions

    def apply_auto_classify_rules_directly(self, categorized_domains: Dict[str, Set[str]]) -> None:
        """
        ç›´æ¥åº”ç”¨è‡ªåŠ¨åˆ†ç±»è§„åˆ™ä¸­çš„åŸŸåï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
        ä¸ä»…é‡æ–°åˆ†ç±»ç°æœ‰åŸŸåï¼Œè¿˜ä¼šä¸»åŠ¨æ·»åŠ è§„åˆ™ä¸­å®šä¹‰çš„åŸŸå
        ä¿®å¤ç‰ˆæœ¬ï¼šskip è§„åˆ™ä¸ä¼šé˜»æ­¢å…¶ä»–æ˜ç¡®çš„è‡ªåŠ¨åˆ†ç±»è§„åˆ™

        Args:
            categorized_domains: åˆ†ç±»åçš„åŸŸåå­—å…¸
        """
        if not self.auto_classify_rules:
            return

        print(f"\nğŸ”„ æ­£åœ¨åº”ç”¨è‡ªåŠ¨åˆ†ç±»è§„åˆ™ä¸­çš„åŸŸå...")

        auto_added_count = 0
        auto_added_samples = []
        skip_overridden_count = 0
        skip_overridden_samples = []

        # æ”¶é›†æ‰€æœ‰å·²å­˜åœ¨çš„åŸŸåï¼ˆç”¨äºè·³è¿‡é‡å¤ï¼‰
        all_existing_domains = set()
        for domain_set in categorized_domains.values():
            all_existing_domains.update(d.lower() for d in domain_set)

        # æŒ‰åŸŸååˆ†ç»„å¤„ç†æ‰€æœ‰è§„åˆ™ï¼Œä»¥ä¾¿å¤„ç†å†²çª
        domain_rules_map = defaultdict(list)

        for rule in self.auto_classify_rules:
            if rule['action'] in ['remove', 'low_priority', 'high_priority', 'skip']:
                domain = rule['domain']

                # å¤„ç†é€šé…ç¬¦åŸŸå
                if domain.startswith('*.'):
                    # å¯¹äºé€šé…ç¬¦è§„åˆ™ï¼Œæˆ‘ä»¬ä¸ç›´æ¥æ·»åŠ ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŒ¹é…è§„åˆ™è€Œä¸æ˜¯å…·ä½“åŸŸå
                    continue

                # æ¸…ç†åŸŸåï¼ˆä½¿ç”¨ä¿æŒåŸå§‹ç»“æ„çš„æ¸…ç†æ–¹æ³•ï¼‰
                cleaned_domain = self.clean_domain_preserve_structure(domain)
                if not cleaned_domain:
                    continue

                domain_rules_map[cleaned_domain].append(rule)

            elif rule['action'] == 'replace':
                # å¤„ç†æ›¿æ¢è§„åˆ™
                old_domain = rule['old_domain']
                new_domain = rule['new_domain']

                cleaned_old_domain = self.clean_domain_preserve_structure(old_domain)
                if cleaned_old_domain and new_domain:
                    # ç”Ÿæˆæ­£åˆ™è¡¨è¾¾å¼æ ¼å¼çš„é”®
                    old_regex = f"(.*\\.)?{re.escape(cleaned_old_domain)}$"
                    self.config["replace_rules"][old_regex] = new_domain
                    auto_added_count += 1

                    if len(auto_added_samples) < 5:
                        auto_added_samples.append(f"{cleaned_old_domain} -> {new_domain} (æ›¿æ¢)")

        # å¤„ç†æ¯ä¸ªåŸŸåçš„è§„åˆ™
        for domain, rules in domain_rules_map.items():
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if domain.lower() in all_existing_domains:
                continue

            # åˆ†æè§„åˆ™ä¼˜å…ˆçº§
            has_skip = any(r['action'] == 'skip' for r in rules)
            non_skip_rules = [r for r in rules if r['action'] != 'skip']

            if non_skip_rules:
                # æœ‰é skip çš„è§„åˆ™ï¼Œä¼˜å…ˆå¤„ç†è¿™äº›è§„åˆ™
                # å¦‚æœæœ‰å¤šä¸ªé skip è§„åˆ™ï¼Œå–æœ€åä¸€ä¸ªï¼ˆæˆ–è€…å¯ä»¥æ ¹æ®ä¼˜å…ˆçº§æ’åºï¼‰
                effective_rule = non_skip_rules[-1]  # å–æœ€åä¸€ä¸ªè§„åˆ™

                action = effective_rule['action']
                if action in categorized_domains:
                    categorized_domains[action].add(domain)
                    all_existing_domains.add(domain.lower())
                    auto_added_count += 1

                    if len(auto_added_samples) < 5:
                        auto_added_samples.append(f"{domain} -> {action}")

                    # å¦‚æœåŒæ—¶æœ‰ skip è§„åˆ™ï¼Œè®°å½•è¦†ç›–æƒ…å†µ
                    if has_skip:
                        skip_overridden_count += 1
                        if len(skip_overridden_samples) < 3:
                            skip_overridden_samples.append(f"{domain} (skip è¢« {action} è¦†ç›–)")

            # å¦‚æœåªæœ‰ skip è§„åˆ™ï¼Œä¸åšä»»ä½•å¤„ç†ï¼ˆä½†ä¸æ˜¯é”™è¯¯ï¼‰

        if auto_added_count > 0:
            print(f"  âœ… ä¸»åŠ¨æ·»åŠ äº† {auto_added_count} ä¸ªåŸŸååˆ°ç›¸åº”ç±»åˆ«")
            self.stats['auto_added'] = auto_added_count

            print(f"  ğŸ“ æ·»åŠ çš„åŸŸåæ ·æœ¬:")
            for sample in auto_added_samples:
                print(f"    + {sample}")

        if skip_overridden_count > 0:
            print(f"  ğŸ”„ skip è§„åˆ™è¢«è¦†ç›–: {skip_overridden_count} ä¸ªåŸŸå")
            self.stats['skip_overridden'] = skip_overridden_count

            print(f"  ğŸ“ è¦†ç›–æƒ…å†µæ ·æœ¬:")
            for sample in skip_overridden_samples:
                print(f"    âš¡ {sample}")

        if auto_added_count == 0 and skip_overridden_count == 0:
            print(f"  â„¹ï¸  æ²¡æœ‰éœ€è¦ä¸»åŠ¨æ·»åŠ çš„åŸŸå")

    def clean_domain_preserve_structure(self, domain: str) -> str:
        """
        æ¸…ç†åŸŸåä½†ä¿æŒåŸå§‹ç»“æ„ï¼ˆç”¨äºè‡ªåŠ¨åˆ†ç±»è§„åˆ™ï¼‰

        Args:
            domain: åŸå§‹åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ¸…ç†åçš„åŸŸåï¼ˆä¿æŒç»“æ„ï¼‰
        """
        if not domain:
            return None

        # ç§»é™¤åè®®
        if domain.startswith(('http://', 'https://')):
            parsed = urlparse(domain)
            domain = parsed.netloc

        # ç§»é™¤æ˜æ˜¾çš„ç«¯å£å·
        if ':' in domain:
            parts = domain.split(':')
            if len(parts) == 2 and parts[1].isdigit():
                domain = parts[0]

        # ç§»é™¤è·¯å¾„
        if '/' in domain:
            domain = domain.split('/')[0]

        # **ä¿æŒ www. å‰ç¼€**
        # ä¸åšä»»ä½•å‰ç¼€ç§»é™¤

        # åªç§»é™¤ç©ºç™½å­—ç¬¦
        domain = domain.strip()

        # æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€
        if self.config["parsing"]["ignore_ip"] and self.is_ip_address(domain):
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯localhost
        if self.config["parsing"]["ignore_localhost"] and domain in ['localhost', '127.0.0.1', '0.0.0.0']:
            return None

        # éªŒè¯åŸŸåæ ¼å¼
        if self.is_valid_domain(domain):
            return domain.lower()

        return None

    def _has_specific_path(self, url_string: str) -> bool:
        """
        ç®€åŒ–ç‰ˆï¼šæ£€æŸ¥URLæ˜¯å¦åŒ…å«å…·ä½“çš„è·¯å¾„ï¼ˆéåŸŸåçº§åˆ«ï¼‰
        åªè¦æœ‰è·¯å¾„éƒ¨åˆ†ï¼ˆä¸ä¸ºç©ºä¸”ä¸æ˜¯å•ç‹¬çš„'*'ï¼‰ï¼Œå°±è®¤ä¸ºæ˜¯ç‰¹å®šè·¯å¾„

        Args:
            url_string: URLå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦åŒ…å«å…·ä½“è·¯å¾„
        """
        # ç§»é™¤åè®®éƒ¨åˆ†
        if url_string.startswith('*://'):
            url_part = url_string[4:]
        elif url_string.startswith('||'):
            url_part = url_string[2:].rstrip('^')
        else:
            url_part = url_string

        # æ£€æŸ¥æ˜¯å¦æœ‰è·¯å¾„éƒ¨åˆ†
        if '/' in url_part:
            domain_and_path = url_part.split('/', 1)
            if len(domain_and_path) > 1:
                path_part = domain_and_path[1]

                # ç®€åŒ–é€»è¾‘ï¼šåªè¦è·¯å¾„éƒ¨åˆ†ä¸ä¸ºç©ºä¸”ä¸æ˜¯å•ç‹¬çš„'*'ï¼Œå°±è®¤ä¸ºæ˜¯ç‰¹å®šè·¯å¾„
                if path_part and path_part != '*':
                    return True

        return False

    def extract_domain_from_rule(self, rule: str) -> str:
        """
        ğŸ”§ ä¿®å¤ï¼šä»è§„åˆ™ä¸­æå–åŸŸåï¼Œæ”¯æŒæ›´å¤šæ ¼å¼

        Args:
            rule: è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            åŸŸåæˆ– None
        """
        rule = rule.strip()

        # ğŸ”„ ä¿®å¤ï¼šå¯¹äºç‰¹å®šè·¯å¾„è§„åˆ™ï¼Œä»ç„¶å°è¯•æå–åŸŸå
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“è·¯å¾„
        has_specific_path = self._has_specific_path(rule)

        # ğŸ”§ æ–°å¢ï¼šæ”¯æŒæ›´å¤šçš„uBlockè§„åˆ™æ ¼å¼
        patterns = [
            # ğŸ”§ æ–°å¢ï¼š*.domain.com/* æ ¼å¼ (é€šé…ç¬¦åŸŸå)
            r'^\*\.([a-zA-Z0-9.-]+)(?:/.*)?(?:\*)?$',
            # ğŸ”§ æ–°å¢ï¼š*.domain.com/path/* æ ¼å¼
            r'^\*\.([a-zA-Z0-9.-]+)/.*(?:\*)?$',
            # åŸæœ‰ï¼š*://*.domain.com/* æˆ– *://*.domain.com (é€šé…ç¬¦å­åŸŸå)
            r'^\*://\*\.([a-zA-Z0-9.-]+)(?:/.*)?$',
            # åŸæœ‰ï¼š*://domain.com/* æˆ– *://domain.com (æ— é€šé…ç¬¦)
            r'^\*://([a-zA-Z0-9.-]+)(?:/.*)?$',
            # ğŸ”§ æ–°å¢ï¼šhttps://domain.com/* æ ¼å¼
            r'^https?://([a-zA-Z0-9.-]+)(?:/.*)?$',
            # åŸæœ‰ï¼š||domain.com^ æˆ– ||domain.com/path
            r'^\|\|([a-zA-Z0-9.-]+)(?:/.*)?(?:\^)?$',
            # ğŸ”§ æ–°å¢ï¼šdomain.com/* æ ¼å¼
            r'^([a-zA-Z0-9.-]+)/.*(?:\*)?$',
            # åŸæœ‰ï¼šæ™®é€šåŸŸåæ ¼å¼
            r'^([a-zA-Z0-9.-]+)(?:/.*)?$',
            # ğŸ”§ ä¿®å¤ï¼šdomain.com* æ ¼å¼ï¼ˆä¸å¸¦æ–œæ çš„é€šé…ç¬¦ï¼‰
            r'^([a-zA-Z0-9.-]+)\*$',
        ]

        for pattern in patterns:
            match = re.match(pattern, rule)
            if match:
                candidate = match.group(1)
                # éªŒè¯æå–çš„å€™é€‰åŸŸå
                if candidate and '.' in candidate and not candidate.startswith('/'):
                    # ğŸ”§ è¿›ä¸€æ­¥éªŒè¯åŸŸåæ ¼å¼
                    if self.is_valid_domain(candidate):
                        return candidate
                    else:
                        # å¦‚æœåŸŸåéªŒè¯å¤±è´¥ï¼Œæ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                        debug_count = getattr(self, '_debug_extract_count', 0)
                        if debug_count < 3:
                            print(f"  ğŸ”§ åŸŸåæ ¼å¼éªŒè¯å¤±è´¥: {rule} -> {candidate}")
                            self._debug_extract_count = debug_count + 1

        # ğŸ”„ å¯¹äº *://*/filename è¿™ç§æ ¼å¼ï¼Œæˆ‘ä»¬æ— æ³•æå–æœ‰æ•ˆåŸŸåï¼Œè¿”å› None
        if rule.startswith('*://*/'):
            return None

        # ğŸ”§ å¢å¼ºçš„é€šç”¨åŸŸåæå–ï¼ˆæœ€åçš„åå¤‡æ–¹æ¡ˆï¼‰
        # å°è¯•æå–æ‰€æœ‰å¯èƒ½çš„åŸŸåæ ¼å¼
        domain_candidates = re.findall(r'([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', rule)
        for candidate in domain_candidates:
            if self.is_valid_domain(candidate):
                return candidate

        return None

    def parse_ublock_rule(self, rule: str) -> Tuple[str, str, bool]:
        """
        ğŸ› ä¿®å¤ï¼šè§£æ uBlock Origin è¯­æ³•è§„åˆ™ï¼Œæå–åŸŸåï¼Œæ­£ç¡®è¯†åˆ«ç‰¹å®šè·¯å¾„è§„åˆ™

        Args:
            rule: uBlock è§„åˆ™å­—ç¬¦ä¸²

        Returns:
            (åŸŸåæˆ– None, å¿½ç•¥åŸå› , æ˜¯å¦æ˜¯ç‰¹å®šè·¯å¾„è§„åˆ™)
        """
        original_rule = rule  # ä¿å­˜åŸå§‹è§„åˆ™ç”¨äºè°ƒè¯•
        rule = rule.strip()
        if not rule or rule.startswith('!') or rule.startswith('#'):
            return None, "æ³¨é‡Šæˆ–ç©ºè¡Œ", False

        # å¤„ç†è¡Œæœ«æ³¨é‡Š - ç§»é™¤ # åé¢çš„æ‰€æœ‰å†…å®¹
        if '#' in rule:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª # çš„ä½ç½®ï¼Œç§»é™¤å®ƒåŠåé¢çš„å†…å®¹
            comment_pos = rule.find('#')
            rule = rule[:comment_pos].strip()

            # å¦‚æœç§»é™¤æ³¨é‡Šåè§„åˆ™ä¸ºç©ºï¼Œåˆ™å¿½ç•¥
            if not rule:
                return None, "ä»…åŒ…å«æ³¨é‡Š", False

        # ğŸ› ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å®šè·¯å¾„è§„åˆ™
        has_specific_path = self._has_specific_path(rule)

        # æå–åŸŸå
        domain = self.extract_domain_from_rule(rule)

        if domain:
            cleaned_domain = self.clean_domain(domain)
            if cleaned_domain:
                # ğŸ”§ æ˜¾ç¤ºè§£ææˆåŠŸçš„æ ·æœ¬ï¼ˆåŒ…æ‹¬é€šé…ç¬¦è§„åˆ™ï¼‰
                debug_count = getattr(self, '_debug_success_count', 0)
                if debug_count < 5:
                    if has_specific_path:
                        print(f"  âœ… ç‰¹å®šè·¯å¾„è§„åˆ™è§£æ: {original_rule} -> åŸŸå: {cleaned_domain}")
                    elif original_rule.startswith('*.'):
                        print(f"  ğŸ”§ é€šé…ç¬¦è§„åˆ™è§£æ: {original_rule} -> åŸŸå: {cleaned_domain}")
                        self.stats['wildcard_rules_processed'] += 1
                    else:
                        print(f"  âœ… æ™®é€šè§„åˆ™è§£æ: {original_rule} -> åŸŸå: {cleaned_domain}")
                    self._debug_success_count = debug_count + 1

                return cleaned_domain, None, has_specific_path
            else:
                return None, "åŸŸåæ¸…ç†åæ— æ•ˆ", has_specific_path
        else:
            # ğŸ”„ å¯¹äºæ— æ³•æå–åŸŸåçš„ç‰¹å®šè·¯å¾„è§„åˆ™ï¼Œä¹Ÿè¦æ ‡è®°ä¸ºç‰¹å®šè·¯å¾„
            if has_specific_path:
                return None, "ç‰¹å®šè·¯å¾„è§„åˆ™ä½†æ— æ³•æå–åŸŸå", True
            else:
                # æ˜¾ç¤ºä¸€äº›æ— æ³•è§£æçš„è§„åˆ™æ ·æœ¬
                debug_count = getattr(self, '_debug_fail_count', 0)
                if debug_count < 3:
                    print(f"  âŒ æ— æ³•è§£æè§„åˆ™: {original_rule}")
                    self._debug_fail_count = debug_count + 1
                return None, "æ— æ³•è§£æè§„åˆ™æ ¼å¼", False

    def determine_path_rule_action(self, source_action: str, specific_path_action: str) -> str:
        """
        ğŸ”§ æ–°å¢ï¼šç¡®å®šç‰¹å®šè·¯å¾„è§„åˆ™çš„æœ€ç»ˆåŠ¨ä½œ

        Args:
            source_action: æ•°æ®æºçš„åŸå§‹åŠ¨ä½œ
            specific_path_action: ç‰¹å®šè·¯å¾„è§„åˆ™çš„é…ç½®åŠ¨ä½œ

        Returns:
            æœ€ç»ˆçš„åŠ¨ä½œ
        """
        if specific_path_action == "keep_action":
            # ä¿æŒæºçš„åŸå§‹åŠ¨ä½œ
            return source_action
        elif specific_path_action == "low_priority":
            # å¼ºåˆ¶è®¾ä¸ºä½ä¼˜å…ˆçº§
            return "low_priority"
        elif specific_path_action == "smart":
            # æ™ºèƒ½å¤„ç†ï¼šremove->low_priorityï¼Œå…¶ä»–ä¿æŒ
            if source_action == "remove":
                return "low_priority"
            else:
                return source_action
        elif specific_path_action == "ignore":
            # å¿½ç•¥ï¼Œè¿”å› None
            return None
        else:
            # é»˜è®¤æƒ…å†µï¼Œä¿æŒåŸå§‹åŠ¨ä½œ
            return source_action

    def fetch_domain_list(self, url: str, format_type: str = "domain", source_name: str = None, csv_config: Dict = None) -> Tuple[Set[str], Set[str], Dict]:
        """
        ğŸ”§ ä¿®å¤ï¼šä»URLè·å–åŸŸååˆ—è¡¨ï¼Œæ­£ç¡®å¤„ç†ç‰¹å®šè·¯å¾„è§„åˆ™çš„åŠ¨ä½œåˆ†é…

        Args:
            url: åŸŸååˆ—è¡¨URL
            format_type: æ ¼å¼ç±»å‹ï¼Œ"domain", "ublock", "v2ray", æˆ– "csv"
            source_name: æ•°æ®æºåç§°ï¼ˆç”¨äºè‡ªåŠ¨åˆ†ç±»ï¼‰
            csv_config: CSV é…ç½®ï¼ˆå½“ format_type ä¸º csv æ—¶ä½¿ç”¨ï¼‰

        Returns:
            (æ™®é€šåŸŸåé›†åˆ, ç‰¹å®šè·¯å¾„åŸŸåé›†åˆ(å·²åˆ†ç±»), ç»Ÿè®¡ä¿¡æ¯)
        """
        domains = set()
        path_domains_classified = {}  # ğŸ”§ æ”¹ä¸ºå­—å…¸å­˜å‚¨åˆ†ç±»åçš„ç‰¹å®šè·¯å¾„åŸŸå
        stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'path_to_low_priority': 0,
            'path_kept_action': 0,      # ğŸ”§ æ–°å¢ç»Ÿè®¡
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0,
            'auto_classified': 0,  # è‡ªåŠ¨åˆ†ç±»å¤„ç†çš„æ•°é‡
            'skipped_domains': 0,   # è·³è¿‡çš„åŸŸåæ•°é‡
            'v2ray_with_tags': 0,   # v2ray å¸¦æ ‡ç­¾çš„è§„åˆ™æ•°é‡
            'csv_parsed_rows': 0,
            'csv_invalid_urls': 0,
            'csv_extracted_domains': 0,
            'wildcard_rules_processed': 0,  # ğŸ”§ å¤„ç†çš„é€šé…ç¬¦è§„åˆ™æ•°é‡
        }

        retry_count = self.config["request_config"]["retry_count"]
        timeout = self.config["request_config"]["timeout"]
        retry_delay = self.config["request_config"]["retry_delay"]

        for attempt in range(retry_count):
            try:
                print(f"æ­£åœ¨è·å– {url} (å°è¯• {attempt + 1}/{retry_count}) - æ ¼å¼: {format_type}")

                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }

                response = requests.get(url, timeout=timeout, headers=headers)
                response.raise_for_status()

                # CSV æ ¼å¼ç‰¹æ®Šå¤„ç†
                if format_type == "csv":
                    if not csv_config:
                        print(f"  âŒ CSV æ ¼å¼éœ€è¦ csv_config é…ç½®")
                        return domains, path_domains_classified, stats

                    return self._parse_csv_from_response(response.text, csv_config, source_name, stats)

                # è®°å½•ä¸€äº›è¢«å¿½ç•¥çš„è§„åˆ™ç”¨äºè°ƒè¯•
                ignored_samples = []
                accepted_samples = []
                comment_samples = []
                path_samples = []  # è·¯å¾„è§„åˆ™æ ·æœ¬
                skip_samples = []  # è·³è¿‡çš„åŸŸåæ ·æœ¬
                path_to_low_priority_samples = []  # ç‰¹å®šè·¯å¾„è½¬ä½ä¼˜å…ˆçº§æ ·æœ¬
                path_kept_action_samples = []      # ğŸ”§ ç‰¹å®šè·¯å¾„ä¿æŒåŠ¨ä½œæ ·æœ¬

                # é‡ç½® v2ray æ ‡ç­¾è®¡æ•°å™¨
                initial_v2ray_tags = self.stats.get('v2ray_with_tags', 0)

                # é‡ç½®è°ƒè¯•è®¡æ•°å™¨
                self._debug_path_count = 0
                self._debug_success_count = 0
                self._debug_fail_count = 0
                self._debug_extract_count = 0

                # ğŸ”§ è·å–æºåŠ¨ä½œå’Œç‰¹å®šè·¯å¾„å¤„ç†é…ç½®
                source_action = getattr(self, '_current_source_action', 'remove')  # ä¸´æ—¶å­˜å‚¨å½“å‰æºåŠ¨ä½œ
                specific_path_action = self.config["parsing"].get("specific_path_action", "keep_action")

                print(f"  ğŸ”§ ç‰¹å®šè·¯å¾„å¤„ç†æ¨¡å¼: {specific_path_action}")
                print(f"  ğŸ”§ æºåŠ¨ä½œ: {source_action}")

                # è§£æåŸŸå
                for line_num, line in enumerate(response.text.strip().split('\n'), 1):
                    line = line.strip()
                    if not line:
                        continue

                    stats['total_rules'] += 1

                    try:
                        if format_type == "ublock":
                            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ–°çš„ parse_ublock_rule æ–¹æ³•
                            domain, ignore_reason, is_path_rule = self.parse_ublock_rule(line)

                            if domain:
                                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä»æ•°æ®æºè·³è¿‡æ­¤åŸŸå
                                should_skip, skip_reason = self.should_skip_domain_from_source(domain, source_name)
                                if should_skip:
                                    stats['skipped_domains'] += 1
                                    if len(skip_samples) < 3:
                                        skip_samples.append(f"{line} -> {domain} ({skip_reason})")
                                else:
                                    # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®æ˜¯å¦æ˜¯ç‰¹å®šè·¯å¾„è§„åˆ™å’Œé…ç½®å†³å®šå¦‚ä½•å¤„ç†
                                    if is_path_rule:
                                        # ç¡®å®šç‰¹å®šè·¯å¾„è§„åˆ™çš„æœ€ç»ˆåŠ¨ä½œ
                                        final_action = self.determine_path_rule_action(source_action, specific_path_action)

                                        if final_action is None:
                                            # å¿½ç•¥è¿™ä¸ªåŸŸå
                                            stats['ignored_with_path'] += 1
                                            if len(path_samples) < 3:
                                                path_samples.append(f"{line} -> {domain} (å¿½ç•¥)")
                                        elif final_action == "low_priority":
                                            # åˆå§‹åŒ–åˆ†ç±»å­—å…¸
                                            if final_action not in path_domains_classified:
                                                path_domains_classified[final_action] = set()
                                            path_domains_classified[final_action].add(domain)
                                            stats['path_to_low_priority'] += 1
                                            if len(path_to_low_priority_samples) < 5:
                                                path_to_low_priority_samples.append(f"{line} -> {domain} (è·¯å¾„è§„åˆ™->ä½ä¼˜å…ˆçº§)")
                                        else:
                                            # ä¿æŒåŸåŠ¨ä½œæˆ–å…¶ä»–åŠ¨ä½œ
                                            if final_action not in path_domains_classified:
                                                path_domains_classified[final_action] = set()
                                            path_domains_classified[final_action].add(domain)
                                            stats['path_kept_action'] += 1
                                            if len(path_kept_action_samples) < 5:
                                                path_kept_action_samples.append(f"{line} -> {domain} (è·¯å¾„è§„åˆ™->{final_action})")
                                    else:
                                        # æ™®é€šåŸŸåè§„åˆ™
                                        if domain in domains:
                                            stats['duplicate_domains'] += 1
                                        else:
                                            domains.add(domain)
                                            stats['parsed_domains'] += 1
                                            if len(accepted_samples) < 3:
                                                accepted_samples.append(f"{line} -> {domain}")
                            else:
                                # ç»Ÿè®¡å¿½ç•¥åŸå› 
                                if "ç‰¹å®šè·¯å¾„" in (ignore_reason or ""):
                                    stats['ignored_with_path'] += 1
                                    if len(path_samples) < 3:
                                        path_samples.append(line)
                                elif ignore_reason in ["æ³¨é‡Šæˆ–ç©ºè¡Œ", "ä»…åŒ…å«æ³¨é‡Š"]:
                                    stats['ignored_comments'] += 1
                                    if len(comment_samples) < 3:
                                        comment_samples.append(line)
                                else:
                                    stats['invalid_domains'] += 1
                                    if len(ignored_samples) < 3:
                                        ignored_samples.append(line)

                        elif format_type == "v2ray":
                            # ä½¿ç”¨ v2ray è¯­æ³•è§£æ
                            domain, ignore_reason = self.parse_v2ray_rule(line)
                            if domain and len(accepted_samples) < 3:
                                accepted_samples.append(f"v2ray: {line} -> {domain}")
                            elif ignore_reason and len(ignored_samples) < 3:
                                ignored_samples.append(f"v2ray: {line} ({ignore_reason})")

                            if domain:
                                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä»æ•°æ®æºè·³è¿‡æ­¤åŸŸå
                                should_skip, skip_reason = self.should_skip_domain_from_source(domain, source_name)
                                if should_skip:
                                    stats['skipped_domains'] += 1
                                    if len(skip_samples) < 3:
                                        skip_samples.append(f"{line} -> {domain} ({skip_reason})")
                                else:
                                    if domain in domains:
                                        stats['duplicate_domains'] += 1
                                    else:
                                        domains.add(domain)
                                        stats['parsed_domains'] += 1
                            else:
                                # ç»Ÿè®¡å¿½ç•¥åŸå› 
                                if ignore_reason in ["æ³¨é‡Šæˆ–ç©ºè¡Œ", "ä»…åŒ…å«æ³¨é‡Š"]:
                                    stats['ignored_comments'] += 1
                                    if len(comment_samples) < 3:
                                        comment_samples.append(line)
                                elif ignore_reason == "æ— æ•ˆåŸŸå":
                                    stats['invalid_domains'] += 1
                                    if len(ignored_samples) < 3:
                                        ignored_samples.append(line)

                        else:
                            # æ™®é€šåŸŸåæ ¼å¼ - ä¹Ÿéœ€è¦å¤„ç†è¡Œæœ«æ³¨é‡Š
                            cleaned_line = line
                            if '#' in line:
                                cleaned_line = line[:line.find('#')].strip()
                                if not cleaned_line:
                                    stats['ignored_comments'] += 1
                                    if len(comment_samples) < 3:
                                        comment_samples.append(line)
                                    continue

                            domain = self.clean_domain(self.extract_domain_from_rule(cleaned_line))
                            if domain:
                                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä»æ•°æ®æºè·³è¿‡æ­¤åŸŸå
                                should_skip, skip_reason = self.should_skip_domain_from_source(domain, source_name)
                                if should_skip:
                                    stats['skipped_domains'] += 1
                                    if len(skip_samples) < 3:
                                        skip_samples.append(f"{line} -> {domain} ({skip_reason})")
                                else:
                                    if domain in domains:
                                        stats['duplicate_domains'] += 1
                                    else:
                                        domains.add(domain)
                                        stats['parsed_domains'] += 1
                                        if len(accepted_samples) < 3:
                                            accepted_samples.append(f"{line} -> {domain}")
                            else:
                                stats['invalid_domains'] += 1
                                if len(ignored_samples) < 3:
                                    ignored_samples.append(line)

                    except Exception as e:
                        print(f"è§£æç¬¬ {line_num} è¡Œæ—¶å‡ºé”™: {line[:50]}... - {e}")
                        stats['invalid_domains'] += 1
                        continue

                # è®¡ç®—æœ¬æ¬¡è¯·æ±‚ä¸­çš„ v2ray æ ‡ç­¾æ•°é‡å’Œé€šé…ç¬¦è§„åˆ™æ•°é‡
                current_v2ray_tags = self.stats.get('v2ray_with_tags', 0) - initial_v2ray_tags
                stats['v2ray_with_tags'] = current_v2ray_tags
                stats['wildcard_rules_processed'] = self.stats.get('wildcard_rules_processed', 0)

                # è®¡ç®—ç‰¹å®šè·¯å¾„åŸŸåæ€»æ•°
                total_path_domains = sum(len(domain_set) for domain_set in path_domains_classified.values())

                print(f"æˆåŠŸè·å– {len(domains)} ä¸ªæ™®é€šåŸŸåï¼Œ{total_path_domains} ä¸ªç‰¹å®šè·¯å¾„åŸŸå")
                print(f"  - æ€»è§„åˆ™: {stats['total_rules']}")
                print(f"  - æˆåŠŸè§£æ: {stats['parsed_domains']}")
                print(f"  - å¿½ç•¥(ç‰¹å®šè·¯å¾„): {stats['ignored_with_path']}")
                print(f"  - ğŸ”§ ç‰¹å®šè·¯å¾„->ä½ä¼˜å…ˆçº§: {stats['path_to_low_priority']}")
                print(f"  - ğŸ”§ ç‰¹å®šè·¯å¾„ä¿æŒåŸåŠ¨ä½œ: {stats['path_kept_action']}")
                print(f"  - å¿½ç•¥(æ³¨é‡Š): {stats['ignored_comments']}")
                print(f"  - å¿½ç•¥(æ— æ•ˆåŸŸå): {stats['invalid_domains']}")
                print(f"  - é‡å¤åŸŸå: {stats['duplicate_domains']}")
                print(f"  - è·³è¿‡åŸŸå: {stats['skipped_domains']}")
                if format_type == "v2ray" and stats['v2ray_with_tags'] > 0:
                    print(f"  - v2ray å¸¦æ ‡ç­¾è§„åˆ™: {stats['v2ray_with_tags']}")
                if stats['wildcard_rules_processed'] > 0:
                    print(f"  - ğŸ”§ é€šé…ç¬¦è§„åˆ™å¤„ç†: {stats['wildcard_rules_processed']}")

                # æ˜¾ç¤ºæ ·æœ¬
                if accepted_samples:
                    print(f"  - æ¥å—çš„è§„åˆ™æ ·æœ¬:")
                    for sample in accepted_samples:
                        print(f"    âœ“ {sample}")

                if path_to_low_priority_samples:
                    print(f"  - ğŸ”§ ç‰¹å®šè·¯å¾„->ä½ä¼˜å…ˆçº§æ ·æœ¬:")
                    for sample in path_to_low_priority_samples:
                        print(f"    ğŸ“ {sample}")

                if path_kept_action_samples:
                    print(f"  - ğŸ”§ ç‰¹å®šè·¯å¾„ä¿æŒåŠ¨ä½œæ ·æœ¬:")
                    for sample in path_kept_action_samples:
                        print(f"    ğŸ¯ {sample}")

                if skip_samples:
                    print(f"  - è·³è¿‡çš„åŸŸåæ ·æœ¬:")
                    for sample in skip_samples:
                        print(f"    â­ï¸ {sample}")

                if path_samples:
                    print(f"  - å¿½ç•¥çš„è·¯å¾„è§„åˆ™æ ·æœ¬:")
                    for sample in path_samples:
                        print(f"    ğŸ›¤ï¸  {sample}")

                if comment_samples:
                    print(f"  - å¿½ç•¥çš„æ³¨é‡Šè§„åˆ™æ ·æœ¬:")
                    for sample in comment_samples:
                        print(f"    # {sample}")

                if ignored_samples:
                    print(f"  - å…¶ä»–å¿½ç•¥çš„è§„åˆ™æ ·æœ¬:")
                    for sample in ignored_samples:
                        print(f"    âœ— {sample}")

                return domains, path_domains_classified, stats

            except requests.RequestException as e:
                print(f"è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}")
                if attempt < retry_count - 1:
                    time.sleep(retry_delay)
                else:
                    print(f"æ”¾å¼ƒè·å– {url}")

        return domains, {}, stats

    def _parse_csv_from_response(self, csv_content: str, csv_config: Dict, source_name: str, stats: Dict) -> Tuple[Set[str], Dict[str, Set[str]], Dict]:
        """
        ğŸ”§ ä¿®å¤ï¼šä» HTTP å“åº”å†…å®¹è§£æ CSV æ ¼å¼çš„åŸŸå

        Args:
            csv_content: CSV å†…å®¹å­—ç¬¦ä¸²
            csv_config: CSV é…ç½®
            source_name: æ•°æ®æºåç§°
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸

        Returns:
            (åŸŸåé›†åˆ, ç‰¹å®šè·¯å¾„åŸŸååˆ†ç±»å­—å…¸, ç»Ÿè®¡ä¿¡æ¯)
        """
        domains = set()
        path_domains = {}  # CSV é€šå¸¸ä¸åŒ…å«ç‰¹å®šè·¯å¾„è§„åˆ™ï¼Œä½†ä¸ºäº†æ¥å£ä¸€è‡´æ€§

        # CSV é…ç½®é»˜è®¤å€¼
        has_header = csv_config.get("has_header", True)
        delimiter = csv_config.get("delimiter", ",")
        column = csv_config.get("column")
        column_index = csv_config.get("column_index")

        try:
            csv_reader = csv.reader(csv_content.strip().split('\n'), delimiter=delimiter)

            headers = None
            actual_column_index = None
            skip_samples = []  # è·³è¿‡çš„åŸŸåæ ·æœ¬
            accepted_samples = []  # æ¥å—çš„åŸŸåæ ·æœ¬

            for row_num, row in enumerate(csv_reader, 1):
                if not row or all(cell.strip() == '' for cell in row):
                    continue  # è·³è¿‡ç©ºè¡Œ

                stats['total_rules'] += 1

                # å¤„ç†å¤´éƒ¨è¡Œ
                if has_header and row_num == 1:
                    headers = [cell.strip() for cell in row]

                    # å¦‚æœæŒ‡å®šäº†åˆ—åï¼Œæ‰¾åˆ°å¯¹åº”çš„ç´¢å¼•
                    if column:
                        try:
                            actual_column_index = headers.index(column)
                            print(f"  ğŸ“ CSV æ‰¾åˆ°ç›®æ ‡åˆ— '{column}' ä½äºç´¢å¼• {actual_column_index}")
                        except ValueError:
                            print(f"  âŒ CSV æœªæ‰¾åˆ°æŒ‡å®šçš„åˆ—å '{column}'")
                            print(f"  ğŸ“‹ CSV å¯ç”¨çš„åˆ—å: {', '.join(headers)}")
                            return domains, path_domains, stats
                    elif column_index is not None:
                        actual_column_index = column_index
                        if actual_column_index < len(headers):
                            print(f"  ğŸ“ CSV ä½¿ç”¨åˆ—ç´¢å¼• {actual_column_index}: '{headers[actual_column_index]}'")
                        else:
                            print(f"  âŒ CSV åˆ—ç´¢å¼• {actual_column_index} è¶…å‡ºèŒƒå›´")
                            return domains, path_domains, stats

                    continue  # è·³è¿‡å¤´éƒ¨è¡Œ

                # å¦‚æœæ²¡æœ‰è®¾ç½®å®é™…åˆ—ç´¢å¼•ï¼Œä½¿ç”¨é…ç½®çš„åˆ—ç´¢å¼•
                if actual_column_index is None and column_index is not None:
                    actual_column_index = column_index

                stats['csv_parsed_rows'] += 1

                try:
                    # è§£æ CSV è¡Œ
                    if actual_column_index is not None and actual_column_index < len(row):
                        url_value = row[actual_column_index].strip()
                        if url_value:
                            domain = self.extract_hostname_from_url(url_value)
                            if domain:
                                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä»æ•°æ®æºè·³è¿‡æ­¤åŸŸå
                                should_skip, skip_reason = self.should_skip_domain_from_source(domain, source_name)
                                if should_skip:
                                    stats['skipped_domains'] += 1
                                    if len(skip_samples) < 3:
                                        skip_samples.append(f"{url_value} -> {domain} ({skip_reason})")
                                else:
                                    if domain not in domains:
                                        domains.add(domain)
                                        stats['parsed_domains'] += 1
                                        stats['csv_extracted_domains'] += 1

                                        # æ˜¾ç¤ºä¸€äº›è§£ææ ·æœ¬
                                        if len(accepted_samples) < 5:
                                            accepted_samples.append(f"{url_value} -> {domain}")
                                    else:
                                        stats['duplicate_domains'] += 1
                            else:
                                stats['csv_invalid_urls'] += 1
                        else:
                            stats['invalid_domains'] += 1
                    else:
                        stats['invalid_domains'] += 1

                except Exception as e:
                    print(f"    âŒ CSV è§£æç¬¬ {row_num} è¡Œæ—¶å‡ºé”™: {e}")
                    stats['invalid_domains'] += 1

            print(f"  âœ… CSV è§£æå®Œæˆ: {stats['csv_extracted_domains']} ä¸ªæœ‰æ•ˆåŸŸå")

            # æ˜¾ç¤ºæ ·æœ¬
            if accepted_samples:
                print(f"  - CSV æ¥å—çš„åŸŸåæ ·æœ¬:")
                for sample in accepted_samples:
                    print(f"    âœ… {sample}")

            if skip_samples:
                print(f"  - CSV è·³è¿‡çš„åŸŸåæ ·æœ¬:")
                for sample in skip_samples:
                    print(f"    â­ï¸ {sample}")

            if stats['csv_invalid_urls'] > 0:
                print(f"  âš ï¸  CSV å¿½ç•¥äº† {stats['csv_invalid_urls']} ä¸ªæ— æ•ˆ URL")

        except Exception as e:
            print(f"    âŒ è§£æ CSV å†…å®¹å¤±è´¥: {e}")

        return domains, path_domains, stats

    def clean_domain(self, domain: str) -> str:
        """
        æ¸…ç†åŸŸåå­—ç¬¦ä¸²
        æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿æŒåŸå§‹ç»“æ„

        Args:
            domain: åŸå§‹åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ¸…ç†åçš„åŸŸå
        """
        if not domain:
            return None

        # å¦‚æœé…ç½®ä¸ºä¿æŒåŸå§‹ç»“æ„ï¼Œä½¿ç”¨ä¸“é—¨çš„æ–¹æ³•
        if self.config["parsing"].get("preserve_original_structure", True):
            return self.clean_domain_preserve_structure(domain)

        # åŸæ¥çš„æ¸…ç†é€»è¾‘ï¼ˆå¯èƒ½ç§»é™¤ www. å‰ç¼€ï¼‰
        return self._clean_domain_legacy(domain)

    def _clean_domain_legacy(self, domain: str) -> str:
        """
        ä¼ ç»Ÿçš„åŸŸåæ¸…ç†æ–¹æ³•ï¼ˆå¯èƒ½ç§»é™¤ www. å‰ç¼€ï¼‰

        Args:
            domain: åŸå§‹åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ¸…ç†åçš„åŸŸå
        """
        if not domain:
            return None

        # ç§»é™¤åè®®
        if domain.startswith(('http://', 'https://')):
            domain = urlparse(domain).netloc

        # ç§»é™¤ç«¯å£
        if ':' in domain:
            domain = domain.split(':')[0]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è·¯å¾„ï¼ˆè¿™é‡Œä¸åº”è¯¥æœ‰ï¼Œä½†ä»¥é˜²ä¸‡ä¸€ï¼‰
        if '/' in domain:
            domain = domain.split('/')[0]

        # ç§»é™¤ www. å‰ç¼€ï¼ˆä¼ ç»Ÿè¡Œä¸ºï¼‰
        if not self.config["parsing"].get("preserve_www_prefix", True):
            if domain.startswith('www.'):
                domain = domain[4:]

        # ç§»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        domain = re.sub(r'[^\w.-]', '', domain)

        # æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€
        if self.config["parsing"]["ignore_ip"] and self.is_ip_address(domain):
            return None

        # æ£€æŸ¥æ˜¯å¦æ˜¯localhost
        if self.config["parsing"]["ignore_localhost"] and domain in ['localhost', '127.0.0.1', '0.0.0.0']:
            return None

        # éªŒè¯åŸŸåæ ¼å¼
        if self.is_valid_domain(domain):
            return domain.lower()

        return None

    def is_ip_address(self, domain: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€

        Args:
            domain: åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦æ˜¯IPåœ°å€
        """
        ip_pattern = re.compile(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')
        return bool(ip_pattern.match(domain))

    def is_valid_domain(self, domain: str) -> bool:
        """
        ğŸ”§ æ”¹è¿›ï¼šéªŒè¯åŸŸåæ ¼å¼ï¼Œæ›´å®½æ¾çš„éªŒè¯é€»è¾‘

        Args:
            domain: åŸŸåå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦ä¸ºæœ‰æ•ˆåŸŸå
        """
        if not domain or len(domain) > 255:
            return False

        # ğŸ”§ æ”¹è¿›çš„åŸŸåæ ¼å¼éªŒè¯ï¼Œæ›´å®½æ¾
        # å…è®¸æ›´å¤šå­—ç¬¦ï¼ŒåŒ…æ‹¬ä¸€äº›ç‰¹æ®Šæƒ…å†µ

        # åŸºæœ¬æ£€æŸ¥ï¼šè‡³å°‘åŒ…å«ä¸€ä¸ªç‚¹
        if '.' not in domain:
            return False

        # åˆ†å‰²åŸŸåå„éƒ¨åˆ†
        parts = domain.split('.')

        # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºçš„éƒ¨åˆ†
        if any(not part for part in parts):
            return False

        # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†çš„æ ¼å¼
        for part in parts:
            # ğŸ”§ å…è®¸æ›´å®½æ¾çš„å­—ç¬¦é›†ï¼ŒåŒ…æ‹¬è¿å­—ç¬¦
            if not re.match(r'^[a-zA-Z0-9]([a-zA-Z0-9-]*[a-zA-Z0-9])?$', part):
                # ğŸ”§ ç‰¹æ®Šæƒ…å†µï¼šå•ä¸ªå­—ç¬¦çš„éƒ¨åˆ†ä¹Ÿå…è®¸
                if len(part) == 1 and re.match(r'^[a-zA-Z0-9]$', part):
                    continue
                return False

            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if len(part) > 63:
                return False

        # æ£€æŸ¥æœ€åä¸€ä¸ªéƒ¨åˆ†ï¼ˆTLDï¼‰æ˜¯å¦è‡³å°‘æœ‰2ä¸ªå­—ç¬¦
        if len(parts[-1]) < 2:
            return False

        return True

    def domain_to_regex(self, domain: str) -> str:
        """
        å°†åŸŸåè½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼

        Args:
            domain: åŸŸå

        Returns:
            æ­£åˆ™è¡¨è¾¾å¼å­—ç¬¦ä¸²
        """
        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
        escaped_domain = re.escape(domain)
        # æ·»åŠ å­åŸŸååŒ¹é…
        return f'(.*\.)?{escaped_domain}$'

    def smart_sort_domains(self, domains: Set[str]) -> List[str]:
        """
        æ™ºèƒ½æ’åºåŸŸåï¼Œä¾¿äºåç»­åˆå¹¶
        å…ˆæŒ‰TLDæ’åºï¼Œå†æŒ‰åŸŸåä¸»ä½“æ’åº

        Args:
            domains: åŸŸåé›†åˆ

        Returns:
            æ’åºåçš„åŸŸååˆ—è¡¨
        """
        def domain_sort_key(domain: str) -> Tuple[str, str]:
            """
            ç”ŸæˆåŸŸåæ’åºé”®ï¼š(TLD, åå‘åŸŸåä¸»ä½“)
            è¿™æ ·å¯ä»¥å°†åŒTLDçš„åŸŸåèšé›†åœ¨ä¸€èµ·ï¼Œä¾¿äºåˆå¹¶
            """
            parts = domain.split('.')
            if len(parts) >= 2:
                # TLD ä½œä¸ºä¸»è¦æ’åºé”®
                tld = parts[-1]
                # åŸŸåä¸»ä½“ä½œä¸ºæ¬¡è¦æ’åºé”®ï¼Œåå‘æ’åºä¾¿äºæ‰¾åˆ°å…¬å…±åç¼€
                base = '.'.join(parts[:-1])
                return (tld, base)
            else:
                return (domain, '')

        if self.config["optimization"].get("sort_before_merge", True):
            sorted_domains = sorted(list(domains), key=domain_sort_key)
            print(f"  ğŸ”„ åŸŸåå·²æŒ‰TLDæ™ºèƒ½æ’åºï¼Œä¾¿äºåˆå¹¶ä¼˜åŒ–")
            return sorted_domains
        else:
            return list(domains)

    def group_domains_by_tld(self, domains: Union[Set[str], List[str]]) -> Dict[str, List[str]]:
        """
        æŒ‰é¡¶çº§åŸŸååˆ†ç»„åŸŸåï¼Œå¹¶ä¿æŒæ’åº

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            æŒ‰TLDåˆ†ç»„çš„åŸŸåå­—å…¸ï¼Œå€¼ä¸ºæ’åºåçš„åˆ—è¡¨
        """
        tld_groups = defaultdict(list)

        # å¦‚æœè¾“å…¥æ˜¯é›†åˆï¼Œå…ˆè½¬æ¢ä¸ºæ™ºèƒ½æ’åºçš„åˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        for domain in domains:
            parts = domain.split('.')
            if len(parts) >= 2:
                # è·å–é¡¶çº§åŸŸåï¼ˆå¦‚ .com, .orgï¼‰
                tld = parts[-1]
                tld_groups[tld].append(domain)
            else:
                # å¤„ç†æ— æ•ˆåŸŸå
                tld_groups['other'].append(domain)

        return dict(tld_groups)

    def get_domain_base_and_tld(self, domain: str) -> Tuple[str, str]:
        """
        æå–åŸŸåçš„ä¸»ä½“éƒ¨åˆ†å’ŒTLD

        Args:
            domain: å®Œæ•´åŸŸå

        Returns:
            (åŸŸåä¸»ä½“, TLD)
        """
        parts = domain.split('.')
        if len(parts) >= 2:
            tld = parts[-1]
            base = '.'.join(parts[:-1])
            return base, tld
        return domain, ''

    def find_common_prefix(self, strings: List[str]) -> str:
        """
        æ‰¾åˆ°å­—ç¬¦ä¸²åˆ—è¡¨çš„å…¬å…±å‰ç¼€

        Args:
            strings: å­—ç¬¦ä¸²åˆ—è¡¨

        Returns:
            å…¬å…±å‰ç¼€
        """
        if not strings:
            return ""

        strings = [s for s in strings if s]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not strings:
            return ""

        min_len = min(len(s) for s in strings)
        prefix = ""

        for i in range(min_len):
            char = strings[0][i]
            if all(s[i] == char for s in strings):
                prefix += char
            else:
                break

        return prefix

    def find_common_suffix(self, strings: List[str]) -> str:
        """
        æ‰¾åˆ°å­—ç¬¦ä¸²åˆ—è¡¨çš„å…¬å…±åç¼€

        Args:
            strings: å­—ç¬¦ä¸²åˆ—è¡¨

        Returns:
            å…¬å…±åç¼€
        """
        if not strings:
            return ""

        strings = [s for s in strings if s]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        if not strings:
            return ""

        # åè½¬å­—ç¬¦ä¸²ï¼Œæ‰¾å‰ç¼€ï¼Œå†åè½¬å›æ¥
        reversed_strings = [s[::-1] for s in strings]
        reversed_suffix = self.find_common_prefix(reversed_strings)
        return reversed_suffix[::-1]

    def create_advanced_tld_regex(self, tld_domains: List[str], tld: str) -> str:
        """
        ä¸ºåŒä¸€TLDçš„åŸŸååˆ›å»ºé«˜çº§ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼
        ä¿®å¤ç‰ˆæœ¬ï¼šç¡®ä¿TLDä¸ä¼šä¸¢å¤±

        Args:
            tld_domains: åŒä¸€TLDçš„åŸŸååˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
            tld: é¡¶çº§åŸŸå

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        if len(tld_domains) == 1:
            return re.escape(tld_domains[0])

        # æå–åŸŸåä¸»ä½“éƒ¨åˆ†
        domain_bases = []
        for domain in tld_domains:
            base, domain_tld = self.get_domain_base_and_tld(domain)
            if domain_tld == tld:
                domain_bases.append(base)
            else:
                # TLDä¸åŒ¹é…çš„æƒ…å†µï¼Œä½¿ç”¨å®Œæ•´åŸŸå
                domain_bases.append(domain)

        if not domain_bases:
            return '|'.join(re.escape(d) for d in tld_domains)

        # å°è¯•æ‰¾åˆ°å…¬å…±æ¨¡å¼
        optimized_pattern = self.optimize_domain_bases(domain_bases)

        # æ£€æŸ¥åŸŸååŸºç¡€éƒ¨åˆ†çš„ç»“æ„
        simple_domains = [base for base in domain_bases if '.' not in base]  # äºŒçº§åŸŸå
        complex_domains = [base for base in domain_bases if '.' in base]     # å¤šçº§åŸŸå

        if len(simple_domains) == len(domain_bases):
            # æ‰€æœ‰éƒ½æ˜¯äºŒçº§åŸŸåï¼Œå¯ä»¥è¿›è¡ŒTLDä¼˜åŒ–
            return f"({optimized_pattern})\\.{re.escape(tld)}"
        elif len(complex_domains) == len(domain_bases):
            # æ‰€æœ‰éƒ½æ˜¯å¤šçº§åŸŸåï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰å…¬å…±çš„äºŒçº§+TLDåç¼€
            return self._optimize_complex_domains_with_tld(domain_bases, tld)
        else:
            # æ··åˆæƒ…å†µï¼šäºŒçº§åŸŸå+å¤šçº§åŸŸå
            return self._optimize_mixed_domains_with_tld(simple_domains, complex_domains, tld)

    def _optimize_complex_domains_with_tld(self, domain_bases: List[str], tld: str) -> str:
        """
        ä¼˜åŒ–å¤šçº§åŸŸåï¼Œç¡®ä¿ä¿ç•™TLD

        Args:
            domain_bases: åŸŸååŸºç¡€éƒ¨åˆ†åˆ—è¡¨ï¼ˆéƒ½æ˜¯å¤šçº§åŸŸåï¼‰
            tld: é¡¶çº§åŸŸå

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¬å…±çš„äºŒçº§åŸŸå+TLDæ¨¡å¼
        # ä¾‹å¦‚ï¼ša.pixnet.net, b.pixnet.net -> (a|b).pixnet.net

        # æ‰¾åˆ°æ‰€æœ‰åŸŸåçš„å…¬å…±åç¼€ï¼ˆä¸åŒ…æ‹¬ç¬¬ä¸€éƒ¨åˆ†ï¼‰
        if len(domain_bases) <= 1:
            if domain_bases:
                return f"{re.escape(domain_bases[0])}\\.{re.escape(tld)}"
            return f".*\\.{re.escape(tld)}"

        # åˆ†æç»“æ„ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰åŸŸåéƒ½æœ‰ç›¸åŒçš„åç¼€ç»“æ„
        common_suffix_parts = None
        prefixes = []

        for base in domain_bases:
            parts = base.split('.')
            if common_suffix_parts is None:
                # ç¬¬ä¸€ä¸ªåŸŸåï¼Œè®¾ç½®å…¬å…±åç¼€å€™é€‰
                if len(parts) >= 2:
                    common_suffix_parts = parts[1:]  # é™¤äº†ç¬¬ä¸€éƒ¨åˆ†çš„å…¶ä½™éƒ¨åˆ†
                    prefixes.append(parts[0])
                else:
                    # å¤„ç†å¼‚å¸¸æƒ…å†µ
                    common_suffix_parts = []
                    prefixes.append(base)
            else:
                # æ£€æŸ¥æ˜¯å¦ä¸å…¬å…±åç¼€åŒ¹é…
                if len(parts) >= len(common_suffix_parts) + 1:
                    current_suffix = parts[-(len(common_suffix_parts)):]
                    if current_suffix == common_suffix_parts:
                        prefixes.append(parts[0])
                    else:
                        # åç¼€ä¸åŒ¹é…ï¼Œæ— æ³•ä¼˜åŒ–ï¼Œç›´æ¥è¿”å›å®Œæ•´åŸŸååˆ—è¡¨
                        escaped_bases = [re.escape(base) for base in domain_bases]
                        return f"({'|'.join(escaped_bases)})\\.{re.escape(tld)}"
                else:
                    # é•¿åº¦ä¸å¤Ÿï¼Œæ— æ³•ä¼˜åŒ–
                    escaped_bases = [re.escape(base) for base in domain_bases]
                    return f"({'|'.join(escaped_bases)})\\.{re.escape(tld)}"

        # å¦‚æœæ‰¾åˆ°äº†å…¬å…±åç¼€ï¼Œè¿›è¡Œä¼˜åŒ–
        if common_suffix_parts and len(set(prefixes)) > 1:
            # ä¼˜åŒ–å‰ç¼€éƒ¨åˆ†
            optimized_prefixes = self.optimize_domain_bases(prefixes)
            escaped_suffix = '\\.'.join(re.escape(part) for part in common_suffix_parts)
            return f"({optimized_prefixes})\\.{escaped_suffix}\\.{re.escape(tld)}"
        else:
            # æ— æ³•æ‰¾åˆ°å…¬å…±æ¨¡å¼ï¼Œä½¿ç”¨åŸºç¡€ä¼˜åŒ–
            optimized_pattern = self.optimize_domain_bases(domain_bases)
            return f"({optimized_pattern})\\.{re.escape(tld)}"

    def _optimize_mixed_domains_with_tld(self, simple_domains: List[str], complex_domains: List[str], tld: str) -> str:
        """
        ä¼˜åŒ–æ··åˆåŸŸåï¼ˆäºŒçº§+å¤šçº§ï¼‰ï¼Œç¡®ä¿ä¿ç•™TLD

        Args:
            simple_domains: äºŒçº§åŸŸååˆ—è¡¨
            complex_domains: å¤šçº§åŸŸååˆ—è¡¨
            tld: é¡¶çº§åŸŸå

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        patterns = []

        # å¤„ç†äºŒçº§åŸŸå
        if simple_domains:
            if len(simple_domains) == 1:
                patterns.append(f"{re.escape(simple_domains[0])}\\.{re.escape(tld)}")
            else:
                optimized_simple = self.optimize_domain_bases(simple_domains)
                patterns.append(f"({optimized_simple})\\.{re.escape(tld)}")

        # å¤„ç†å¤šçº§åŸŸå
        if complex_domains:
            complex_pattern = self._optimize_complex_domains_with_tld(complex_domains, tld)
            patterns.append(complex_pattern)

        # åˆå¹¶æ‰€æœ‰æ¨¡å¼
        if len(patterns) == 1:
            return patterns[0]
        else:
            return f"({'|'.join(patterns)})"

    def optimize_domain_bases(self, domain_bases: List[str]) -> str:
        """
        ä¼˜åŒ–åŸŸååŸºç¡€éƒ¨åˆ†åˆ—è¡¨

        Args:
            domain_bases: åŸŸååŸºç¡€éƒ¨åˆ†åˆ—è¡¨

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        """
        if len(domain_bases) <= 1:
            return '|'.join(re.escape(base) for base in domain_bases)

        optimization_config = self.config["optimization"]

        # å°è¯•å‰ç¼€ä¼˜åŒ–
        if optimization_config.get("enable_prefix_optimization", True):
            common_prefix = self.find_common_prefix(domain_bases)
            min_prefix_len = optimization_config.get("min_common_prefix_length", 3)

            if len(common_prefix) >= min_prefix_len:
                # ç§»é™¤å…¬å…±å‰ç¼€
                suffixes = [base[len(common_prefix):] for base in domain_bases]
                suffixes = [s for s in suffixes if s]  # è¿‡æ»¤ç©ºåç¼€
                if suffixes and len(set(suffixes)) > 1:  # ç¡®ä¿æœ‰ä¸åŒçš„åç¼€
                    suffix_pattern = self.optimize_domain_bases(suffixes)
                    return f"{re.escape(common_prefix)}({suffix_pattern})"

        # å°è¯•åç¼€ä¼˜åŒ–
        if optimization_config.get("enable_suffix_optimization", True):
            common_suffix = self.find_common_suffix(domain_bases)
            min_suffix_len = optimization_config.get("min_common_suffix_length", 3)

            if len(common_suffix) >= min_suffix_len:
                # ç§»é™¤å…¬å…±åç¼€
                prefixes = [base[:-len(common_suffix)] for base in domain_bases]
                prefixes = [p for p in prefixes if p]  # è¿‡æ»¤ç©ºå‰ç¼€
                if prefixes and len(set(prefixes)) > 1:  # ç¡®ä¿æœ‰ä¸åŒçš„å‰ç¼€
                    prefix_pattern = self.optimize_domain_bases(prefixes)
                    return f"({prefix_pattern}){re.escape(common_suffix)}"

        # æ²¡æœ‰æ‰¾åˆ°ä¼˜åŒ–æ¨¡å¼ï¼Œç›´æ¥è¿æ¥
        return '|'.join(re.escape(base) for base in domain_bases)

    def create_single_regex_rule(self, domains: Union[Set[str], List[str]]) -> str:
        """
        åˆ›å»ºåŒ…å«æ‰€æœ‰åŸŸåçš„å•è¡Œæ­£åˆ™è¡¨è¾¾å¼ï¼ˆé«˜çº§TLDä¼˜åŒ–ï¼‰

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            å•è¡Œæ­£åˆ™è¡¨è¾¾å¼
        """
        if not domains:
            return ""

        # è½¬æ¢ä¸ºæ’åºåˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"

        print(f"ğŸš€ æ­£åœ¨ç”Ÿæˆé«˜çº§TLDä¼˜åŒ–å•è¡Œæ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ…å« {len(domains)} ä¸ªåŸŸå")

        # å¯ç”¨é«˜çº§TLDåˆå¹¶
        if self.config["optimization"].get("enable_advanced_tld_merge", True):
            # æŒ‰TLDåˆ†ç»„
            tld_groups = self.group_domains_by_tld(domains)
            tld_patterns = []

            print(f"  ğŸ“Š TLDåˆ†å¸ƒæƒ…å†µ:")
            for tld, tld_domains in sorted(tld_groups.items(), key=lambda x: len(x[1]), reverse=True):
                print(f"    .{tld}: {len(tld_domains)} ä¸ªåŸŸå")
                # æ˜¾ç¤ºä¸€äº›åŸŸåæ ·æœ¬
                if len(tld_domains) <= 3:
                    for domain in tld_domains:
                        print(f"      - {domain}")
                else:
                    for domain in tld_domains[:3]:
                        print(f"      - {domain}")
                    print(f"      - ... è¿˜æœ‰ {len(tld_domains)-3} ä¸ªåŸŸå")

            for tld, tld_domains in tld_groups.items():
                if len(tld_domains) == 1:
                    # å•ä¸ªåŸŸåç›´æ¥å¤„ç†
                    domain = tld_domains[0]
                    tld_patterns.append(re.escape(domain))
                else:
                    # å¤šä¸ªåŸŸåè¿›è¡Œé«˜çº§ä¼˜åŒ–
                    optimized_pattern = self.create_advanced_tld_regex(tld_domains, tld)
                    tld_patterns.append(optimized_pattern)
                    print(f"  âœ… TLD .{tld}: {len(tld_domains)} ä¸ªåŸŸåå·²ä¼˜åŒ–åˆå¹¶")

            # åˆå¹¶æ‰€æœ‰TLDç»„çš„æ¨¡å¼
            if len(tld_patterns) == 1:
                combined_pattern = tld_patterns[0]
            else:
                combined_pattern = f"({'|'.join(tld_patterns)})"

            single_regex = f"(.*\\.)?{combined_pattern}$"
        else:
            # ç®€å•åˆå¹¶æ¨¡å¼
            escaped_domains = [re.escape(d) for d in domains]
            combined_pattern = '|'.join(escaped_domains)
            single_regex = f"(.*\\.)?({combined_pattern})$"

        # æ˜¾ç¤ºè§„åˆ™é•¿åº¦ä¿¡æ¯
        rule_length = len(single_regex)
        print(f"  ğŸ“ ç”Ÿæˆçš„å•è¡Œè§„åˆ™é•¿åº¦: {rule_length:,} å­—ç¬¦")

        if rule_length > 100000:
            print("  âš ï¸  è§„åˆ™æé•¿ï¼Œå¯èƒ½ä¸¥é‡å½±å“æ€§èƒ½ï¼Œå»ºè®®åˆ†å‰²")
        elif rule_length > 50000:
            print("  âš ï¸  è§„åˆ™å¾ˆé•¿ï¼Œå¯èƒ½å½±å“åŒ¹é…æ€§èƒ½")
        elif rule_length > 10000:
            print("  âš ï¸  è§„åˆ™è¾ƒé•¿ï¼Œè¯·æ³¨æ„æ€§èƒ½")
        else:
            print("  âœ… è§„åˆ™é•¿åº¦é€‚ä¸­")

        return single_regex

    def create_multiple_optimized_rules(self, domains: Union[Set[str], List[str]]) -> List[str]:
        """
        åˆ›å»ºå¤šä¸ªä¼˜åŒ–çš„è§„åˆ™ï¼ˆéå•è¡Œæ¨¡å¼ï¼‰

        Args:
            domains: åŸŸåé›†åˆæˆ–åˆ—è¡¨

        Returns:
            ä¼˜åŒ–åçš„è§„åˆ™åˆ—è¡¨
        """
        if not domains:
            return []

        # è½¬æ¢ä¸ºæ’åºåˆ—è¡¨
        if isinstance(domains, set):
            domains = self.smart_sort_domains(domains)

        optimization_config = self.config["optimization"]
        max_domains_per_rule = optimization_config.get("max_domains_per_rule", 30)
        max_rule_length = optimization_config.get("max_rule_length", 4000)

        # æŒ‰TLDåˆ†ç»„å¤„ç†
        if optimization_config.get("group_by_tld", True):
            tld_groups = self.group_domains_by_tld(domains)
            rules = []

            for tld, tld_domains in tld_groups.items():
                if len(tld_domains) <= 1:
                    # å•ä¸ªåŸŸåç›´æ¥è½¬æ¢
                    rules.extend([self.domain_to_regex(domain) for domain in tld_domains])
                else:
                    # å¤šä¸ªåŸŸååˆ†æ‰¹å¤„ç†
                    tld_rules = self._create_batched_rules(tld_domains, tld, max_domains_per_rule, max_rule_length)
                    rules.extend(tld_rules)
                    print(f"  ğŸ“¦ TLD .{tld}: {len(tld_domains)} ä¸ªåŸŸå -> {len(tld_rules)} ä¸ªè§„åˆ™")

            return rules
        else:
            # ä¸åˆ†ç»„ï¼Œç›´æ¥åˆ†æ‰¹å¤„ç†
            return self._create_batched_rules(domains, None, max_domains_per_rule, max_rule_length)

    def _create_batched_rules(self, domains: List[str], tld: str = None, max_domains_per_rule: int = 30, max_rule_length: int = 4000) -> List[str]:
        """
        ä¸ºåŸŸååˆ—è¡¨åˆ›å»ºåˆ†æ‰¹çš„è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨
            tld: é¡¶çº§åŸŸåï¼ˆå¯é€‰ï¼Œç”¨äºä¼˜åŒ–ï¼‰
            max_domains_per_rule: æ¯ä¸ªè§„åˆ™çš„æœ€å¤§åŸŸåæ•°
            max_rule_length: æœ€å¤§è§„åˆ™é•¿åº¦

        Returns:
            åˆ†æ‰¹åçš„è§„åˆ™åˆ—è¡¨
        """
        rules = []
        current_batch = []

        for domain in domains:
            test_batch = current_batch + [domain]

            # åˆ›å»ºæµ‹è¯•è§„åˆ™
            if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                test_rule = self._create_tld_optimized_rule(test_batch, tld)
            else:
                test_rule = self._create_simple_rule(test_batch)

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if (len(test_batch) > max_domains_per_rule or
                len(test_rule) > max_rule_length):

                # ä¿å­˜å½“å‰æ‰¹æ¬¡
                if current_batch:
                    if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                        rules.append(self._create_tld_optimized_rule(current_batch, tld))
                    else:
                        rules.append(self._create_simple_rule(current_batch))

                # å¼€å§‹æ–°æ‰¹æ¬¡
                current_batch = [domain]
            else:
                current_batch.append(domain)

        # å¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            if tld and self.config["optimization"].get("enable_advanced_tld_merge", True):
                rules.append(self._create_tld_optimized_rule(current_batch, tld))
            else:
                rules.append(self._create_simple_rule(current_batch))

        return rules

    def _create_tld_optimized_rule(self, domains: List[str], tld: str) -> str:
        """
        ä¸ºåŒTLDåŸŸååˆ›å»ºä¼˜åŒ–è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨
            tld: é¡¶çº§åŸŸå

        Returns:
            TLDä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™
        """
        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"

        optimized_pattern = self.create_advanced_tld_regex(domains, tld)
        return f"(.*\\.)?{optimized_pattern}$"

    def _create_simple_rule(self, domains: List[str]) -> str:
        """
        ä¸ºåŸŸååˆ—è¡¨åˆ›å»ºç®€å•è§„åˆ™

        Args:
            domains: åŸŸååˆ—è¡¨

        Returns:
            ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™
        """
        if len(domains) == 1:
            return f"(.*\\.)?{re.escape(domains[0])}$"
        else:
            pattern = self.optimize_domain_bases(domains)
            return f"(.*\\.)?({pattern})$"

    def merge_domains_to_regex(self, domains: Set[str]) -> List[str]:
        """
        å°†å¤šä¸ªåŸŸååˆå¹¶ä¸ºä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨

        Args:
            domains: åŸŸåé›†åˆ

        Returns:
            ä¼˜åŒ–åçš„æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨
        """
        if not domains:
            return []

        # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ç”Ÿæˆå•è¡Œæ­£åˆ™
        if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
            print(f"ğŸš€ å¯ç”¨å¼ºåˆ¶å•è¡Œæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼")
            single_rule = self.create_single_regex_rule(domains)
            return [single_rule] if single_rule else []

        optimization_config = self.config["optimization"]

        # å¦‚æœæœªå¯ç”¨åˆå¹¶ä¼˜åŒ–ï¼Œè¿”å›å•ç‹¬çš„è§„åˆ™
        if not optimization_config.get("merge_domains", True):
            return [self.domain_to_regex(domain) for domain in domains]

        # ä½¿ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼
        print(f"ğŸ”§ å¯ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼ï¼Œå¤„ç† {len(domains)} ä¸ªåŸŸå")
        rules = self.create_multiple_optimized_rules(domains)

        print(f"  âœ… ä¼˜åŒ–å®Œæˆ: {len(domains)} ä¸ªåŸŸå -> {len(rules)} ä¸ªè§„åˆ™")
        return rules

    def collect_domains(self) -> Dict[str, Set[str]]:
        """
        ğŸ”§ ä¿®å¤ï¼šä»æ‰€æœ‰é…ç½®çš„æºæ”¶é›†åŸŸåï¼Œæ­£ç¡®å¤„ç†ç‰¹å®šè·¯å¾„è§„åˆ™çš„åŠ¨ä½œåˆ†é…

        Returns:
            æŒ‰åŠ¨ä½œåˆ†ç±»çš„åŸŸåé›†åˆ
        """
        categorized_domains = {
            'remove': set(),
            'low_priority': set(),
            'high_priority': set()
        }

        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_rules': 0,
            'parsed_domains': 0,
            'ignored_with_path': 0,
            'path_to_low_priority': 0,  # ç‰¹å®šè·¯å¾„è½¬ä½ä¼˜å…ˆçº§æ•°é‡
            'path_kept_action': 0,      # ğŸ”§ ç‰¹å®šè·¯å¾„ä¿æŒåŸåŠ¨ä½œæ•°é‡
            'invalid_domains': 0,
            'duplicate_domains': 0,
            'ignored_comments': 0,
            'auto_classified': 0,
            'skipped_domains': 0,
            'auto_added': 0,
            'skipped_from_sources': 0,
            'skip_overridden': 0,
            'v2ray_with_tags': 0,
            'csv_parsed_rows': 0,
            'csv_invalid_urls': 0,
            'csv_extracted_domains': 0,
            'wildcard_rules_processed': 0,  # ğŸ”§ å¤„ç†çš„é€šé…ç¬¦è§„åˆ™æ•°é‡
        }

        # è®°å½•æ¯ä¸ªç±»åˆ«çš„åŸŸåæ•°é‡
        self.category_domain_counts = {
            'remove': 0,
            'low_priority': 0,
            'high_priority': 0,
            'replace': 0
        }

        # ä»åœ¨çº¿æºæ”¶é›†åŸŸå
        for source in self.config["sources"]:
            if not source.get("enabled", True):
                continue

            print(f"\nå¤„ç†æ•°æ®æº: {source['name']}")
            format_type = source.get("format", "domain")
            csv_config = source.get("csv_config") if format_type == "csv" else None
            source_action = source.get("action", "remove")
            print(f"æ ¼å¼ç±»å‹: {format_type}ï¼ŒåŸå§‹åŠ¨ä½œ: {source_action}")

            # ğŸ”§ è®¾ç½®ä¸´æ—¶å˜é‡ä¾› fetch_domain_list ä½¿ç”¨
            self._current_source_action = source_action

            # ğŸ”§ ä¿®å¤ï¼šè·å–æ™®é€šåŸŸåå’Œå·²åˆ†ç±»çš„ç‰¹å®šè·¯å¾„åŸŸå
            domains, path_domains_classified, source_stats = self.fetch_domain_list(source["url"], format_type, source["name"], csv_config)

            # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
            for key in self.stats:
                if key in source_stats:
                    self.stats[key] += source_stats[key]

            # ğŸ”§ å¤„ç†æ™®é€šåŸŸååˆ†ç±»
            auto_classified_count = 0

            for domain in list(domains):
                # æ£€æŸ¥è‡ªåŠ¨åˆ†ç±»è§„åˆ™
                auto_action, reason = self.get_auto_classify_action(domain)
                if auto_action:
                    # ä»åŸå§‹é›†åˆä¸­ç§»é™¤ï¼Œæ·»åŠ åˆ°ç›¸åº”ç±»åˆ«
                    domains.remove(domain)
                    categorized_domains[auto_action].add(domain)
                    auto_classified_count += 1
                    if auto_classified_count <= 5:  # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬
                        print(f"  ğŸ”„ è‡ªåŠ¨åˆ†ç±»: {domain} -> {auto_action} ({reason})")
                else:
                    # ä½¿ç”¨æºçš„é»˜è®¤åŠ¨ä½œ
                    if source_action in categorized_domains:
                        categorized_domains[source_action].add(domain)

            # ğŸ”§ å¤„ç†ç‰¹å®šè·¯å¾„åŸŸåï¼ˆå·²ç»æŒ‰åŠ¨ä½œåˆ†ç±»ï¼‰
            path_auto_classified_count = 0
            total_path_domains = 0

            for path_action, path_domain_set in path_domains_classified.items():
                for domain in path_domain_set:
                    # æ£€æŸ¥è‡ªåŠ¨åˆ†ç±»è§„åˆ™ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
                    auto_action, reason = self.get_auto_classify_action(domain)
                    if auto_action:
                        categorized_domains[auto_action].add(domain)
                        path_auto_classified_count += 1
                        if path_auto_classified_count <= 3:
                            print(f"  ğŸ”„ ç‰¹å®šè·¯å¾„åŸŸåè‡ªåŠ¨åˆ†ç±»è¦†ç›–: {domain} -> {auto_action} ({reason}) (åŸä¸º {path_action})")
                    else:
                        # ä½¿ç”¨å·²ç¡®å®šçš„è·¯å¾„åŠ¨ä½œ
                        if path_action in categorized_domains:
                            categorized_domains[path_action].add(domain)
                        total_path_domains += 1

            auto_classified_count += path_auto_classified_count

            if auto_classified_count > 0:
                print(f"  âœ… è‡ªåŠ¨åˆ†ç±»å¤„ç†: {auto_classified_count} ä¸ªåŸŸå (æ™®é€š: {auto_classified_count - path_auto_classified_count}, ç‰¹å®šè·¯å¾„: {path_auto_classified_count})")
                self.stats['auto_classified'] += auto_classified_count

            # è®°å½•ä»æ•°æ®æºè·³è¿‡çš„åŸŸåæ•°é‡
            self.stats['skipped_from_sources'] += source_stats.get('skipped_domains', 0)

            total_added = len(domains) + total_path_domains
            print(f"å·²æ·»åŠ  {total_added} ä¸ªåŸŸååˆ°ç›¸åº”ç±»åˆ« (æ™®é€š: {len(domains)}, ç‰¹å®šè·¯å¾„: {total_path_domains})")

            # æ¸…é™¤ä¸´æ—¶å˜é‡
            delattr(self, '_current_source_action')

        # ä»è‡ªå®šä¹‰è§„åˆ™æ–‡ä»¶åŠ è½½
        print(f"\nå¤„ç†è‡ªå®šä¹‰è§„åˆ™æ–‡ä»¶...")
        if self.config.get("custom_rules", {}).get("enabled", False):
            custom_sources = self.config["custom_rules"]["sources"]

            for source in custom_sources:
                if not source.get("enabled", True):
                    continue

                print(f"\nå¤„ç†è‡ªå®šä¹‰è§„åˆ™: {source['name']}")
                file_path = source["file"]
                format_type = source.get("format", "domain")
                action = source.get("action", "remove")
                csv_config = source.get("csv_config") if format_type == "csv" else None

                if not os.path.exists(file_path):
                    print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    continue

                domains, replace_rules, source_stats = self.load_custom_rules_from_file(
                    file_path, format_type, action, csv_config
                )

                # ç´¯åŠ ç»Ÿè®¡ä¿¡æ¯
                self.stats['total_rules'] += source_stats['total_rules']
                self.stats['parsed_domains'] += source_stats['parsed_domains']
                self.stats['invalid_domains'] += source_stats['invalid_domains']
                self.stats['ignored_comments'] += source_stats['ignored_comments']
                self.stats['csv_parsed_rows'] += source_stats.get('csv_parsed_rows', 0)
                self.stats['csv_invalid_urls'] += source_stats.get('csv_invalid_urls', 0)
                self.stats['csv_extracted_domains'] += source_stats.get('csv_extracted_domains', 0)

                # å°†åŸŸåæ·»åŠ åˆ°ç›¸åº”ç±»åˆ«
                if action in categorized_domains:
                    categorized_domains[action].update(domains)

                # å¤„ç†æ›¿æ¢è§„åˆ™
                if action == "replace" and replace_rules:
                    # æ›´æ–°é…ç½®ä¸­çš„æ›¿æ¢è§„åˆ™
                    self.config["replace_rules"].update(replace_rules)

                print(f"  âœ… ä»æ–‡ä»¶åŠ è½½äº† {len(domains) + len(replace_rules)} ä¸ªè§„åˆ™åˆ° {action} ç±»åˆ«")

        # ğŸ”¥ åº”ç”¨è‡ªåŠ¨åˆ†ç±»è§„åˆ™ä¸­çš„åŸŸå
        self.apply_auto_classify_rules_directly(categorized_domains)

        return categorized_domains

    def sort_rules(self, rules: Union[Dict, List]) -> Union[Dict, List]:
        """
        å¯¹è§„åˆ™è¿›è¡Œæ’åº

        Args:
            rules: è§„åˆ™æ•°æ®

        Returns:
            æ’åºåçš„è§„åˆ™
        """
        if isinstance(rules, dict):
            # å¯¹å­—å…¸æŒ‰é”®æ’åº
            return OrderedDict(sorted(rules.items()))
        elif isinstance(rules, list):
            # å¯¹åˆ—è¡¨æŒ‰å€¼æ’åº
            return sorted(rules)
        else:
            return rules

    def generate_rules(self) -> Dict[str, any]:
        """
        ç”Ÿæˆå„ç±»è§„åˆ™

        Returns:
            è§„åˆ™å­—å…¸
        """
        print("\nå¼€å§‹ç”Ÿæˆ SearXNG hostnames è§„åˆ™...")

        # æ˜¾ç¤ºä¼˜åŒ–æ¨¡å¼ä¿¡æ¯
        if self.force_single_regex or self.config["optimization"].get("force_single_regex", False):
            print("ğŸš€ å·²å¯ç”¨é«˜çº§TLDä¼˜åŒ–å•è¡Œæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼")
            print("   æ¯ä¸ªç±»åˆ«å°†ç”Ÿæˆå•ä¸ªåŒ…å«æ‰€æœ‰åŸŸåçš„é«˜çº§ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼")
        else:
            print("ğŸ”§ å·²å¯ç”¨å¤šè§„åˆ™ä¼˜åŒ–æ¨¡å¼")
            print("   å°†ç”Ÿæˆå¤šä¸ªæ€§èƒ½ä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™")

        # æ˜¾ç¤ºè§£æé…ç½®
        parsing_config = self.config["parsing"]
        print(f"ğŸ“ è§£æé…ç½®:")
        print(f"   - å¿½ç•¥ç‰¹å®šè·¯å¾„è§„åˆ™: {parsing_config.get('ignore_specific_paths', False)}")
        print(f"   - ğŸ”§ ç‰¹å®šè·¯å¾„è§„åˆ™å¤„ç†: {parsing_config.get('specific_path_action', 'keep_action')}")
        print(f"   - ä¸¥æ ¼åŸŸåçº§åˆ«æ£€æŸ¥: {parsing_config.get('strict_domain_level_check', True)}")
        print(f"   - ä¿æŒåŸå§‹ç»“æ„: {parsing_config.get('preserve_original_structure', True)}")
        print(f"   - ä¿æŒ www. å‰ç¼€: {parsing_config.get('preserve_www_prefix', True)}")

        # æ”¶é›†åŸŸå
        categorized_domains = self.collect_domains()

        # å¤„ç†è‡ªåŠ¨åˆ†ç±»æ›¿æ¢è§„åˆ™
        auto_replace_rules = {}
        for rule in self.auto_classify_rules:
            if rule['action'] == 'replace':
                old_regex = f"(.*\\.)?{re.escape(rule['old_domain'])}$"
                auto_replace_rules[old_regex] = rule['new_domain']

        rules = {}

        # æ›¿æ¢è§„åˆ™ (å­—å…¸æ ¼å¼)
        all_replace_rules = {}
        all_replace_rules.update(self.config["replace_rules"])
        all_replace_rules.update(auto_replace_rules)

        if all_replace_rules:
            rules["replace"] = all_replace_rules
            # è®°å½•æ›¿æ¢è§„åˆ™çš„åŸŸåæ•°é‡
            self.category_domain_counts["replace"] = len(all_replace_rules)
        else:
            # å³ä½¿æ²¡æœ‰æ›¿æ¢è§„åˆ™ï¼Œä¹Ÿåˆ›å»ºç©ºè§„åˆ™ä»¥ç¡®ä¿æ–‡ä»¶è¢«åˆ›å»º
            self.category_domain_counts["replace"] = 0

        # ç§»é™¤è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆç§»é™¤è§„åˆ™...")
        remove_rules = []
        remove_rules.extend(self.config["fixed_remove"])

        # è®°å½•å›ºå®šç§»é™¤è§„åˆ™æ•°é‡
        fixed_remove_count = len(self.config["fixed_remove"])

        if categorized_domains["remove"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['remove'])} ä¸ªç§»é™¤åŸŸå...")
            self.category_domain_counts["remove"] = len(categorized_domains["remove"]) + fixed_remove_count
            merged_remove_rules = self.merge_domains_to_regex(categorized_domains["remove"])
            remove_rules.extend(merged_remove_rules)
        else:
            self.category_domain_counts["remove"] = fixed_remove_count

        if remove_rules:
            rules["remove"] = remove_rules
        else:
            # åˆ›å»ºç©ºç§»é™¤è§„åˆ™åˆ—è¡¨ï¼Œç¡®ä¿æ–‡ä»¶è¢«åˆ›å»º
            rules["remove"] = []

        # ä½ä¼˜å…ˆçº§è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆä½ä¼˜å…ˆçº§è§„åˆ™...")
        low_priority_rules = []
        low_priority_rules.extend(self.config["fixed_low_priority"])

        # è®°å½•å›ºå®šä½ä¼˜å…ˆçº§è§„åˆ™æ•°é‡
        fixed_low_priority_count = len(self.config["fixed_low_priority"])

        if categorized_domains["low_priority"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['low_priority'])} ä¸ªä½ä¼˜å…ˆçº§åŸŸå...")
            self.category_domain_counts["low_priority"] = len(categorized_domains["low_priority"]) + fixed_low_priority_count
            merged_low_priority_rules = self.merge_domains_to_regex(categorized_domains["low_priority"])
            low_priority_rules.extend(merged_low_priority_rules)
        else:
            self.category_domain_counts["low_priority"] = fixed_low_priority_count

        if low_priority_rules:
            rules["low_priority"] = low_priority_rules
        else:
            # åˆ›å»ºç©ºä½ä¼˜å…ˆçº§è§„åˆ™åˆ—è¡¨ï¼Œç¡®ä¿æ–‡ä»¶è¢«åˆ›å»º
            rules["low_priority"] = []

        # é«˜ä¼˜å…ˆçº§è§„åˆ™ (åˆ—è¡¨æ ¼å¼) - ä½¿ç”¨ä¼˜åŒ–çš„åˆå¹¶
        print(f"\nç”Ÿæˆé«˜ä¼˜å…ˆçº§è§„åˆ™...")
        high_priority_rules = []
        high_priority_rules.extend(self.config["fixed_high_priority"])

        # è®°å½•å›ºå®šé«˜ä¼˜å…ˆçº§è§„åˆ™æ•°é‡
        fixed_high_priority_count = len(self.config["fixed_high_priority"])

        if categorized_domains["high_priority"]:
            print(f"æ­£åœ¨ä¼˜åŒ– {len(categorized_domains['high_priority'])} ä¸ªé«˜ä¼˜å…ˆçº§åŸŸå...")
            self.category_domain_counts["high_priority"] = len(categorized_domains["high_priority"]) + fixed_high_priority_count
            merged_high_priority_rules = self.merge_domains_to_regex(categorized_domains["high_priority"])
            high_priority_rules.extend(merged_high_priority_rules)
        else:
            self.category_domain_counts["high_priority"] = fixed_high_priority_count

        if high_priority_rules:
            rules["high_priority"] = high_priority_rules
        else:
            # åˆ›å»ºç©ºé«˜ä¼˜å…ˆçº§è§„åˆ™åˆ—è¡¨ï¼Œç¡®ä¿æ–‡ä»¶è¢«åˆ›å»º
            rules["high_priority"] = []

        # å¯¹æ‰€æœ‰è§„åˆ™è¿›è¡Œæ’åºå’Œå»é‡
        for rule_type in rules:
            if rule_type == "replace":
                # æ›¿æ¢è§„åˆ™æŒ‰é”®æ’åº
                rules[rule_type] = self.sort_rules(rules[rule_type])
            else:
                # åˆ—è¡¨è§„åˆ™å»é‡å¹¶æ’åº
                rules[rule_type] = self.sort_rules(list(set(rules[rule_type])))

        return rules

    def save_separate_files(self, rules: Dict[str, any]) -> None:
        """
        ä¿å­˜ä¸ºåˆ†ç¦»çš„æ–‡ä»¶

        Args:
            rules: è§„åˆ™å­—å…¸
        """
        output_dir = self.config["output"]["directory"]
        files_config = self.config["output"]["files"]

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # ç”Ÿæˆä¸»é…ç½®æ–‡ä»¶ (ç”¨äºå¼•ç”¨å¤–éƒ¨æ–‡ä»¶)
        main_config = {"hostnames": {}}

        # ä¿å­˜å„ç±»è§„åˆ™åˆ°å•ç‹¬æ–‡ä»¶ - ç¡®ä¿æ‰€æœ‰ç±»åˆ«çš„æ–‡ä»¶éƒ½è¢«åˆ›å»º
        expected_rule_types = ["replace", "remove", "low_priority", "high_priority"]

        for rule_type in expected_rule_types:
            if rule_type in files_config:
                filename = files_config[rule_type]
                filepath = os.path.join(output_dir, filename)

                # è·å–è§„åˆ™æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç©ºæ•°æ®
                rule_data = rules.get(rule_type, [] if rule_type != "replace" else {})

                try:
                    with open(filepath, 'w', encoding='utf-8') as f:
                        # ç®€åŒ–çš„æ–‡ä»¶å¤´æ³¨é‡Š
                        rule_count = len(rule_data) if isinstance(rule_data, (list, dict)) else 0
                        domain_count = self.category_domain_counts.get(rule_type, 0)

                        f.write(f"# SearXNG {rule_type} rules\n")
                        f.write(f"# Total rules: {rule_count}, Total domains: {domain_count}\n")
                        f.write("\n")

                        # ç›´æ¥å†™å…¥è§„åˆ™å†…å®¹ï¼Œä¸åŒ…å«é¡¶çº§é”®
                        if rule_data or rule_type in rules:  # åªæœ‰å½“æœ‰æ•°æ®æˆ–åŸæœ¬å°±åœ¨rulesä¸­æ‰å†™å…¥å†…å®¹
                            yaml.dump(rule_data, f, default_flow_style=False, allow_unicode=True, indent=2)
                        else:
                            # å†™å…¥ç©ºå†…å®¹æ ‡è®°
                            if rule_type == "replace":
                                f.write("{}\n")  # ç©ºå­—å…¸
                            else:
                                f.write("[]\n")  # ç©ºåˆ—è¡¨

                    print(f"å·²ä¿å­˜ {rule_type} è§„åˆ™åˆ°: {filepath} ({rule_count} æ¡è§„åˆ™)")

                    # åœ¨ä¸»é…ç½®ä¸­å¼•ç”¨å¤–éƒ¨æ–‡ä»¶
                    main_config["hostnames"][rule_type] = filename

                except Exception as e:
                    print(f"ä¿å­˜ {rule_type} è§„åˆ™å¤±è´¥: {e}")

        # ä¿å­˜ä¸»é…ç½®æ–‡ä»¶
        if main_config["hostnames"]:
            main_config_path = os.path.join(output_dir, files_config["main_config"])
            try:
                with open(main_config_path, 'w', encoding='utf-8') as f:
                    # ç®€åŒ–çš„ä¸»é…ç½®æ–‡ä»¶å¤´
                    f.write("# SearXNG hostnames configuration\n")
                    f.write("# This file references external rule files\n")
                    f.write("\n")
                    yaml.dump(main_config, f, default_flow_style=False, allow_unicode=True, indent=2)
                print(f"å·²ä¿å­˜ä¸»é…ç½®åˆ°: {main_config_path}")
            except Exception as e:
                print(f"ä¿å­˜ä¸»é…ç½®å¤±è´¥: {e}")

    def save_single_file(self, rules: Dict[str, any]) -> None:
        """
        ä¿å­˜ä¸ºå•ä¸ªæ–‡ä»¶

        Args:
            rules: è§„åˆ™å­—å…¸
        """
        output_dir = self.config["output"]["directory"]
        os.makedirs(output_dir, exist_ok=True)

        # ç¡®ä¿æ‰€æœ‰ç±»åˆ«éƒ½åœ¨è§„åˆ™ä¸­
        expected_rule_types = ["replace", "remove", "low_priority", "high_priority"]
        for rule_type in expected_rule_types:
            if rule_type not in rules:
                if rule_type == "replace":
                    rules[rule_type] = {}
                else:
                    rules[rule_type] = []

        # æ„å»ºå®Œæ•´çš„ hostnames é…ç½®
        hostnames_config = {"hostnames": rules}

        filepath = os.path.join(output_dir, "hostnames.yml")

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                # ç®€åŒ–çš„æ–‡ä»¶å¤´æ³¨é‡Š
                total_rules = sum(len(rule_data) if isinstance(rule_data, (list, dict)) else 0 for rule_data in rules.values())
                total_domains = sum(self.category_domain_counts.values())

                f.write("# SearXNG hostnames configuration\n")
                f.write(f"# Total rules: {total_rules}, Total domains: {total_domains}\n")
                f.write("\n")

                yaml.dump(hostnames_config, f, default_flow_style=False, allow_unicode=True, indent=2)

            print(f"å·²ä¿å­˜å®Œæ•´é…ç½®åˆ°: {filepath}")

        except Exception as e:
            print(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")

    def run(self) -> None:
        """
        è¿è¡Œç”Ÿæˆå™¨
        """
        print("SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨å¯åŠ¨ - åŸŸåæå–ä¿®å¤ç‰ˆ")
        print("ğŸ”§ ä¿®å¤ï¼šæ”¹è¿›åŸŸåæå–é€»è¾‘ï¼Œæ”¯æŒæ›´å¤šè§„åˆ™æ ¼å¼")
        print("ğŸ”§ ä¿®å¤ï¼šæ”¹è¿›åŸŸåéªŒè¯é€»è¾‘ï¼Œå‡å°‘è¯¯åˆ¤")
        print("ğŸ”§ æ–°å¢ï¼šæ”¯æŒé€šé…ç¬¦è§„åˆ™å¤„ç†")
        print("=" * 60)

        try:
            # ç”Ÿæˆè§„åˆ™
            rules = self.generate_rules()

            # æ ¹æ®é…ç½®ä¿å­˜æ–‡ä»¶
            if self.config["output"]["mode"] == "separate_files":
                self.save_separate_files(rules)
            else:
                self.save_single_file(rules)

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.print_statistics(rules)

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
        except Exception as e:
            print(f"\nç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

    def print_statistics(self, rules: Dict[str, any]) -> None:
        """
        è¾“å‡ºç»Ÿè®¡ä¿¡æ¯

        Args:
            rules: ç”Ÿæˆçš„è§„åˆ™
        """
        print("\n" + "=" * 70)
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")

        total_rules = 0
        total_domains = 0

        for rule_type, rule_data in rules.items():
            if isinstance(rule_data, dict):
                rule_count = len(rule_data)
                domain_count = self.category_domain_counts.get(rule_type, 0)
                print(f"  {rule_type} è§„åˆ™: {rule_count} æ¡ (åŒ…å« {domain_count} ä¸ªåŸŸå)")
                total_rules += rule_count
                total_domains += domain_count
            elif isinstance(rule_data, list):
                rule_count = len(rule_data)
                domain_count = self.category_domain_counts.get(rule_type, 0)
                print(f"  {rule_type} è§„åˆ™: {rule_count} æ¡ (åŒ…å« {domain_count} ä¸ªåŸŸå)")
                total_rules += rule_count
                total_domains += domain_count

        print(f"\nğŸ“ˆ æ€»è®¡: {total_rules} æ¡è§„åˆ™ (åŒ…å« {total_domains} ä¸ªåŸŸå)")

        print(f"\nğŸ” è§£æç»Ÿè®¡:")
        print(f"  - æ€»è¾“å…¥è§„åˆ™: {self.stats['total_rules']:,}")
        print(f"  - æˆåŠŸè§£æåŸŸå: {self.stats['parsed_domains']:,}")
        print(f"  - å¿½ç•¥(ç‰¹å®šè·¯å¾„): {self.stats['ignored_with_path']:,}")
        print(f"  - ğŸ”§ ç‰¹å®šè·¯å¾„->ä½ä¼˜å…ˆçº§: {self.stats.get('path_to_low_priority', 0):,}")
        print(f"  - ğŸ”§ ç‰¹å®šè·¯å¾„ä¿æŒåŸåŠ¨ä½œ: {self.stats.get('path_kept_action', 0):,}")
        print(f"  - å¿½ç•¥(æ³¨é‡Š): {self.stats['ignored_comments']:,}")
        print(f"  - å¿½ç•¥(æ— æ•ˆåŸŸå): {self.stats['invalid_domains']:,}")
        print(f"  - é‡å¤åŸŸå: {self.stats['duplicate_domains']:,}")

        if self.stats.get('wildcard_rules_processed', 0) > 0:
            print(f"  - ğŸ”§ é€šé…ç¬¦è§„åˆ™å¤„ç†: {self.stats.get('wildcard_rules_processed', 0):,}")
        if self.stats.get('auto_classified', 0) > 0:
            print(f"  - è‡ªåŠ¨åˆ†ç±»å¤„ç†: {self.stats.get('auto_classified', 0):,}")
        if self.stats.get('auto_added', 0) > 0:
            print(f"  - ä¸»åŠ¨æ·»åŠ åŸŸå: {self.stats.get('auto_added', 0):,}")
        if self.stats.get('skipped_from_sources', 0) > 0:
            print(f"  - ä»æ•°æ®æºè·³è¿‡: {self.stats.get('skipped_from_sources', 0):,}")
        if self.stats.get('v2ray_with_tags', 0) > 0:
            print(f"  - v2ray å¸¦æ ‡ç­¾è§„åˆ™: {self.stats.get('v2ray_with_tags', 0):,}")
        if self.stats.get('csv_extracted_domains', 0) > 0:
            print(f"  - CSV æå–åŸŸå: {self.stats.get('csv_extracted_domains', 0):,}")

        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.config['output']['directory']}")

        print(f"\nğŸ”§ ç‰¹å®šè·¯å¾„è§„åˆ™å¤„ç†:")
        specific_path_action = self.config['parsing'].get('specific_path_action', 'keep_action')
        print(f"  - å¤„ç†æ¨¡å¼: {specific_path_action}")
        if specific_path_action == 'low_priority':
            print(f"  - è½¬ä¸ºä½ä¼˜å…ˆçº§çš„æ•°é‡: {self.stats.get('path_to_low_priority', 0):,}")
            print(f"  - æ•ˆæœ: æ‰€æœ‰ç‰¹å®šè·¯å¾„è§„åˆ™å¼ºåˆ¶è®¾ç½®ä¸ºä½ä¼˜å…ˆçº§")
        elif specific_path_action == 'keep_action':
            print(f"  - ä¿æŒåŸåŠ¨ä½œçš„æ•°é‡: {self.stats.get('path_kept_action', 0):,}")
            print(f"  - è½¬ä¸ºä½ä¼˜å…ˆçº§çš„æ•°é‡: {self.stats.get('path_to_low_priority', 0):,}")
            print(f"  - æ•ˆæœ: ç‰¹å®šè·¯å¾„è§„åˆ™ä¿æŒæºçš„åŸå§‹åŠ¨ä½œ (æ¨è)")
        elif specific_path_action == 'smart':
            print(f"  - æ™ºèƒ½å¤„ç†çš„æ•°é‡: {self.stats.get('path_kept_action', 0):,} + {self.stats.get('path_to_low_priority', 0):,}")
            print(f"  - æ•ˆæœ: remove->low_priorityï¼Œå…¶ä»–åŠ¨ä½œä¿æŒä¸å˜")
        elif specific_path_action == 'ignore':
            print(f"  - æ•ˆæœ: ç‰¹å®šè·¯å¾„è§„åˆ™è¢«å®Œå…¨å¿½ç•¥")

        print(f"\nâœ¨ ä¼˜åŒ–æ•ˆæœ:")
        if total_domains > 0 and total_rules > 0:
            compression_ratio = (total_rules / total_domains) * 100
            print(f"  - å‹ç¼©æ¯”ç‡: {compression_ratio:.1f}% ({total_domains:,} ä¸ªåŸŸå -> {total_rules} æ¡è§„åˆ™)")

        print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        if self.config["output"]["mode"] == "separate_files":
            print("åœ¨ SearXNG settings.yml ä¸­æ·»åŠ :")
            print("hostnames:")
            for rule_type, filename in self.config["output"]["files"].items():
                if rule_type != "main_config" and rule_type in ["replace", "remove", "low_priority", "high_priority"]:
                    print(f"  {rule_type}: '{filename}'")
        else:
            print("å°†ç”Ÿæˆçš„ hostnames.yml å†…å®¹å¤åˆ¶åˆ° SearXNG settings.yml ä¸­")


def main():
    parser = argparse.ArgumentParser(description="SearXNG Hostnames è§„åˆ™ç”Ÿæˆå™¨ - åŸŸåæå–ä¿®å¤ç‰ˆ")
    parser.add_argument("-c", "--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--single-regex", action="store_true", help="å¼ºåˆ¶ç”Ÿæˆé«˜çº§TLDä¼˜åŒ–çš„å•è¡Œæ­£åˆ™è¡¨è¾¾å¼")

    args = parser.parse_args()

    generator = SearXNGHostnamesGenerator(args.config, force_single_regex=args.single_regex)
    generator.run()


if __name__ == "__main__":
    main()
